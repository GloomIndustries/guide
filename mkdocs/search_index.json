{
    "docs": [
        {
            "location": "/", 
            "text": "Introducing Screwdriver\n\n    \nA collection of services that facilitate the workflow for continuous delivery pipelines.\n\n\n\n\n\n\n    \n\n        \nSecure Continuous Delivery\n\n        \nScrewdriver treats Continuous Delivery as a first-class citizen in your build pipeline.\n        Easily define the path that your code takes from Pull Request to Production.\n\n    \n\n    \n\n        \n\n    \n\n\n\n\n\n\n    \n\n        \n\n    \n\n    \n\n        \nIntegrates with Daily Habits\n\n        \nScrewdriver ties directly into your DevOps daily habits.\n        It tests your pull requests, builds your merged commits, and deploys to your environments.\n        Define load tests, canary deployments, and multi-environment deployment pipelines with ease.\n\n    \n\n\n\n\n\n\n    \n\n        \nPipeline as Code\n\n        \nDefine your pipeline in a simple YAML file that lives beside your code.\n        There is no external configuration of your pipeline to deal with,\n        so your pipeline changes can be reviewed and rolled out with the rest of your codebase.\n\n    \n\n    \n\n        \n\n    \n\n\n\n\n\n\n    \n\n        \n\n    \n\n    \n\n        \nRuns Anywhere\n\n        \nScrewdriver's architecture uses pluggable components under the hood\n        to allow you to swap out the pieces that make sense for your infrastructure.\n        Swap in Postgres for the Datastore or Jenkins for the Executor.\n        You can even dynamically select an execution engine based on the needs of each pipeline.\n        For example, send golang builds to the kubernetes executor while your iOS builds got to a\n        Jenkins execution farm.", 
            "title": "Home"
        }, 
        {
            "location": "/getting-started/cluster-setup/", 
            "text": "Setting Up a Screwdriver Cluster on AWS using Kubernetes\n\n\nYou can setup a Screwdriver cluster using \nKubernetes\n.\n\n\nScrewdriver cluster\n\n\nA Screwdriver cluster consists of a Kubernetes cluster running the Screwdriver API. The Screwdriver API modifies Screwdriver tables in DynamoDB.\n\n\n\n\nPrerequisites\n\n\n\n\nkubectl\n\n\nan \nAWS\n account\n\n\nAWS CLI\n\n\nScrewdriver tables on DynamoDB\n\n\n\n\nCreate your Kubernetes cluster\n\n\nFollow instructions at \nGetting Started with AWS\n.\n\n\nSetup Kubernetes secrets\n\n\nA \nSecret\n is an object that contains a small amount of sensitive data such as a password, a token, or a key. You will need to setup secrets for Screwdriver for your cluster to work properly.\n\n\nFirst, you will need to gather some secrets:\n\n\n\n\n\n\n\n\nSecret Key\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDATASTORE_DYNAMODB_ID\n\n\nAWS Access Key ID\n\n\n\n\n\n\nDATASTORE_DYNAMODB_SECRET\n\n\nAWS Secret Access Key\n\n\n\n\n\n\nSECRET_OAUTH_CLIENT_ID\n\n\nThe client ID used for \nOAuth\n with Github\n\n\n\n\n\n\nSECRET_OAUTH_CLIENT_SECRET\n\n\nThe client secret used for OAuth with github\n\n\n\n\n\n\nSECRET_JWT_PRIVATE_KEY\n\n\nA private key used for signing JWT tokens. Can be anything\n\n\n\n\n\n\nSECRET_PASSWORD\n\n\nA password used for encrypting session, and OAuth data. Can be anything. \nNeeds to be minimum 32 characters\n\n\n\n\n\n\n\n\nHere are some directions for getting your secrets:\n\n\nGet your DynamoDB secrets\n\n\nTo get your \naccessKeyId\n and \nsecretAccessKey\n:\n\n\n\n\n\n\nNavigate to \nIAM\n in your AWS console\n\n\n\n\n\n\nClick on Users, and create a Screwdriver user. We recommend installing using an account which has read/write access to AWS DynamoDB. \nFor information on how to create an AWS DynamoDB policy, see \nPolicy Examples\n.\n\n\n\n\n\n\nSelect the \nSecurity Credentials\n tab\n\n\n\n\n\n\nClick \nCreate Access Key\n\n\n\n\n\n\nDownload the file and keep note of those values for your \ndynamodbid\n and \ndynamodbsecret\n.\n\n\n\n\n\n\nGet your OAuth Client ID and Secret\n\n\n\n\n\n\nNavigate to the \nOAuth applications\n page.\n\n\n\n\n\n\nClick Register a new application.\n\n\n\n\n\n\nFill out the information and click Register application.\n\n\n\n\n\n\n\n\nYou should see a \nClient ID\n and \nClient Secret\n, which will be used for your \noauthclientid\n and \noauthclientsecret\n, respectively.\n\n\nBase64 encode your secrets\n\n\nEach secret must be \nbase64 encoded\n. You must base64 encode each of your secrets:\n\n\n$ echo -n \nsomejwtprivatekey\n | base64\nc29tZWp3dHByaXZhdGVrZXk=\n$ echo -n \n1f2d1e2e67df\n | base64\nMWYyZDFlMmU2N2Rm\n\n\n\n\nSetting up secrets in Kubernetes\n\n\nTo create secrets in Kubernetes, create a \nsecret.yaml\n file and populate it with your secrets. These secrets will be used in your Kubernetes \ndeployment.yaml\n file.\n\n\nIt should look similar to the following:\n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secrets\ntype: Opaque\ndata:\n  # make sure the values are all base64 encoded\n  dynamodbid: someid\n  dynamodbsecret: somesecret\n  password: MWYyZDFlMmU2N2Rm\n  oauthclientid: someclientid\n  oauthclientsecret: someclientsecret\n  jwtprivatekey: c29tZWp3dHByaXZhdGVrZXk=\n\n\n\n\nCreate the secrets using \nkubectl create\n:\n\n\n$ kubectl create -f ./secret.yaml\n\n\n\n\nAdditional environment variables\n\n\nOther environment variables can also be customized for Screwdriver. For a full list, see the \ncustom-environment-variables.yaml\n file.\n\n\nDeploy Screwdriver\n\n\nYou can check out the \napi.yaml\n in the \nScrewdriver Kubernetes repo\n for service and deployment definitions to run the Screwdriver API.\n\n\nCreate a Service\n\n\nA Kubernetes Service is an abstraction which defines a set of Pods and is assigned a unique IP address which persists.\nFollow instructions in \nCreating a Service\n to set up your \nservice.yaml\n.\n\n\nIt should look like the Service in \napi.yaml\n.\n\n\nTo create your service, run the \nkubectl create\n command on your \nservice.yaml\n file:\n\n\n$ kubectl create -f service.yaml\n\n\n\n\nGet your Kubernetes token name\n\n\nKubernetes actually sets up your Kubernetes token by default. You will need this for your \ndeployment.yaml\n.\nKubectl can be used to see your \nKubernetes secrets\n.\n\n\nGet the \nDEFAULT_TOKEN_NAME\n, by running:\n\n\n$ kubectl get secrets\n\nNAME                      TYPE                                  DATA      AGE\ndefault-token-abc55       kubernetes.io/service-account-token   3         50d\n\n\n\n\nThe \nDEFAULT_TOKEN_NAME\n will be listed under \nName\n when the \nType\n is \nkubernetes.io/service-account-token\n.\n\n\nGet your URI\n\n\nYou will need to get the Load Balancer Ingress to set your \nURI\n in your \ndeployment.yaml\n.\n\n\nGet the \nLoadBalancer Ingress\n, by running:\n\n\n$ kubectl describe services sdapi\n\n\n\n\nCreate a Deployment\n\n\nA Deployment makes sure a specified number of pod \u201creplicas\u201d are running at any one time. If there are too many, it will kill some; if there are too few, it will start more. Follow instructions on the \nDeploying Applications\n page to create your \ndeployment.yaml\n.\n\n\nIt should look like the Deployment in \napi.yaml\n.\n\n\nDeploy\n\n\nFor a fresh deployment, run the \nkubectl create\n command on your \ndeployment.yaml\n file:\n\n\n$ kubectl create -f deployment.yaml\n\n\n\n\nView your pods\n\n\nA Kubernetes \npod\n is a group of containers, tied together for the purposes of administration and networking.\n\n\nTo view the pod created by the deployment, run:\n\n\n$ kubectl get pods\n\n\n\n\nTo view the stdout / stderr from a pod, run:\n\n\n$ kubectl logs \nPOD-NAME\n\n\n\n\n\nUpdate your OAuth Application\n\n\nYou will need to navigate back to your original OAuth Application that you used for your OAuth Client ID and Secret to update the URLs.\n\n\n\n\n\n\nNavigate to the \nOAuth applications\n page.\n\n\n\n\n\n\nClick on the application you created to get your OAuth Client ID and Secret.\n\n\n\n\n\n\nFill out the \nHomepage URL\n and \nAuthorization callback URL\n with your \nLoadBalancer Ingress\n.", 
            "title": "Set Up a Screwdriver Cluster"
        }, 
        {
            "location": "/getting-started/cluster-setup/#setting-up-a-screwdriver-cluster-on-aws-using-kubernetes", 
            "text": "You can setup a Screwdriver cluster using  Kubernetes .", 
            "title": "Setting Up a Screwdriver Cluster on AWS using Kubernetes"
        }, 
        {
            "location": "/getting-started/cluster-setup/#screwdriver-cluster", 
            "text": "A Screwdriver cluster consists of a Kubernetes cluster running the Screwdriver API. The Screwdriver API modifies Screwdriver tables in DynamoDB.", 
            "title": "Screwdriver cluster"
        }, 
        {
            "location": "/getting-started/cluster-setup/#prerequisites", 
            "text": "kubectl  an  AWS  account  AWS CLI  Screwdriver tables on DynamoDB", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/getting-started/cluster-setup/#create-your-kubernetes-cluster", 
            "text": "Follow instructions at  Getting Started with AWS .", 
            "title": "Create your Kubernetes cluster"
        }, 
        {
            "location": "/getting-started/cluster-setup/#setup-kubernetes-secrets", 
            "text": "A  Secret  is an object that contains a small amount of sensitive data such as a password, a token, or a key. You will need to setup secrets for Screwdriver for your cluster to work properly.  First, you will need to gather some secrets:     Secret Key  Description      DATASTORE_DYNAMODB_ID  AWS Access Key ID    DATASTORE_DYNAMODB_SECRET  AWS Secret Access Key    SECRET_OAUTH_CLIENT_ID  The client ID used for  OAuth  with Github    SECRET_OAUTH_CLIENT_SECRET  The client secret used for OAuth with github    SECRET_JWT_PRIVATE_KEY  A private key used for signing JWT tokens. Can be anything    SECRET_PASSWORD  A password used for encrypting session, and OAuth data. Can be anything.  Needs to be minimum 32 characters     Here are some directions for getting your secrets:", 
            "title": "Setup Kubernetes secrets"
        }, 
        {
            "location": "/getting-started/cluster-setup/#get-your-dynamodb-secrets", 
            "text": "To get your  accessKeyId  and  secretAccessKey :    Navigate to  IAM  in your AWS console    Click on Users, and create a Screwdriver user. We recommend installing using an account which has read/write access to AWS DynamoDB.  For information on how to create an AWS DynamoDB policy, see  Policy Examples .    Select the  Security Credentials  tab    Click  Create Access Key    Download the file and keep note of those values for your  dynamodbid  and  dynamodbsecret .", 
            "title": "Get your DynamoDB secrets"
        }, 
        {
            "location": "/getting-started/cluster-setup/#get-your-oauth-client-id-and-secret", 
            "text": "Navigate to the  OAuth applications  page.    Click Register a new application.    Fill out the information and click Register application.     You should see a  Client ID  and  Client Secret , which will be used for your  oauthclientid  and  oauthclientsecret , respectively.", 
            "title": "Get your OAuth Client ID and Secret"
        }, 
        {
            "location": "/getting-started/cluster-setup/#base64-encode-your-secrets", 
            "text": "Each secret must be  base64 encoded . You must base64 encode each of your secrets:  $ echo -n  somejwtprivatekey  | base64\nc29tZWp3dHByaXZhdGVrZXk=\n$ echo -n  1f2d1e2e67df  | base64\nMWYyZDFlMmU2N2Rm", 
            "title": "Base64 encode your secrets"
        }, 
        {
            "location": "/getting-started/cluster-setup/#setting-up-secrets-in-kubernetes", 
            "text": "To create secrets in Kubernetes, create a  secret.yaml  file and populate it with your secrets. These secrets will be used in your Kubernetes  deployment.yaml  file.  It should look similar to the following:  apiVersion: v1\nkind: Secret\nmetadata:\n  name: secrets\ntype: Opaque\ndata:\n  # make sure the values are all base64 encoded\n  dynamodbid: someid\n  dynamodbsecret: somesecret\n  password: MWYyZDFlMmU2N2Rm\n  oauthclientid: someclientid\n  oauthclientsecret: someclientsecret\n  jwtprivatekey: c29tZWp3dHByaXZhdGVrZXk=  Create the secrets using  kubectl create :  $ kubectl create -f ./secret.yaml", 
            "title": "Setting up secrets in Kubernetes"
        }, 
        {
            "location": "/getting-started/cluster-setup/#additional-environment-variables", 
            "text": "Other environment variables can also be customized for Screwdriver. For a full list, see the  custom-environment-variables.yaml  file.", 
            "title": "Additional environment variables"
        }, 
        {
            "location": "/getting-started/cluster-setup/#deploy-screwdriver", 
            "text": "You can check out the  api.yaml  in the  Screwdriver Kubernetes repo  for service and deployment definitions to run the Screwdriver API.", 
            "title": "Deploy Screwdriver"
        }, 
        {
            "location": "/getting-started/cluster-setup/#create-a-service", 
            "text": "A Kubernetes Service is an abstraction which defines a set of Pods and is assigned a unique IP address which persists.\nFollow instructions in  Creating a Service  to set up your  service.yaml .  It should look like the Service in  api.yaml .  To create your service, run the  kubectl create  command on your  service.yaml  file:  $ kubectl create -f service.yaml", 
            "title": "Create a Service"
        }, 
        {
            "location": "/getting-started/cluster-setup/#get-your-kubernetes-token-name", 
            "text": "Kubernetes actually sets up your Kubernetes token by default. You will need this for your  deployment.yaml .\nKubectl can be used to see your  Kubernetes secrets .  Get the  DEFAULT_TOKEN_NAME , by running:  $ kubectl get secrets\n\nNAME                      TYPE                                  DATA      AGE\ndefault-token-abc55       kubernetes.io/service-account-token   3         50d  The  DEFAULT_TOKEN_NAME  will be listed under  Name  when the  Type  is  kubernetes.io/service-account-token .", 
            "title": "Get your Kubernetes token name"
        }, 
        {
            "location": "/getting-started/cluster-setup/#get-your-uri", 
            "text": "You will need to get the Load Balancer Ingress to set your  URI  in your  deployment.yaml .  Get the  LoadBalancer Ingress , by running:  $ kubectl describe services sdapi", 
            "title": "Get your URI"
        }, 
        {
            "location": "/getting-started/cluster-setup/#create-a-deployment", 
            "text": "A Deployment makes sure a specified number of pod \u201creplicas\u201d are running at any one time. If there are too many, it will kill some; if there are too few, it will start more. Follow instructions on the  Deploying Applications  page to create your  deployment.yaml .  It should look like the Deployment in  api.yaml .", 
            "title": "Create a Deployment"
        }, 
        {
            "location": "/getting-started/cluster-setup/#deploy", 
            "text": "For a fresh deployment, run the  kubectl create  command on your  deployment.yaml  file:  $ kubectl create -f deployment.yaml", 
            "title": "Deploy"
        }, 
        {
            "location": "/getting-started/cluster-setup/#view-your-pods", 
            "text": "A Kubernetes  pod  is a group of containers, tied together for the purposes of administration and networking.  To view the pod created by the deployment, run:  $ kubectl get pods  To view the stdout / stderr from a pod, run:  $ kubectl logs  POD-NAME", 
            "title": "View your pods"
        }, 
        {
            "location": "/getting-started/cluster-setup/#update-your-oauth-application", 
            "text": "You will need to navigate back to your original OAuth Application that you used for your OAuth Client ID and Secret to update the URLs.    Navigate to the  OAuth applications  page.    Click on the application you created to get your OAuth Client ID and Secret.    Fill out the  Homepage URL  and  Authorization callback URL  with your  LoadBalancer Ingress .", 
            "title": "Update your OAuth Application"
        }, 
        {
            "location": "/getting-started/datastore-setup/", 
            "text": "Setting Up a Datastore\n\n\nScrewdriver can be configured to store data in \nDynamoDB\n.\n\n\nDynamoDB\n\n\nSetting up AWS Credentials\n\n\nTo setup Screwdriver to use DynamoDB as the datastore, you'll need setup your \nAWS credentials\n from \nAWS\n. If you already have, skip to the next step.\n\n\nCreating the credentials file\n\n\nCreate a credentials file at \n~/.aws/credentials\n on Mac/Linux.\n\n\n[default]\naws_access_key_id = {YOUR_ACCESS_KEY_ID}\naws_secret_access_key = {YOUR_SECRET_ACCESS_KEY}\n\n\n\n\nSubstitute your own AWS credentials values for \n{YOUR_ACCESS_KEY_ID}\n and \n{YOUR_SECRET_ACCESS_KEY}\n.\n\n\nCreate Screwdriver tables in DynamoDB\n\n\nInstall dynamic-dynamodb\n\n\nDynamic-dynamodb\n is a utility CLI for creating Screwdriver datastore tables in DynamoDB\n\n\n$ npm install -g screwdriver-dynamic-dynamodb\n\n\n\n\nCreate Screwdriver tables\n\n\nDynamic-dynamodb CLI will create tables \nbuilds\n, \njobs\n, \npipelines\n, \nusers\n in DynamoDB for you.\nPick a region that is near your location for the best performance (Screwdriver default region is \nus-west-2\n). Depending on what region you are in, run the appropriate command below:\n\n\n\n\nFor \nus-west-2\n region\n\n\n\n\n$ screwdriver-db-setup create\n\n\n\n\n\n\nIn a specific region (ex: Ireland)\n\n\n\n\n$ screwdriver-db-setup --region eu-west-1 create\n\n\n\n\nViewing your Screwdriver tables\n\n\nTo see your newly created Screwdriver tables, navigate to the DynamoDB service. If you click on tables, you should see something similar to this:\n\n\n\n\nNote: In the upper right corner you can select the region where the table will be created. Select the region you specified when creating Screwdriver tables above.", 
            "title": "Set Up a Datastore"
        }, 
        {
            "location": "/getting-started/datastore-setup/#setting-up-a-datastore", 
            "text": "Screwdriver can be configured to store data in  DynamoDB .", 
            "title": "Setting Up a Datastore"
        }, 
        {
            "location": "/getting-started/datastore-setup/#dynamodb", 
            "text": "", 
            "title": "DynamoDB"
        }, 
        {
            "location": "/getting-started/datastore-setup/#setting-up-aws-credentials", 
            "text": "To setup Screwdriver to use DynamoDB as the datastore, you'll need setup your  AWS credentials  from  AWS . If you already have, skip to the next step.", 
            "title": "Setting up AWS Credentials"
        }, 
        {
            "location": "/getting-started/datastore-setup/#creating-the-credentials-file", 
            "text": "Create a credentials file at  ~/.aws/credentials  on Mac/Linux.  [default]\naws_access_key_id = {YOUR_ACCESS_KEY_ID}\naws_secret_access_key = {YOUR_SECRET_ACCESS_KEY}  Substitute your own AWS credentials values for  {YOUR_ACCESS_KEY_ID}  and  {YOUR_SECRET_ACCESS_KEY} .", 
            "title": "Creating the credentials file"
        }, 
        {
            "location": "/getting-started/datastore-setup/#create-screwdriver-tables-in-dynamodb", 
            "text": "", 
            "title": "Create Screwdriver tables in DynamoDB"
        }, 
        {
            "location": "/getting-started/datastore-setup/#install-dynamic-dynamodb", 
            "text": "Dynamic-dynamodb  is a utility CLI for creating Screwdriver datastore tables in DynamoDB  $ npm install -g screwdriver-dynamic-dynamodb", 
            "title": "Install dynamic-dynamodb"
        }, 
        {
            "location": "/getting-started/datastore-setup/#create-screwdriver-tables", 
            "text": "Dynamic-dynamodb CLI will create tables  builds ,  jobs ,  pipelines ,  users  in DynamoDB for you.\nPick a region that is near your location for the best performance (Screwdriver default region is  us-west-2 ). Depending on what region you are in, run the appropriate command below:   For  us-west-2  region   $ screwdriver-db-setup create   In a specific region (ex: Ireland)   $ screwdriver-db-setup --region eu-west-1 create", 
            "title": "Create Screwdriver tables"
        }, 
        {
            "location": "/getting-started/datastore-setup/#viewing-your-screwdriver-tables", 
            "text": "To see your newly created Screwdriver tables, navigate to the DynamoDB service. If you click on tables, you should see something similar to this:   Note: In the upper right corner you can select the region where the table will be created. Select the region you specified when creating Screwdriver tables above.", 
            "title": "Viewing your Screwdriver tables"
        }, 
        {
            "location": "/getting-started/create-pipeline/", 
            "text": "Creating a Pipeline\n\n\nUsing the Screwdriver CLI, you can build pipelines locally on your machine.\n\n\nSet up Screwdriver CLI\n\n\nThe Screwdriver CLI does not exist yet, but here is what the instructions would look like.\n\n\nInstalling Go\n\n\nGo\n v1.6 or higher is required. To find out how to setup your Go environment, follow instructions in their \nGetting Started\n page.\n\n\nInstalling the CLI\n\n\n$ go get github.com/screwdriver-cd/client\n$ $GOPATH/bin/client\nexport PATH=$PATH:$GOPATH/bin\n\n\n\n\nConfigure your Git token\n\n\nGet the URL to login to Screwdriver\n\n\n$ sd login  # prompts you to visit http://the-api-url.us-west-2.elb.amazonaws.com/v3/login\n\n\n\n\nCopy your token and paste it\n\n\n$ sd config token {YOUR_TOKEN_HERE}\n\n\n\n\nUsing the Screwdriver CLI\n\n\nNow that the Screwdriver CLI is set up, you can use it to create a pipeline. Creating a pipeline will automatically create a \nmain\n job and start a build.\n\n\nStart the CLI\n\n\nTo enter the read-eval-print loop, run:\n\n\n$ sd repl\n\n\n\n\nCreate a Pipeline\n\n\nTo create a pipeline, you will need your \nscmUrl\n.\n\n\nsd$ pipelines create {YOUR_SCMURL}  # (ex: git@github.com:screwdriver-cd/hashr.git#master)\n\n\n\n\nCongratulations! You just created your first pipeline!\n\n\nViewing your Pipeline\n\n\nTo see your pipeline, run:\n\n\nsd$ pipelines list\nsd$ jobs list\nsd$ builds list\n\n\n\n\nSee the logs at:\n\n\nsd$ builds log {BUILD_ID}\n\n\n\n\nFor more information on how to use the Screwdriver CLI, type:\n\n\nsd$ help", 
            "title": "Create a Pipeline"
        }, 
        {
            "location": "/getting-started/create-pipeline/#creating-a-pipeline", 
            "text": "Using the Screwdriver CLI, you can build pipelines locally on your machine.", 
            "title": "Creating a Pipeline"
        }, 
        {
            "location": "/getting-started/create-pipeline/#set-up-screwdriver-cli", 
            "text": "The Screwdriver CLI does not exist yet, but here is what the instructions would look like.", 
            "title": "Set up Screwdriver CLI"
        }, 
        {
            "location": "/getting-started/create-pipeline/#installing-go", 
            "text": "Go  v1.6 or higher is required. To find out how to setup your Go environment, follow instructions in their  Getting Started  page.", 
            "title": "Installing Go"
        }, 
        {
            "location": "/getting-started/create-pipeline/#installing-the-cli", 
            "text": "$ go get github.com/screwdriver-cd/client\n$ $GOPATH/bin/client\nexport PATH=$PATH:$GOPATH/bin", 
            "title": "Installing the CLI"
        }, 
        {
            "location": "/getting-started/create-pipeline/#configure-your-git-token", 
            "text": "Get the URL to login to Screwdriver  $ sd login  # prompts you to visit http://the-api-url.us-west-2.elb.amazonaws.com/v3/login  Copy your token and paste it  $ sd config token {YOUR_TOKEN_HERE}", 
            "title": "Configure your Git token"
        }, 
        {
            "location": "/getting-started/create-pipeline/#using-the-screwdriver-cli", 
            "text": "Now that the Screwdriver CLI is set up, you can use it to create a pipeline. Creating a pipeline will automatically create a  main  job and start a build.", 
            "title": "Using the Screwdriver CLI"
        }, 
        {
            "location": "/getting-started/create-pipeline/#start-the-cli", 
            "text": "To enter the read-eval-print loop, run:  $ sd repl", 
            "title": "Start the CLI"
        }, 
        {
            "location": "/getting-started/create-pipeline/#create-a-pipeline", 
            "text": "To create a pipeline, you will need your  scmUrl .  sd$ pipelines create {YOUR_SCMURL}  # (ex: git@github.com:screwdriver-cd/hashr.git#master)  Congratulations! You just created your first pipeline!", 
            "title": "Create a Pipeline"
        }, 
        {
            "location": "/getting-started/create-pipeline/#viewing-your-pipeline", 
            "text": "To see your pipeline, run:  sd$ pipelines list\nsd$ jobs list\nsd$ builds list  See the logs at:  sd$ builds log {BUILD_ID}  For more information on how to use the Screwdriver CLI, type:  sd$ help", 
            "title": "Viewing your Pipeline"
        }, 
        {
            "location": "/getting-started/yaml/", 
            "text": "Yaml Configuration\n\n\nThis is an interactive guide for exploring various important properties of the screwdriver.yaml configuration for projects.\n\n\nYou can access information about properties by hovering over the property name.\n\n\n\n\n\n\n\nworkflow\n:\n    - \npublish\n\n    - \nparallel\n:\n        - \nseries\n:\n            - \ndeploy-east\n\n            - \nvalidate-east\n\n        - \nseries\n:\n            - \ndeploy-west\n\n            - \nvalidate-west\n\n\nshared\n:\n\n    \nenvironment\n:\n        \nNODE_ENV\n: \ntest\n\n\njobs\n:\n\n    \nmain\n:\n\n        \nimage\n: \nnode:{{NODE_VERSION}}\n\n        \nmatrix\n:\n    \nNODE_VERSION\n: \n[4,5,6]\n\n        \nsteps\n:\n    - \ninit\n: \nnpm install\n\n    - \ntest\n: \nnpm test\n\n    \npublish\n:\n    \nimage\n: \nnode:6\n\n    \nsteps\n:\n        - \npublish\n: \nnpm publish\n\n    \ndeploy-west\n:\n    \nimage\n: \nnode:6\n\n    \nenvironment\n:\n        \nDEPLOY_ENV\n: \nwest\n\n    \nsteps\n:\n        - \ninit\n: \nnpm install\n\n        - \npublish\n: \nnpm deploy\n\n    \n...\n\n\n\n    \n\n        \n\n            \nWorkflow\n\n            \nDefines the order of jobs that are executed for the project. All jobs referenced by the workflow must be defined in the jobs section.\n\n            \nJobs can execute in parallel, in series, or in any combination of the two, per this example. Special keywords \nparallel\n and \nseries\n define the flow of the jobs. By default, the jobs in the workflow list are run in series after \nmain\n job has completed successfully.\n\n        \n\n        \n\n            \nShared\n\n            \nDefines a global configuration that applies to all jobs. Shared configurations are merged with each job, but may be overridden by more specific configuration in a specific job.\n\n        \n\n        \n\n            \nEnvironment\n\n            \nA set of key/value pairs for environment variables that need to be set. Any configuration that is valid for a job configuration is valid in shared, but will be overridden by specific job configurations.\n\n        \n\n        \n\n            \nJobs\n\n            \nA series of jobs that define the behavior of your builds.\n\n        \n\n        \n\n            \nMain\n\n            \nThe only required job. This job is executed automatically whenever there is a code change.\n\n        \n\n        \n\n            \nImage\n\n            \nThis defines the docker image(s) used for the builds. This example shows a template replacement, where a variable is enclosed in curly braces, e.g. {{NODE_VERSION}}. This variable will be changed to the value(s) of the equivalent variable in the matrix setting, resulting in multiple builds running in parallel, each using one of those various images.\n\n        \n\n        \n\n            \nMatrix\n\n            \nThis causes the builds for the job to execute on multiple images in parallel, when used a templated image configuration.\n\n        \n\n        \n\n            \nSteps\n\n            \nDefines the explicit list of commands that are executed in the build, just as if they were entered on the command line. Step definitions are required for all jobs.", 
            "title": "Config File Reference"
        }, 
        {
            "location": "/getting-started/yaml/#yaml-configuration", 
            "text": "This is an interactive guide for exploring various important properties of the screwdriver.yaml configuration for projects.  You can access information about properties by hovering over the property name.    workflow :\n    -  publish \n    -  parallel :\n        -  series :\n            -  deploy-east \n            -  validate-east \n        -  series :\n            -  deploy-west \n            -  validate-west  shared : \n     environment :\n         NODE_ENV :  test  jobs : \n     main : \n         image :  node:{{NODE_VERSION}} \n         matrix :\n     NODE_VERSION :  [4,5,6] \n         steps :\n    -  init :  npm install \n    -  test :  npm test \n     publish :\n     image :  node:6 \n     steps :\n        -  publish :  npm publish \n     deploy-west :\n     image :  node:6 \n     environment :\n         DEPLOY_ENV :  west \n     steps :\n        -  init :  npm install \n        -  publish :  npm deploy \n     ...", 
            "title": "Yaml Configuration"
        }, 
        {
            "location": "/getting-started/secrets/", 
            "text": "Build Secrets\n\n\nYou've got secrets to share with your jobs, but these shouldn't be shared with everyone. Screwdriver provides a mechanism to insert secrets as environment variables. Since secrets are exposed as environment variables, they are easy to use inside builds.\n\n\nSecurity\n\n\nThe Screwdriver team takes security very seriously and encrypts all traffic between its various services. User secrets are stored encrypted in our datastore, and their values are only released to those builds that are authorized by the user.\n\n\nWe understand that you, the security-conscious pipeline admin, may not wish to put secrets into pull-request builds, as a malicious pull-requester could expose those secrets without your consent, but still need those secrets as part of the main build. Screwdriver provides an additional flag on secrets, \nallowInPR\n (default: false), that is required to be enabled for a secret to be exposed.\n\n\nSecrets may only be added, modified, or removed by people that are listed as admins of the Git repository associated with a given pipeline. People with \"push\" privileges may also fetch a list of secrets, but not the secret values.\n\n\nConfiguring a job to expose secrets\n\n\nA list of allowed secrets are added to your pipeline configuration. Secret keys may only contain A-Z and underscore characters ( _ ) and must start with a letter.\n\n\nIn the below example, an \nNPM_TOKEN\n secret is added to the \npublish\n job:\n\n\npublish:\n    steps:\n        - publish-npm: npm publish\n    secrets:\n        # Publishing to NPM\n        - NPM_TOKEN\n\n\n\n\nYou may add secrets to any jobs you wish.\n\n\nSecrets in pull-requests\n\n\nFor your own security, we don't recommend exposing secrets to pull-request builds. Pull-request jobs can be created with modified \nscrewdriver.yaml\n files including changes to the secrets configuration.\n\n\nPull-requests operate essentially as copies of the \nmain\n job. The \nmain\n job can be set up to use a secret, and does not expose that secret to pull-requests by default.\n\n\nmain:\n    steps:\n        - my-step: maybeDoSomethingWithASecret.sh\n    secrets:\n        - MY_SECRET\n\n\n\n\nWhen a secret is created via the UI, or API, enabling \nallowInPR\n will cause that secret to be available to pull-request builds, if those secrets are also configured to be exposed in the \nmain\n job.\n\n\nSecrets UI\n\n\nThe easiest way to create a secret for your pipeline is via the Screwdriver UI.\n\n\n\nCreating Secrets\n\n\nSimply enter the key and value in the inputs in the grey box, and click the add button. A checkbox is provided to allow you to enable \nallowInPR\n.\n\n\nUpdating Secrets\n\n\nA secret's original value is never delivered to the UI, but values of secrets may be updated in the UI by adding a new value in the text field next to the appropriate key name and clicking the update button.\n\n\nDeleting secrets\n\n\nIndividual secrets may be removed by clicking the Delete button.\n\n\nScrewdriver API\n\n\nCreating secrets\n\n\nSend a POST request to the screwdriver api server with appropriate security token headers:\n\n\nPOST /secrets\n\n\n\n\nPayload:\n\n\n{\n    \npipelineId\n: \nabcd\n,\n    \nname\n: \nMY_SECRET_NAME\n,\n    \nvalue\n: \nMY_SECRET_VALUE!\n,\n    \nallowInPR\n: false\n}\n\n\n\n\nFetching Secrets\n\n\nSend a GET request to the screwdriver api server with appropriate security token headers:\n\n\nGET /pipeline/:id/secrets\n\n\n\n\nUpdating Secrets\n\n\nSend a PUT request to the screwdriver api server with appropriate security token headers:\n\n\nPUT /secrets/:id\n\n\n\n\nPayload:\n\n\n{\n    \nvalue\n: \nMY_SECRET_VALUE!\n,\n    \nallowInPR\n: false\n}\n\n\n\n\nDeleting Secrets\n\n\nSend a DELETE request to the screwdriver api server with appropriate security token headers:\n\n\nDELETE /secrets/:id", 
            "title": "Build Secrets"
        }, 
        {
            "location": "/getting-started/secrets/#build-secrets", 
            "text": "You've got secrets to share with your jobs, but these shouldn't be shared with everyone. Screwdriver provides a mechanism to insert secrets as environment variables. Since secrets are exposed as environment variables, they are easy to use inside builds.", 
            "title": "Build Secrets"
        }, 
        {
            "location": "/getting-started/secrets/#security", 
            "text": "The Screwdriver team takes security very seriously and encrypts all traffic between its various services. User secrets are stored encrypted in our datastore, and their values are only released to those builds that are authorized by the user.  We understand that you, the security-conscious pipeline admin, may not wish to put secrets into pull-request builds, as a malicious pull-requester could expose those secrets without your consent, but still need those secrets as part of the main build. Screwdriver provides an additional flag on secrets,  allowInPR  (default: false), that is required to be enabled for a secret to be exposed.  Secrets may only be added, modified, or removed by people that are listed as admins of the Git repository associated with a given pipeline. People with \"push\" privileges may also fetch a list of secrets, but not the secret values.", 
            "title": "Security"
        }, 
        {
            "location": "/getting-started/secrets/#configuring-a-job-to-expose-secrets", 
            "text": "A list of allowed secrets are added to your pipeline configuration. Secret keys may only contain A-Z and underscore characters ( _ ) and must start with a letter.  In the below example, an  NPM_TOKEN  secret is added to the  publish  job:  publish:\n    steps:\n        - publish-npm: npm publish\n    secrets:\n        # Publishing to NPM\n        - NPM_TOKEN  You may add secrets to any jobs you wish.", 
            "title": "Configuring a job to expose secrets"
        }, 
        {
            "location": "/getting-started/secrets/#secrets-in-pull-requests", 
            "text": "For your own security, we don't recommend exposing secrets to pull-request builds. Pull-request jobs can be created with modified  screwdriver.yaml  files including changes to the secrets configuration.  Pull-requests operate essentially as copies of the  main  job. The  main  job can be set up to use a secret, and does not expose that secret to pull-requests by default.  main:\n    steps:\n        - my-step: maybeDoSomethingWithASecret.sh\n    secrets:\n        - MY_SECRET  When a secret is created via the UI, or API, enabling  allowInPR  will cause that secret to be available to pull-request builds, if those secrets are also configured to be exposed in the  main  job.", 
            "title": "Secrets in pull-requests"
        }, 
        {
            "location": "/getting-started/secrets/#secrets-ui", 
            "text": "The easiest way to create a secret for your pipeline is via the Screwdriver UI.", 
            "title": "Secrets UI"
        }, 
        {
            "location": "/getting-started/secrets/#creating-secrets", 
            "text": "Simply enter the key and value in the inputs in the grey box, and click the add button. A checkbox is provided to allow you to enable  allowInPR .", 
            "title": "Creating Secrets"
        }, 
        {
            "location": "/getting-started/secrets/#updating-secrets", 
            "text": "A secret's original value is never delivered to the UI, but values of secrets may be updated in the UI by adding a new value in the text field next to the appropriate key name and clicking the update button.", 
            "title": "Updating Secrets"
        }, 
        {
            "location": "/getting-started/secrets/#deleting-secrets", 
            "text": "Individual secrets may be removed by clicking the Delete button.", 
            "title": "Deleting secrets"
        }, 
        {
            "location": "/getting-started/secrets/#screwdriver-api", 
            "text": "", 
            "title": "Screwdriver API"
        }, 
        {
            "location": "/getting-started/secrets/#creating-secrets_1", 
            "text": "Send a POST request to the screwdriver api server with appropriate security token headers:  POST /secrets  Payload:  {\n     pipelineId :  abcd ,\n     name :  MY_SECRET_NAME ,\n     value :  MY_SECRET_VALUE! ,\n     allowInPR : false\n}", 
            "title": "Creating secrets"
        }, 
        {
            "location": "/getting-started/secrets/#fetching-secrets", 
            "text": "Send a GET request to the screwdriver api server with appropriate security token headers:  GET /pipeline/:id/secrets", 
            "title": "Fetching Secrets"
        }, 
        {
            "location": "/getting-started/secrets/#updating-secrets_1", 
            "text": "Send a PUT request to the screwdriver api server with appropriate security token headers:  PUT /secrets/:id  Payload:  {\n     value :  MY_SECRET_VALUE! ,\n     allowInPR : false\n}", 
            "title": "Updating Secrets"
        }, 
        {
            "location": "/getting-started/secrets/#deleting-secrets_1", 
            "text": "Send a DELETE request to the screwdriver api server with appropriate security token headers:  DELETE /secrets/:id", 
            "title": "Deleting Secrets"
        }, 
        {
            "location": "/internals/overall-architecture/", 
            "text": "Overall Architecture\n\n\nScrewdriver is a collection of services that facilitate the workflow for\nContinuous Delivery pipelines.\n\n\n\n\nWorkflow\n\n\n\n\n\n\nCommit new code\n\n\nUser starts a new build by one of the following operations:\n\n\n\n\nUser pushes code to GitHub\n\n\nUser opens a new pull request on GitHub\n\n\nUser pushes code to GitHub on an open pull request\n\n\nUser tells Screwdriver (via API or UI) to rebuild a given commit\n\n\n\n\n\n\n\n\nNotify Screwdriver\n\n\nSigned \nwebhooks\n notify\nScrewdriver's API about the change.\n\n\n\n\n\n\nTrigger execution engine\n\n\nScrewdriver starts a job on the specified execution engine passing the\nuser's configuration and git information.\n\n\n\n\n\n\nBuild software\n\n\nScrewdriver's Launcher executes the commands specified in the user's\nconfiguration after checking out the source code from git inside the\ndesired container.\n\n\n\n\n\n\nPublish artifacts\n \n(optional)\n\n\nUser can optionally push produced artifacts to respective artifact\nrepositories (RPMs, Docker images, Node Modules, etc.).\n\n\n\n\n\n\nContinue pipeline\n\n\nOn completion of the job, Screwdriver's Launcher notifies the API and\nif there's more in the pipeline, the API triggers the next job on the\nexecution engine (\nGOTO:3\n).\n\n\n\n\n\n\nComponents\n\n\nScrewdriver consists of five main components, the first three of which are\nbuilt/maintained by Screwdriver:\n\n\n\n\n\n\nREST API\n\n\nRESTful interface for creating, monitoring, and interacting with pipelines.\n\n\n\n\n\n\nWeb UI\n\n\nHuman consumable interface for the \nREST API\n.\n\n\n\n\n\n\nLauncher\n\n\nSelf-contained tool to clone, setup the environment, and execute the\nshell commands defined in your job.\n\n\n\n\n\n\nExecution Engine\n\n\nPluggable build executor that supports executing commands inside of a\ncontainer (e.g. Jenkins, Kubernetes, and Mesos).\n\n\n\n\n\n\nDatastore\n\n\nPluggable NoSQL-based storage for keeping information about pipelines\n(e.g. DynamoDB, MongoDB, and CouchDB).", 
            "title": "Overall Architecture"
        }, 
        {
            "location": "/internals/overall-architecture/#overall-architecture", 
            "text": "Screwdriver is a collection of services that facilitate the workflow for\nContinuous Delivery pipelines.", 
            "title": "Overall Architecture"
        }, 
        {
            "location": "/internals/overall-architecture/#workflow", 
            "text": "Commit new code  User starts a new build by one of the following operations:   User pushes code to GitHub  User opens a new pull request on GitHub  User pushes code to GitHub on an open pull request  User tells Screwdriver (via API or UI) to rebuild a given commit     Notify Screwdriver  Signed  webhooks  notify\nScrewdriver's API about the change.    Trigger execution engine  Screwdriver starts a job on the specified execution engine passing the\nuser's configuration and git information.    Build software  Screwdriver's Launcher executes the commands specified in the user's\nconfiguration after checking out the source code from git inside the\ndesired container.    Publish artifacts   (optional)  User can optionally push produced artifacts to respective artifact\nrepositories (RPMs, Docker images, Node Modules, etc.).    Continue pipeline  On completion of the job, Screwdriver's Launcher notifies the API and\nif there's more in the pipeline, the API triggers the next job on the\nexecution engine ( GOTO:3 ).", 
            "title": "Workflow"
        }, 
        {
            "location": "/internals/overall-architecture/#components", 
            "text": "Screwdriver consists of five main components, the first three of which are\nbuilt/maintained by Screwdriver:    REST API  RESTful interface for creating, monitoring, and interacting with pipelines.    Web UI  Human consumable interface for the  REST API .    Launcher  Self-contained tool to clone, setup the environment, and execute the\nshell commands defined in your job.    Execution Engine  Pluggable build executor that supports executing commands inside of a\ncontainer (e.g. Jenkins, Kubernetes, and Mesos).    Datastore  Pluggable NoSQL-based storage for keeping information about pipelines\n(e.g. DynamoDB, MongoDB, and CouchDB).", 
            "title": "Components"
        }, 
        {
            "location": "/internals/api/", 
            "text": "API Design\n\n\nOur API was designed with three principles in mind:\n\n\n\n\nAll interactions of user's data should be done through the API, so that\nthere is a consistent interface across the tools (CLI, Web UI, etc.).\n\n\nResources should be ReST-ful and operations should be atomic so that intent\nis clear and human readable.\n\n\nAPI should be versioned and self-documented, so that client code generation\nis possible.\n\n\n\n\n\n\nVersion 3\n is the current API, all links should be prefixed with \n/v3\n\n\n\n\nAuthN and AuthZ\n\n\nFor Authentication we're using \nJSON Web Tokens\n. They need to be passed via\nan \nAuthorization\n header. Generating a JWT can be done by visiting our\n\n/login\n endpoint.\n\n\nAuthorization on the other hand is handled by \nGitHub OAuth\n. This occurs when\nyou visit the \n/login\n endpoint. Screwdriver uses the GitHub user tokens\nand identity to:\n\n\n\n\nidentify what repositories you have read, write, and admin access to\n\n\nread allows you to view the pipeline\n\n\nwrite allows you to start or stop jobs\n\n\nadmin allows you to create, edit, or delete pipelines\n\n\n\n\n\n\nread the repository's \nscrewdriver.yaml\n\n\nenumerate the list of pull-requests open on your repository\n\n\nupdate the pull-request with the success/failure of the build\n\n\nadd/remove repository web-hooks so Screwdriver can be notified on changes\n\n\n\n\nSwagger\n\n\nAll of our APIs and the data models around them are documented via \nSwagger\n.\nThis prevents out-of-date documentation, enables clients to be\nauto-generated, and most importantly exposes a human-readable interface.\n\n\nOur documentation is at: \n/documentation\n\n\nOur swagger is at: \n/swagger.json", 
            "title": "API"
        }, 
        {
            "location": "/internals/api/#api-design", 
            "text": "Our API was designed with three principles in mind:   All interactions of user's data should be done through the API, so that\nthere is a consistent interface across the tools (CLI, Web UI, etc.).  Resources should be ReST-ful and operations should be atomic so that intent\nis clear and human readable.  API should be versioned and self-documented, so that client code generation\nis possible.    Version 3  is the current API, all links should be prefixed with  /v3", 
            "title": "API Design"
        }, 
        {
            "location": "/internals/api/#authn-and-authz", 
            "text": "For Authentication we're using  JSON Web Tokens . They need to be passed via\nan  Authorization  header. Generating a JWT can be done by visiting our /login  endpoint.  Authorization on the other hand is handled by  GitHub OAuth . This occurs when\nyou visit the  /login  endpoint. Screwdriver uses the GitHub user tokens\nand identity to:   identify what repositories you have read, write, and admin access to  read allows you to view the pipeline  write allows you to start or stop jobs  admin allows you to create, edit, or delete pipelines    read the repository's  screwdriver.yaml  enumerate the list of pull-requests open on your repository  update the pull-request with the success/failure of the build  add/remove repository web-hooks so Screwdriver can be notified on changes", 
            "title": "AuthN and AuthZ"
        }, 
        {
            "location": "/internals/api/#swagger", 
            "text": "All of our APIs and the data models around them are documented via  Swagger .\nThis prevents out-of-date documentation, enables clients to be\nauto-generated, and most importantly exposes a human-readable interface.  Our documentation is at:  /documentation  Our swagger is at:  /swagger.json", 
            "title": "Swagger"
        }, 
        {
            "location": "/internals/execution-engines/", 
            "text": "Execution Engines\n\n\n\n\nA workload management system for the scheduling and running of jobs.\n\n\n\n\nThe typical enterprise requires support for multiple operating systems.  A core tenet of the project is to leverage both open source projects and commercial cloud products and their capabilities where we can.\nTODO: Tie back to targeted use cases and typical enterprise\n\n\nSupported environments\n\n\nTier 1:\n\n\n\n\nLinux and a typical use case of web serving and associated backend systems.  For the Open Source release we are evaluating a number of options as explained below.\n\n\n\n\nTier 2:\n\n\n\n\nMac OSX for iOS and desktop clients.  Windows may be supported in the future.  Jenkins will be a supported execution engine to handle scheduling across Mac OSX slaves.  Jenkins support across many OSes is useful here.\n\n\n\n\nWhy not Jenkins everywhere?  While Jenkins serves us well in other areas, it has issues scaling and limits overall performance with its architecture.  In addition, managing a Jenkins cluster has high operational overhead.  A downside to not using Jenkins is not having access to the existing plugin ecosystem.\n\n\nSelection Criteria\n\n\n\n\nAvailability outside of Yahoo\n\n\nEase of setup\n\n\nCommunity momentum (leverage industry innovation and future proof our solution)\n\n\nCapabilities (semi-persistent storage, scheduler options, etc)\n\n\nRun on-premise or in cloud (AWS or GCP)\n\n\nOperability\n\n\n\n\nCandidates\n\n\n\n\nKubernetes (also GCP's Container Engine)\n\n\nAmazon's ECS\n\n\nMesos\n\n\nDocker Swarm\n\n\n\n\nInitial analysis\n\n\n\n\nAmazon ECS and Kubernetes had easiest setup.  This allows users to play with the platform easily.  These also have low operational overhead (if using GCP Container Engine in Kubernetes' case) since they are cloud options.\n\n\nKubernetes allows us to have the same interface for on-premise use as well as a native supported interface in GCP.\n\n\nECS would limit us to Amazon and doesn't have an on-premise option.\n\n\nMesos is used internally at Yahoo.  Getting started is more difficult. Need more time to investigate frameworks.\n\n\nDocker Swarm is a candidate but is less mature than other options.  Something to keep an eye on.\n\n\n\n\nCapabilities analysis requires learning the underlying systems to a certain degree.  The evaluation process includes an end to end integration to understand integration points as well as the strength and weaknesses of the system.  Kubernetes was chosen for the first end to end integration.\n\n\nTODO: add results of evaluations", 
            "title": "Execution Engines"
        }, 
        {
            "location": "/internals/execution-engines/#execution-engines", 
            "text": "A workload management system for the scheduling and running of jobs.   The typical enterprise requires support for multiple operating systems.  A core tenet of the project is to leverage both open source projects and commercial cloud products and their capabilities where we can.\nTODO: Tie back to targeted use cases and typical enterprise", 
            "title": "Execution Engines"
        }, 
        {
            "location": "/internals/execution-engines/#supported-environments", 
            "text": "Tier 1:   Linux and a typical use case of web serving and associated backend systems.  For the Open Source release we are evaluating a number of options as explained below.   Tier 2:   Mac OSX for iOS and desktop clients.  Windows may be supported in the future.  Jenkins will be a supported execution engine to handle scheduling across Mac OSX slaves.  Jenkins support across many OSes is useful here.   Why not Jenkins everywhere?  While Jenkins serves us well in other areas, it has issues scaling and limits overall performance with its architecture.  In addition, managing a Jenkins cluster has high operational overhead.  A downside to not using Jenkins is not having access to the existing plugin ecosystem.", 
            "title": "Supported environments"
        }, 
        {
            "location": "/internals/execution-engines/#selection-criteria", 
            "text": "Availability outside of Yahoo  Ease of setup  Community momentum (leverage industry innovation and future proof our solution)  Capabilities (semi-persistent storage, scheduler options, etc)  Run on-premise or in cloud (AWS or GCP)  Operability", 
            "title": "Selection Criteria"
        }, 
        {
            "location": "/internals/execution-engines/#candidates", 
            "text": "Kubernetes (also GCP's Container Engine)  Amazon's ECS  Mesos  Docker Swarm", 
            "title": "Candidates"
        }, 
        {
            "location": "/internals/execution-engines/#initial-analysis", 
            "text": "Amazon ECS and Kubernetes had easiest setup.  This allows users to play with the platform easily.  These also have low operational overhead (if using GCP Container Engine in Kubernetes' case) since they are cloud options.  Kubernetes allows us to have the same interface for on-premise use as well as a native supported interface in GCP.  ECS would limit us to Amazon and doesn't have an on-premise option.  Mesos is used internally at Yahoo.  Getting started is more difficult. Need more time to investigate frameworks.  Docker Swarm is a candidate but is less mature than other options.  Something to keep an eye on.   Capabilities analysis requires learning the underlying systems to a certain degree.  The evaluation process includes an end to end integration to understand integration points as well as the strength and weaknesses of the system.  Kubernetes was chosen for the first end to end integration.  TODO: add results of evaluations", 
            "title": "Initial analysis"
        }, 
        {
            "location": "/internals/domain/", 
            "text": "Domain Model\n\n\n\n\n\n\nSource Code\n\n\nSource Code is a specified GitHub repository and branch that contains a \nscrewdriver.yaml\n and the code required to build, test, and publish your application.\n\n\nStep\n\n\nA step is a named action that needs to be performed, usually a single shell command. If the command finishes with a non-zero exit code, the step is considered a failure.\n\n\nContainer\n\n\nA container runs \nsteps\n in an isolated environment. This is done in order to test the compatibility of code running in different environments with different versions, without affecting other \nbuilds\n that may be running at the same time. This is implemented using Docker containers.\n\n\nJob\n\n\nA job consists of executing multiple sequential \nsteps\n inside a specified \ncontainer\n. If any step in the series fails, then the entire job is considered failed and subsequent steps will be skipped (unless configured otherwise).\n\n\nJobs work by checking out the \nsource code\n to a specified commit, setting the desired environment variables, and executing the specified \nsteps\n.\n\n\nDuring the job, the executing \nsteps\n share three pieces of context:\n\n\n\n\nFilesystem\n\n\nContainer\n\n\nMetadata\n\n\n\n\nJobs can be started automatically by changes made in the \nsource code\n or triggered through the \nworkflow\n. Jobs can also be started manually through the UI.\n\n\nParallelization\n\n\nIt is possible to parallelize a job by defining a matrix of environment variables. These are usually used for testing against multiple \ncontainers\n or test types.\n\n\nIn this example job definition, 4 \nbuilds\n will run in parallel:\n\n\nimage: node:{{NODE_VERSION}}\nsteps:\n    test: npm run test-${TEST_TYPE}\nmatrix:\n    NODE_VERSION:\n        - 4\n        - 6\n    TEST_TYPE:\n        - unit\n        - functional\n\n\n\n\n\n\nNODE_VERSION=4\n and \nTEST_TYPE=unit\n\n\nNODE_VERSION=4\n and \nTEST_TYPE=functional\n\n\nNODE_VERSION=6\n and \nTEST_TYPE=unit\n\n\nNODE_VERSION=6\n and \nTEST_TYPE=functional\n\n\n\n\nBuild\n\n\nA build is an instance of a running \njob\n. All builds are assigned a unique build number. With a basic job configuration, only one build of a job will be running at any given time. If a \njob matrix\n is configured, then there can be multiple builds running in parallel.\n\n\nA build can be in one of five different states:\n\n\n\n\nqueued\n - Build is waiting for available resources\n\n\nrunning\n - Build is actively running on an executor\n\n\nsuccess\n - All steps completed successfully\n\n\naborted\n - User canceled the running build\n\n\nfailure\n - One of the steps failed\n\n\n\n\nMetadata\n\n\nMetadata is a structured key/value storage of relevant information about a \nbuild\n. This is automatically populated with basic information like git SHA1 and start/stop time. It can be updated throughout the job by using the built-in CLI (\nmeta\n).\n\n\nExample:\n\n\n$ meta set meta.coverage 99.95\n$ meta get meta.coverage\n99.95\n$ meta get --json meta\n{\ncoverage\n:99.95}\n\n\n\n\nWorkflow\n\n\nWorkflow is the order that \njobs\n will execute in after a successful \nbuild\n of the \nmain\n job on the default branch. Jobs can be executed in parallel, series, or a combination of the two to allow for all possibilities. Workflow must contain all defined jobs in the pipeline.\n\n\nAll jobs executed in a given workflow share:\n\n\n\n\nSource code checked out from the same git commit\n\n\nAccess to \nmetadata\n from a \nmain\n build that triggered or was selected for this job's build\n\n\n\n\nIn the following example of a workflow section, this is the flow:\n\n\nworkflow:\n    - publish\n    - parallel:\n        - series:\n            - deploy-west\n            - validate-west\n        - series:\n            - deploy-east\n            - validate-east\n\n\n\n\nAfter the merge of a pull-request to master:\n\n\n\n\nmain\n will run and trigger \npublish\n\n\npublish\n will trigger \ndeploy-west\n and \ndeploy-east\n in parallel\n\n\ndeploy-west\n will trigger \nvalidate-west\n\n\ndeploy-east\n will trigger \nvalidate-east\n\n\n\n\nPipeline\n\n\nA pipeline represents a collection of \njobs\n that share the same \nsource code\n. These jobs are executed in the order defined by the \nworkflow\n.\n\n\nThe \nmain\n job is required to be defined in every pipeline as it is the one that builds for each change made to the source code (and proposed changes).", 
            "title": "Domain Model"
        }, 
        {
            "location": "/internals/domain/#domain-model", 
            "text": "", 
            "title": "Domain Model"
        }, 
        {
            "location": "/internals/domain/#source-code", 
            "text": "Source Code is a specified GitHub repository and branch that contains a  screwdriver.yaml  and the code required to build, test, and publish your application.", 
            "title": "Source Code"
        }, 
        {
            "location": "/internals/domain/#step", 
            "text": "A step is a named action that needs to be performed, usually a single shell command. If the command finishes with a non-zero exit code, the step is considered a failure.", 
            "title": "Step"
        }, 
        {
            "location": "/internals/domain/#container", 
            "text": "A container runs  steps  in an isolated environment. This is done in order to test the compatibility of code running in different environments with different versions, without affecting other  builds  that may be running at the same time. This is implemented using Docker containers.", 
            "title": "Container"
        }, 
        {
            "location": "/internals/domain/#job", 
            "text": "A job consists of executing multiple sequential  steps  inside a specified  container . If any step in the series fails, then the entire job is considered failed and subsequent steps will be skipped (unless configured otherwise).  Jobs work by checking out the  source code  to a specified commit, setting the desired environment variables, and executing the specified  steps .  During the job, the executing  steps  share three pieces of context:   Filesystem  Container  Metadata   Jobs can be started automatically by changes made in the  source code  or triggered through the  workflow . Jobs can also be started manually through the UI.", 
            "title": "Job"
        }, 
        {
            "location": "/internals/domain/#parallelization", 
            "text": "It is possible to parallelize a job by defining a matrix of environment variables. These are usually used for testing against multiple  containers  or test types.  In this example job definition, 4  builds  will run in parallel:  image: node:{{NODE_VERSION}}\nsteps:\n    test: npm run test-${TEST_TYPE}\nmatrix:\n    NODE_VERSION:\n        - 4\n        - 6\n    TEST_TYPE:\n        - unit\n        - functional   NODE_VERSION=4  and  TEST_TYPE=unit  NODE_VERSION=4  and  TEST_TYPE=functional  NODE_VERSION=6  and  TEST_TYPE=unit  NODE_VERSION=6  and  TEST_TYPE=functional", 
            "title": "Parallelization"
        }, 
        {
            "location": "/internals/domain/#build", 
            "text": "A build is an instance of a running  job . All builds are assigned a unique build number. With a basic job configuration, only one build of a job will be running at any given time. If a  job matrix  is configured, then there can be multiple builds running in parallel.  A build can be in one of five different states:   queued  - Build is waiting for available resources  running  - Build is actively running on an executor  success  - All steps completed successfully  aborted  - User canceled the running build  failure  - One of the steps failed", 
            "title": "Build"
        }, 
        {
            "location": "/internals/domain/#metadata", 
            "text": "Metadata is a structured key/value storage of relevant information about a  build . This is automatically populated with basic information like git SHA1 and start/stop time. It can be updated throughout the job by using the built-in CLI ( meta ).  Example:  $ meta set meta.coverage 99.95\n$ meta get meta.coverage\n99.95\n$ meta get --json meta\n{ coverage :99.95}", 
            "title": "Metadata"
        }, 
        {
            "location": "/internals/domain/#workflow", 
            "text": "Workflow is the order that  jobs  will execute in after a successful  build  of the  main  job on the default branch. Jobs can be executed in parallel, series, or a combination of the two to allow for all possibilities. Workflow must contain all defined jobs in the pipeline.  All jobs executed in a given workflow share:   Source code checked out from the same git commit  Access to  metadata  from a  main  build that triggered or was selected for this job's build   In the following example of a workflow section, this is the flow:  workflow:\n    - publish\n    - parallel:\n        - series:\n            - deploy-west\n            - validate-west\n        - series:\n            - deploy-east\n            - validate-east  After the merge of a pull-request to master:   main  will run and trigger  publish  publish  will trigger  deploy-west  and  deploy-east  in parallel  deploy-west  will trigger  validate-west  deploy-east  will trigger  validate-east", 
            "title": "Workflow"
        }, 
        {
            "location": "/internals/domain/#pipeline", 
            "text": "A pipeline represents a collection of  jobs  that share the same  source code . These jobs are executed in the order defined by the  workflow .  The  main  job is required to be defined in every pipeline as it is the one that builds for each change made to the source code (and proposed changes).", 
            "title": "Pipeline"
        }, 
        {
            "location": "/community/contributing/", 
            "text": "Contributing\n\n\nThank you for considering contributing! There are many ways you can help.\n\n\nIssues\n\n\nFile an issue if you think you've found a bug. Be sure to describe\n\n\n\n\nHow can it be reproduced?\n\n\nWhat did you expect?\n\n\nWhat actually occurred?\n\n\nVersion, platform, etc. if possibly relevant.\n\n\n\n\nDocs\n\n\nDocumentation, READMEs, and examples are extremely important. Please help improve them and if you find a typo or notice a problem, please send a fix or say something.\n\n\nSubmitting Patches\n\n\nPatches for fixes, features, and improvements are accepted through pull requests.\n\n\n\n\nWrite good commit messages, in the present tense! (Add X, not Added X). Short title, blank line, bullet points if needed. Capitalize the first letter of the title or bullet item. No punctuation in the title.\n\n\nCode must pass lint and style checks.\n\n\nAll external methods must be documented.\n\n\nInclude tests to improve coverage and prevent regressions.\n\n\nSquash changes into a single commit per feature/fix. Ask if you're unsure how to discretize your work.\n\n\n\n\nPlease ask before embarking on a large improvement so you're not disappointed if it does not align with the goals of the project or owner(s).\n\n\nFeature Requests\n\n\nMake the case for a feature via an issue with a good title. The feature should be discussed and given a target inclusion milestone or closed.\n\n\nWhere to contribute\n\n\nScrewdriver has a modular architecture, and the various responsibilities are split up in separate repos.\n\n\nScrewdriver API\n \n \n\n\nThe \nscrewdriver\n repo is the core of screwdriver, providing the API endpoints for everything that screwdriver does. The API is based on the \nhapijs framework\n and is implemented in node as a series of plugins.\n\n\nLauncher\n \n\n\nThe \nlauncher\n performs step execution and housekeeping internal to build containers. This is written in Go, and mounted into build containers as a binary.\n\n\nExecutors\n\n\nAn executor is used to manage build containers for any given job. Several implementations of executors have been created. All are designed to follow a common interface. Executor implementations are written in node:\n\n\n\n\nexecutor-base\n: Common Interface \n \n\n\nexecutor-k8s\n: Kubernetes Implementation \n \n\n\nexecutor-j5s\n: Jenkins Implementation \n \n\n\n\n\nModels\n\n\nThe object models provide the definition of the data that is stored in data stores. This is done in two parts:\n\n\n\n\ndata-schema\n: Schema definition with \nJoi\n \n \n\n\nmodels\n: Specific business logic around the data schema \n \n\n\n\n\nDatastores\n\n\nA datastore implementation is used as the interface between the API and a data storage mechanism. There are several implementations written in node around a common interface.\n\n\n\n\ndatastore-base\n: Common Interface \n \n\n\ndatastore-dynamodb\n: DynamoDB Implementation \n \n\n\ndatastore-imdb\n: In-memory Implementation \n \n\n\n\n\nConfig Parser\n \n \n\n\nNode module for validating and parsing user's \nscrewdriver.yaml\n configurations.\n\n\nGuide\n \n\n\nThis documentation! Everything you ever hoped to know about the Screwdriver project.\n\n\nMiscellaneous Tools\n\n\n\n\nclient\n: Simple Go-based CLI for accessing the Screwdriver API \n\n\njob-tools\n: Generic docker container implementation to bootstrap and execute a build \n\n\ngitversion\n: Go-based tool for updating git tags on a repo for a new version number \n\n\ncircuit-fuses\n: Wrapper to provide a node-circuitbreaker w/ callback interface \n \n\n\nkeymbinatorial\n: Generates the unique combinations of key values by taking a single value from each keys array \n \n\n\n\n\nAdding a New Screwdriver Repo\n\n\nWe have some tools to help start out new repos for screwdriver:\n\n\n\n\ngenerator-screwdriver\n: Yeoman generator that bootstraps new repos for screwdriver\n\n\neslint-config-screwdriver\n: Our eslint rules for node-based code. Included in each new repo as part of the bootstrap process", 
            "title": "Contributing"
        }, 
        {
            "location": "/community/contributing/#contributing", 
            "text": "Thank you for considering contributing! There are many ways you can help.", 
            "title": "Contributing"
        }, 
        {
            "location": "/community/contributing/#issues", 
            "text": "File an issue if you think you've found a bug. Be sure to describe   How can it be reproduced?  What did you expect?  What actually occurred?  Version, platform, etc. if possibly relevant.", 
            "title": "Issues"
        }, 
        {
            "location": "/community/contributing/#docs", 
            "text": "Documentation, READMEs, and examples are extremely important. Please help improve them and if you find a typo or notice a problem, please send a fix or say something.", 
            "title": "Docs"
        }, 
        {
            "location": "/community/contributing/#submitting-patches", 
            "text": "Patches for fixes, features, and improvements are accepted through pull requests.   Write good commit messages, in the present tense! (Add X, not Added X). Short title, blank line, bullet points if needed. Capitalize the first letter of the title or bullet item. No punctuation in the title.  Code must pass lint and style checks.  All external methods must be documented.  Include tests to improve coverage and prevent regressions.  Squash changes into a single commit per feature/fix. Ask if you're unsure how to discretize your work.   Please ask before embarking on a large improvement so you're not disappointed if it does not align with the goals of the project or owner(s).", 
            "title": "Submitting Patches"
        }, 
        {
            "location": "/community/contributing/#feature-requests", 
            "text": "Make the case for a feature via an issue with a good title. The feature should be discussed and given a target inclusion milestone or closed.", 
            "title": "Feature Requests"
        }, 
        {
            "location": "/community/contributing/#where-to-contribute", 
            "text": "Screwdriver has a modular architecture, and the various responsibilities are split up in separate repos.", 
            "title": "Where to contribute"
        }, 
        {
            "location": "/community/contributing/#screwdriver-api", 
            "text": "The  screwdriver  repo is the core of screwdriver, providing the API endpoints for everything that screwdriver does. The API is based on the  hapijs framework  and is implemented in node as a series of plugins.", 
            "title": "Screwdriver API"
        }, 
        {
            "location": "/community/contributing/#launcher", 
            "text": "The  launcher  performs step execution and housekeeping internal to build containers. This is written in Go, and mounted into build containers as a binary.", 
            "title": "Launcher"
        }, 
        {
            "location": "/community/contributing/#executors", 
            "text": "An executor is used to manage build containers for any given job. Several implementations of executors have been created. All are designed to follow a common interface. Executor implementations are written in node:   executor-base : Common Interface     executor-k8s : Kubernetes Implementation     executor-j5s : Jenkins Implementation", 
            "title": "Executors"
        }, 
        {
            "location": "/community/contributing/#models", 
            "text": "The object models provide the definition of the data that is stored in data stores. This is done in two parts:   data-schema : Schema definition with  Joi      models : Specific business logic around the data schema", 
            "title": "Models"
        }, 
        {
            "location": "/community/contributing/#datastores", 
            "text": "A datastore implementation is used as the interface between the API and a data storage mechanism. There are several implementations written in node around a common interface.   datastore-base : Common Interface     datastore-dynamodb : DynamoDB Implementation     datastore-imdb : In-memory Implementation", 
            "title": "Datastores"
        }, 
        {
            "location": "/community/contributing/#config-parser", 
            "text": "Node module for validating and parsing user's  screwdriver.yaml  configurations.", 
            "title": "Config Parser"
        }, 
        {
            "location": "/community/contributing/#guide", 
            "text": "This documentation! Everything you ever hoped to know about the Screwdriver project.", 
            "title": "Guide"
        }, 
        {
            "location": "/community/contributing/#miscellaneous-tools", 
            "text": "client : Simple Go-based CLI for accessing the Screwdriver API   job-tools : Generic docker container implementation to bootstrap and execute a build   gitversion : Go-based tool for updating git tags on a repo for a new version number   circuit-fuses : Wrapper to provide a node-circuitbreaker w/ callback interface     keymbinatorial : Generates the unique combinations of key values by taking a single value from each keys array", 
            "title": "Miscellaneous Tools"
        }, 
        {
            "location": "/community/contributing/#adding-a-new-screwdriver-repo", 
            "text": "We have some tools to help start out new repos for screwdriver:   generator-screwdriver : Yeoman generator that bootstraps new repos for screwdriver  eslint-config-screwdriver : Our eslint rules for node-based code. Included in each new repo as part of the bootstrap process", 
            "title": "Adding a New Screwdriver Repo"
        }, 
        {
            "location": "/community/support/", 
            "text": "Support\n\n\nGitHub\n\n\nScrewdriver is completely open source and can be found under the \nscrewdriver-cd organization\n\non Github. We welcome any \nissues\n and \npull requests\n!\nFor more information on our Github repositories and how to contribute, see the \nContributing\n page.\n\n\nSlack\n\n\nWe use Slack for discussion and support. For any Screwdriver-related questions, join the \n#general\n channel on the\n\nScrewdriver Slack team\n. For everything else, join the \n#random\n channel.\n\n\nTo sign up, use our \nSlack inviter\n.\n\n\nStack Overflow\n\n\nWe monitor Stack Overflow for any posts tagged with \nscrewdriver\n. If\nthere aren't any existing questions that help with your problem, feel free to ask a new one!", 
            "title": "Support"
        }, 
        {
            "location": "/community/support/#support", 
            "text": "", 
            "title": "Support"
        }, 
        {
            "location": "/community/support/#github", 
            "text": "Screwdriver is completely open source and can be found under the  screwdriver-cd organization \non Github. We welcome any  issues  and  pull requests !\nFor more information on our Github repositories and how to contribute, see the  Contributing  page.", 
            "title": "GitHub"
        }, 
        {
            "location": "/community/support/#slack", 
            "text": "We use Slack for discussion and support. For any Screwdriver-related questions, join the  #general  channel on the Screwdriver Slack team . For everything else, join the  #random  channel.  To sign up, use our  Slack inviter .", 
            "title": "Slack"
        }, 
        {
            "location": "/community/support/#stack-overflow", 
            "text": "We monitor Stack Overflow for any posts tagged with  screwdriver . If\nthere aren't any existing questions that help with your problem, feel free to ask a new one!", 
            "title": "Stack Overflow"
        }
    ]
}