{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Screwdriver!\n\n    \nWe've split documentation into 3 distinct sections:\n\n\n\n\n\n\n    \n\n        \nCluster Management\n\n        \nTo find more information about managing your own Screwdriver cluster,\n        visit the \nCluster Management\n section.\n\n    \n\n    \n\n        \nUser Guide\n\n        \nIf you'd like to use Screwdriver to run a build, visit our \nUser Guide\n.\n\n    \n\n    \n\n        \nAbout\n\n        \nTo learn more about Screwdriver in general, visit the \nAbout\n section.\n\n    \n\n\n\n\n\n\n    \n\n        \nIf you are new to Screwdriver, we suggest you start by reading through the \nDomain model\n and \nYAML configuration\n to get an idea of different concepts in Screwdriver and how they tie together.\n\n        To see some working examples, you can try out our \nquickstart examples\n with different languages to choose from.", 
            "title": "Home"
        }, 
        {
            "location": "/cluster-management/", 
            "text": "Overall Architecture\n\n\nScrewdriver is a collection of services that facilitate the workflow for\nContinuous Delivery pipelines.\n\n\n\n\nWorkflow\n\n\n\n\n\n\nCommit new code\n\n\nUser starts a new build by one of the following operations:\n\n\n\n\nUser pushes code to SCM\n\n\nUser opens a new pull request on SCM\n\n\nUser pushes code to SCM on an open pull request\n\n\nUser tells Screwdriver (via API or UI) to rebuild a given commit\n\n\n\n\n\n\n\n\nNotify Screwdriver\n\n\nSigned \nwebhooks\n notify\nScrewdriver's API about the change.\n\n\n\n\n\n\nTrigger execution engine\n\n\nScrewdriver starts a job on the specified execution engine passing the\nuser's configuration and git information.\n\n\n\n\n\n\nBuild software\n\n\nScrewdriver's Launcher executes the commands specified in the user's\nconfiguration after checking out the source code from git inside the\ndesired container.\n\n\n\n\n\n\nPublish artifacts\n \n(optional)\n\n\nUser can optionally push produced artifacts to respective artifact\nrepositories (RPMs, Docker images, Node Modules, etc.).\n\n\n\n\n\n\nContinue pipeline\n\n\nOn completion of the job, Screwdriver's Launcher notifies the API and\nif there's more in the pipeline, the API triggers the next job on the\nexecution engine (\nGOTO:3\n).\n\n\n\n\n\n\nComponents\n\n\nScrewdriver consists of five main components, the first three of which are\nbuilt/maintained by Screwdriver:\n\n\n\n\n\n\nREST API\n\n\nRESTful interface for creating, monitoring, and interacting with pipelines.\n\n\n\n\n\n\nWeb UI\n\n\nHuman consumable interface for the \nREST API\n.\n\n\n\n\n\n\nLauncher\n\n\nSelf-contained tool to clone, setup the environment, and execute the\nshell commands defined in your job.\n\n\n\n\n\n\nExecution Engine\n\n\nPluggable build executor that supports executing commands inside of a\ncontainer (e.g. Jenkins, Kubernetes, and Docker).\n\n\n\n\n\n\nDatastore\n\n\nPluggable storage for keeping information about pipelines\n(e.g. Postgres, MySQL, and Sqlite).", 
            "title": "Overall Architecture"
        }, 
        {
            "location": "/cluster-management/#overall-architecture", 
            "text": "Screwdriver is a collection of services that facilitate the workflow for\nContinuous Delivery pipelines.", 
            "title": "Overall Architecture"
        }, 
        {
            "location": "/cluster-management/#workflow", 
            "text": "Commit new code  User starts a new build by one of the following operations:   User pushes code to SCM  User opens a new pull request on SCM  User pushes code to SCM on an open pull request  User tells Screwdriver (via API or UI) to rebuild a given commit     Notify Screwdriver  Signed  webhooks  notify\nScrewdriver's API about the change.    Trigger execution engine  Screwdriver starts a job on the specified execution engine passing the\nuser's configuration and git information.    Build software  Screwdriver's Launcher executes the commands specified in the user's\nconfiguration after checking out the source code from git inside the\ndesired container.    Publish artifacts   (optional)  User can optionally push produced artifacts to respective artifact\nrepositories (RPMs, Docker images, Node Modules, etc.).    Continue pipeline  On completion of the job, Screwdriver's Launcher notifies the API and\nif there's more in the pipeline, the API triggers the next job on the\nexecution engine ( GOTO:3 ).", 
            "title": "Workflow"
        }, 
        {
            "location": "/cluster-management/#components", 
            "text": "Screwdriver consists of five main components, the first three of which are\nbuilt/maintained by Screwdriver:    REST API  RESTful interface for creating, monitoring, and interacting with pipelines.    Web UI  Human consumable interface for the  REST API .    Launcher  Self-contained tool to clone, setup the environment, and execute the\nshell commands defined in your job.    Execution Engine  Pluggable build executor that supports executing commands inside of a\ncontainer (e.g. Jenkins, Kubernetes, and Docker).    Datastore  Pluggable storage for keeping information about pipelines\n(e.g. Postgres, MySQL, and Sqlite).", 
            "title": "Components"
        }, 
        {
            "location": "/cluster-management/configure-api/", 
            "text": "Managing the API\n\n\nPackages\n\n\nLike the other services, the API is shipped as a \nDocker image\n with port 8080 exposed.\n\n\n$ docker run -d -p 9000:8080 screwdrivercd/screwdriver:stable\n$ open http://localhost:9000\n\n\n\n\nOur images are tagged for their version (eg. \n1.2.3\n) as well as a floating \nlatest\n and \nstable\n.  Most installations should be using \nstable\n or the fixed version tags.\n\n\nConfiguration\n\n\nScrewdriver already \ndefaults most configuration\n, but you can override defaults using a \nconfig/local.yaml\n or environment variables. All the possible environment variables are \ndefined here\n.\n\n\nAuthentication / Authorization\n\n\nConfigure how users can and who can access the API.\n\n\n\n\n\n\n\n\nKey\n\n\nRequired\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSECRET_JWT_PRIVATE_KEY\n\n\nYes\n\n\nA private key uses for signing jwt tokens. Generate one by running \n$ openssl genrsa -out jwt.pem 2048\n\n\n\n\n\n\nSECRET_JWT_PUBLIC_KEY\n\n\nYes\n\n\nThe public key used for verifying the signature. Generate one by running \n$ openssl rsa -in jwt.pem -pubout -out jwt.pub\n\n\n\n\n\n\nSECRET_COOKIE_PASSWORD\n\n\nYes\n\n\nA password used for encrypting session data. \nNeeds to be minimum 32 characters\n\n\n\n\n\n\nSECRET_PASSWORD\n\n\nYes\n\n\nA password used for encrypting stored secrets. \nNeeds to be minimum 32 characters\n\n\n\n\n\n\nIS_HTTPS\n\n\nNo\n\n\nA flag to set if the server is running over https. Used as a flag for the OAuth flow (default to \nfalse\n)\n\n\n\n\n\n\nSECRET_WHITELIST\n\n\nNo\n\n\nWhitelist of users able to authenticate against the system. If empty, it allows everyone. (JSON Array format)\n\n\n\n\n\n\nSECRET_ADMINS\n\n\nNo\n\n\nWhitelist of users able to authenticate against the system. If empty, it allows everyone. (JSON Array format)\n\n\n\n\n\n\n\n\n# config/local.yaml\nauth:\n    jwtPrivateKey: |\n        PRIVATE KEY HERE\n    jwtPublicKey: |\n        PUBLIC KEY HERE\n    cookiePassword: 975452d6554228b581bf34197bcb4e0a08622e24\n    encryptionPassword: 5c6d9edc3a951cda763f650235cfc41a3fc23fe8\n    https: false\n    whitelist:\n        - batman\n        - robin\n    admins:\n        - batman\n\n\n\n\nBookend Plugins\n\n\nYou can globally configure which built-in bookend plugins will be used during a build. By default, \nscm\n is enabled to begin builds with a SCM checkout command.\n\n\nIf you're looking to include a custom bookend in the API, please refer \nhere\n.\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBOOKENDS_SETUP\n\n\nNone\n\n\nThe ordered list of plugins to execute at the beginning of every build. Take the forms of \n'[\"first\", \"second\", ...]'\n\n\n\n\n\n\nBOOKENDS_TEARDOWN\n\n\nNone\n\n\nThe ordered list of plugins to execute at the end of every build. Take the forms of \n'[\"first\", \"second\", ...]'\n\n\n\n\n\n\n\n\n# config/local.yaml\nbookends:\n    setup:\n        - scm\n        - my-custom-bookend\n\n\n\n\nServing\n\n\nConfigure the how the service is listening for traffic.\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPORT\n\n\n80\n\n\nPort to listen on\n\n\n\n\n\n\nHOST\n\n\n0.0.0.0\n\n\nHost to listen on (set to localhost to only accept connections from this machine)\n\n\n\n\n\n\nURI\n\n\nhttp://localhost:80\n\n\nExternally routable URI (usually your load balancer or CNAME)\n\n\n\n\n\n\nHTTPD_TLS\n\n\nfalse\n\n\nSSL support; for SSL, replace \nfalse\n with a JSON object that provides the options required by \ntls.createServer\n\n\n\n\n\n\n\n\n# config/local.yaml\nhttpd:\n    port: 443\n    host: 0.0.0.0\n    uri: https://localhost\n    tls:\n        key: |\n            PRIVATE KEY HERE\n        cert: |\n            YOUR CERT HERE\n\n\n\n\nEcosystem\n\n\nSpecify externally routable URLs for your UI, Artifact Store, and Badge service.\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nECOSYSTEM_UI\n\n\nhttps://cd.screwdriver.cd\n\n\nURL for the User Interface\n\n\n\n\n\n\nECOSYSTEM_STORE\n\n\nhttps://store.screwdriver.cd\n\n\nURL for the Artifact Store\n\n\n\n\n\n\nECOSYSTEM_BADGES\n\n\nhttps://img.shields.io/badge/build-{{status}}-{{color}}.svg\n\n\nURL with templates for status text and color\n\n\n\n\n\n\n\n\n# config/local.yaml\necosystem:\n    # Externally routable URL for the User Interface\n    ui: https://cd.screwdriver.cd\n    # Externally routable URL for the Artifact Store\n    store: https://store.screwdriver.cd\n    # Badge service (needs to add a status and color)\n    badges: https://img.shields.io/badge/build-{{status}}-{{color}}.svg\n\n\n\n\nDatastore Plugin\n\n\nTo use Postgres, MySQL, and Sqlite, use \nsequelize\n plugin.\n\n\nSequelize\n\n\nSet these environment variables:\n\n\n\n\n\n\n\n\nEnvironment name\n\n\nRequired\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDATASTORE_PLUGIN\n\n\nYes\n\n\n\n\nSet to \nsequelize\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_DIALECT\n\n\nNo\n\n\nmysql\n\n\nCan be \nsqlite\n, \npostgres\n, \nmysql\n, or \nmssql\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_DATABASE\n\n\nNo\n\n\nscrewdriver\n\n\nDatabase name\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_USERNAME\n\n\nNo for sqlite\n\n\n\n\nLogin username\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_PASSWORD\n\n\nNo for sqlite\n\n\n\n\nLogin password\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_STORAGE\n\n\nYes for sqlite\n\n\n\n\nStorage location for sqlite\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_HOST\n\n\nNo\n\n\n\n\nNetwork host\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_PORT\n\n\nNo\n\n\n\n\nNetwork port\n\n\n\n\n\n\n\n\n# config/local.yaml\ndatastore:\n    plugin: sequelize\n    sequelize:\n        dialect: TYPE-OF-SERVER\n        storage: STORAGE-LOCATION\n        database: DATABASE-NAME\n        username: DATABASE-USERNAME\n        password: DATABASE-PASSWORD\n        host: NETWORK-HOST\n        port: NETWORK-PORT\n\n\n\n\nExecutor Plugin\n\n\nWe currently support \nkubernetes\n and \ndocker\n executor\n\n\nKubernetes\n\n\nSet these environment variables:\n\n\n\n\n\n\n\n\nEnvironment name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEXECUTOR_PLUGIN\n\n\n\n\nSet to \nk8s\n\n\n\n\n\n\nLAUNCH_VERSION\n\n\n\n\nLauncher version to use\n\n\n\n\n\n\nK8S_HOST\n\n\n\n\nKubernetes host\n\n\n\n\n\n\nK8S_TOKEN\n\n\n\n\nJWT for authenticating Kubernetes requests\n\n\n\n\n\n\nK8S_JOBS_NAMESPACE\n\n\ndefault\n\n\nJobs namespace for Kubernetes jobs URL\n\n\n\n\n\n\n\n\n# config/local.yaml\nexecutor:\n    plugin: k8s\n    k8s:\n        kubernetes:\n            # The host or IP of the kubernetes cluster\n            host: YOUR-KUBERNETES-HOST\n            token: JWT-FOR-AUTHENTICATING-KUBERNETES-REQUEST\n            jobsNamespace: default\n        launchVersion: stable\n\n\n\n\nDocker\n\n\nOr set these environment variables:\n\n\n\n\n\n\n\n\nEnvironment name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEXECUTOR_PLUGIN\n\n\ndocker\n\n\nSet to \ndocker\n\n\n\n\n\n\nLAUNCH_VERSION\n\n\nstable\n\n\nLauncher version to use\n\n\n\n\n\n\nEXECUTOR_DOCKER_DOCKER\n\n\n{}\n\n\nDockerode configuration\n (JSON object)\n\n\n\n\n\n\n\n\n# config/local.yaml\nexecutor:\n    plugin: docker\n    docker:\n        docker:\n            socketPath: /var/lib/docker.sock\n        launchVersion: stable\n\n\n\n\nEmail Notifications\n\n\nConfigure the SMTP server and sender address that email notifications will be sent from.\n\n\n# config/local.yaml\nnotifications:\n    email:\n        host: smtp.yourhost.com\n        port: 25\n        from: example@email.com\n\n\n\n\nConfigurable authentication settings have not yet been built, but can easily be added. We\u2019re using the \nnodemailer\n package to power emails, so authentication features will be similar to any typical nodemailer setup. Contribute at: \nhttps://github.com/screwdriver-cd/notifications-email\n\n\nSource Control Plugin\n\n\nWe currently support \nGithub\n and \nBitbucket.org\n\n\nStep 1: Set up your OAuth Application\n\n\nYou will need to set up an OAuth Application and retrieve your OAuth Client ID and Secret.\n\n\nGithub:\n\n\n\n\nNavigate to the \nGithub OAuth applications\n page.\n\n\nClick on the application you created to get your OAuth Client ID and Secret.\n\n\nFill out the \nHomepage URL\n and \nAuthorization callback URL\n to be the IP address of where your API is running.\n\n\n\n\nBitbucket.org:\n\n\n\n\nNavigate to the Bitbucket OAuth applications: \nhttps://bitbucket.org/account/user/{your-username}/api\n\n\nClick on \nAdd Consumer\n.\n\n\nFill out the \nURL\n and \nCallback URL\n to be the IP address of where your API is running.\n\n\n\n\nStep 2: Configure your SCM plugin\n\n\nSet these environment variables:\n\n\n\n\n\n\n\n\nEnvironment name\n\n\nRequired\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSCM_PLUGIN\n\n\nNo\n\n\ngithub\n\n\ngithub\n or \nbitbucket\n\n\n\n\n\n\nSECRET_OAUTH_CLIENT_ID\n\n\nYes\n\n\n\n\nYour OAuth Client Id (Application key)\n\n\n\n\n\n\nSECRET_OAUTH_CLIENT_SECRET\n\n\nYes\n\n\n\n\nYou OAuth Client secret (Application secret)\n\n\n\n\n\n\nWEBHOOK_GITHUB_SECRET\n\n\nYes for Github\n\n\n\n\nSecret to sign for webhooks\n\n\n\n\n\n\nSCM_GITHUB_GHE_HOST\n\n\nYes for Github Enterprise\n\n\n\n\nGHE host for Github Enterprise\n\n\n\n\n\n\nSCM_PRIVATE_REPO_SUPPORT\n\n\nNo\n\n\nfalse\n\n\nAsk Github users for 'repo' scope to allow read/write access to public and private repo\n\n\n\n\n\n\nSCM_USERNAME\n\n\nNo\n\n\nsd-buildbot\n\n\nUsername for checkout\n\n\n\n\n\n\nSCM_EMAIL\n\n\nNo\n\n\ndev-null@screwdriver.cd\n\n\nEmail of user for checkout\n\n\n\n\n\n\n\n\nGithub:\n\n\n# config/local.yaml\nscm:\n    plugin: github\n    github:\n        oauthClientId: YOUR-OAUTH-CLIENT-ID\n        oauthClientSecret: YOUR-OAUTH-CLIENT-SECRET\n        # Secret to add to GitHub webhooks so that we can validate them\n        secret: SUPER-SECRET-SIGNING-THING\n        # You can also configure for use with GitHub enterprise\n        # gheHost: github.screwdriver.cd\n        # Whether to support private repo\n        # privateRepo: true\n\n\n\n\nIf users want to use private repo, they also need to set up \nSCM_USERNAME\n and \nSCM_ACCESS_TOKEN\n as \nsecrets\n in their \nscrewdriver.yaml\n.\n\n\nBitbucket.org\n\n\n# config/local.yaml\nscm:\n    plugin: bitbucket\n    bitbucket:\n        oauthClientId: YOUR-APP-KEY\n        oauthClientSecret: YOUR-APP-SECRET\n\n\n\n\nExtending the Docker container\n\n\nThere are some scenarios where you would prefer to extend the Screwdriver.cd Docker image, such as using custom Bookend plugins. This section is not meant to be exhaustive or complete, but will provide insight into some of the fundamental cases.\n\n\nUsing a custom bookend\n\n\nUsing a custom bookend is a common case where you would extend the Screwdriver.cd Docker image.\n\n\nIn this chosen example, we want to have our bookend execute before the \nscm\n (which checks out the code from the configured SCM). Although the bookend plugins can be configured by environment variables, we will show how to accomplish the same task with a \nlocal.yaml\n file.\n\n\nThis is shown in the following \nlocal.yaml\n snippet:\n\n\n# local.yaml\n---\n  ...\nbookends:\n  setup:\n    - my-custom-bookend\n    - scm\n\n\n\n\nFor building our extended Docker image, we will need to create a \nDockerfile\n that will have our extra dependencies installed. If you would prefer to save the \nlocal.yaml\n configuration file in the Docker image instead of mounting it in later, you may do so in the Dockerfile as well.\n\n\n# Dockerfile\nFROM screwdrivercd/screwdriver:stable\n\n# Install additional NPM bookend plugin\nRUN cd /usr/src/app \n /usr/local/bin/npm install my-custom-bookend\n\n# Optionally save the configuration file in the image\nADD local.yaml /config/local.yaml\n\n\n\n\nOnce you build the Docker image, you will need to deploy it to your Screwdriver.cd cluster. For instance, if you're using Kubernetes, you would replace the \nscrewdrivercd/api:stable\n image to your custom Docker image.\n\n\nThe following is an example snippet of an updated Kubernetes deployment configuration:\n\n\n# partial Kubernetes configuration\n  ...\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: screwdriver-api\n        # The image name is the one you specified when built\n        # The tag name is the tag you specified when built\n        image: my_extended_docker_image_name:tag_name", 
            "title": "Configuring the API"
        }, 
        {
            "location": "/cluster-management/configure-api/#managing-the-api", 
            "text": "", 
            "title": "Managing the API"
        }, 
        {
            "location": "/cluster-management/configure-api/#packages", 
            "text": "Like the other services, the API is shipped as a  Docker image  with port 8080 exposed.  $ docker run -d -p 9000:8080 screwdrivercd/screwdriver:stable\n$ open http://localhost:9000  Our images are tagged for their version (eg.  1.2.3 ) as well as a floating  latest  and  stable .  Most installations should be using  stable  or the fixed version tags.", 
            "title": "Packages"
        }, 
        {
            "location": "/cluster-management/configure-api/#configuration", 
            "text": "Screwdriver already  defaults most configuration , but you can override defaults using a  config/local.yaml  or environment variables. All the possible environment variables are  defined here .", 
            "title": "Configuration"
        }, 
        {
            "location": "/cluster-management/configure-api/#authentication-authorization", 
            "text": "Configure how users can and who can access the API.     Key  Required  Description      SECRET_JWT_PRIVATE_KEY  Yes  A private key uses for signing jwt tokens. Generate one by running  $ openssl genrsa -out jwt.pem 2048    SECRET_JWT_PUBLIC_KEY  Yes  The public key used for verifying the signature. Generate one by running  $ openssl rsa -in jwt.pem -pubout -out jwt.pub    SECRET_COOKIE_PASSWORD  Yes  A password used for encrypting session data.  Needs to be minimum 32 characters    SECRET_PASSWORD  Yes  A password used for encrypting stored secrets.  Needs to be minimum 32 characters    IS_HTTPS  No  A flag to set if the server is running over https. Used as a flag for the OAuth flow (default to  false )    SECRET_WHITELIST  No  Whitelist of users able to authenticate against the system. If empty, it allows everyone. (JSON Array format)    SECRET_ADMINS  No  Whitelist of users able to authenticate against the system. If empty, it allows everyone. (JSON Array format)     # config/local.yaml\nauth:\n    jwtPrivateKey: |\n        PRIVATE KEY HERE\n    jwtPublicKey: |\n        PUBLIC KEY HERE\n    cookiePassword: 975452d6554228b581bf34197bcb4e0a08622e24\n    encryptionPassword: 5c6d9edc3a951cda763f650235cfc41a3fc23fe8\n    https: false\n    whitelist:\n        - batman\n        - robin\n    admins:\n        - batman", 
            "title": "Authentication / Authorization"
        }, 
        {
            "location": "/cluster-management/configure-api/#bookend-plugins", 
            "text": "You can globally configure which built-in bookend plugins will be used during a build. By default,  scm  is enabled to begin builds with a SCM checkout command.  If you're looking to include a custom bookend in the API, please refer  here .     Key  Default  Description      BOOKENDS_SETUP  None  The ordered list of plugins to execute at the beginning of every build. Take the forms of  '[\"first\", \"second\", ...]'    BOOKENDS_TEARDOWN  None  The ordered list of plugins to execute at the end of every build. Take the forms of  '[\"first\", \"second\", ...]'     # config/local.yaml\nbookends:\n    setup:\n        - scm\n        - my-custom-bookend", 
            "title": "Bookend Plugins"
        }, 
        {
            "location": "/cluster-management/configure-api/#serving", 
            "text": "Configure the how the service is listening for traffic.     Key  Default  Description      PORT  80  Port to listen on    HOST  0.0.0.0  Host to listen on (set to localhost to only accept connections from this machine)    URI  http://localhost:80  Externally routable URI (usually your load balancer or CNAME)    HTTPD_TLS  false  SSL support; for SSL, replace  false  with a JSON object that provides the options required by  tls.createServer     # config/local.yaml\nhttpd:\n    port: 443\n    host: 0.0.0.0\n    uri: https://localhost\n    tls:\n        key: |\n            PRIVATE KEY HERE\n        cert: |\n            YOUR CERT HERE", 
            "title": "Serving"
        }, 
        {
            "location": "/cluster-management/configure-api/#ecosystem", 
            "text": "Specify externally routable URLs for your UI, Artifact Store, and Badge service.     Key  Default  Description      ECOSYSTEM_UI  https://cd.screwdriver.cd  URL for the User Interface    ECOSYSTEM_STORE  https://store.screwdriver.cd  URL for the Artifact Store    ECOSYSTEM_BADGES  https://img.shields.io/badge/build-{{status}}-{{color}}.svg  URL with templates for status text and color     # config/local.yaml\necosystem:\n    # Externally routable URL for the User Interface\n    ui: https://cd.screwdriver.cd\n    # Externally routable URL for the Artifact Store\n    store: https://store.screwdriver.cd\n    # Badge service (needs to add a status and color)\n    badges: https://img.shields.io/badge/build-{{status}}-{{color}}.svg", 
            "title": "Ecosystem"
        }, 
        {
            "location": "/cluster-management/configure-api/#datastore-plugin", 
            "text": "To use Postgres, MySQL, and Sqlite, use  sequelize  plugin.", 
            "title": "Datastore Plugin"
        }, 
        {
            "location": "/cluster-management/configure-api/#sequelize", 
            "text": "Set these environment variables:     Environment name  Required  Default Value  Description      DATASTORE_PLUGIN  Yes   Set to  sequelize    DATASTORE_SEQUELIZE_DIALECT  No  mysql  Can be  sqlite ,  postgres ,  mysql , or  mssql    DATASTORE_SEQUELIZE_DATABASE  No  screwdriver  Database name    DATASTORE_SEQUELIZE_USERNAME  No for sqlite   Login username    DATASTORE_SEQUELIZE_PASSWORD  No for sqlite   Login password    DATASTORE_SEQUELIZE_STORAGE  Yes for sqlite   Storage location for sqlite    DATASTORE_SEQUELIZE_HOST  No   Network host    DATASTORE_SEQUELIZE_PORT  No   Network port     # config/local.yaml\ndatastore:\n    plugin: sequelize\n    sequelize:\n        dialect: TYPE-OF-SERVER\n        storage: STORAGE-LOCATION\n        database: DATABASE-NAME\n        username: DATABASE-USERNAME\n        password: DATABASE-PASSWORD\n        host: NETWORK-HOST\n        port: NETWORK-PORT", 
            "title": "Sequelize"
        }, 
        {
            "location": "/cluster-management/configure-api/#executor-plugin", 
            "text": "We currently support  kubernetes  and  docker  executor", 
            "title": "Executor Plugin"
        }, 
        {
            "location": "/cluster-management/configure-api/#kubernetes", 
            "text": "Set these environment variables:     Environment name  Default Value  Description      EXECUTOR_PLUGIN   Set to  k8s    LAUNCH_VERSION   Launcher version to use    K8S_HOST   Kubernetes host    K8S_TOKEN   JWT for authenticating Kubernetes requests    K8S_JOBS_NAMESPACE  default  Jobs namespace for Kubernetes jobs URL     # config/local.yaml\nexecutor:\n    plugin: k8s\n    k8s:\n        kubernetes:\n            # The host or IP of the kubernetes cluster\n            host: YOUR-KUBERNETES-HOST\n            token: JWT-FOR-AUTHENTICATING-KUBERNETES-REQUEST\n            jobsNamespace: default\n        launchVersion: stable", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/cluster-management/configure-api/#docker", 
            "text": "Or set these environment variables:     Environment name  Default Value  Description      EXECUTOR_PLUGIN  docker  Set to  docker    LAUNCH_VERSION  stable  Launcher version to use    EXECUTOR_DOCKER_DOCKER  {}  Dockerode configuration  (JSON object)     # config/local.yaml\nexecutor:\n    plugin: docker\n    docker:\n        docker:\n            socketPath: /var/lib/docker.sock\n        launchVersion: stable", 
            "title": "Docker"
        }, 
        {
            "location": "/cluster-management/configure-api/#email-notifications", 
            "text": "Configure the SMTP server and sender address that email notifications will be sent from.  # config/local.yaml\nnotifications:\n    email:\n        host: smtp.yourhost.com\n        port: 25\n        from: example@email.com  Configurable authentication settings have not yet been built, but can easily be added. We\u2019re using the  nodemailer  package to power emails, so authentication features will be similar to any typical nodemailer setup. Contribute at:  https://github.com/screwdriver-cd/notifications-email", 
            "title": "Email Notifications"
        }, 
        {
            "location": "/cluster-management/configure-api/#source-control-plugin", 
            "text": "We currently support  Github  and  Bitbucket.org", 
            "title": "Source Control Plugin"
        }, 
        {
            "location": "/cluster-management/configure-api/#step-1-set-up-your-oauth-application", 
            "text": "You will need to set up an OAuth Application and retrieve your OAuth Client ID and Secret.", 
            "title": "Step 1: Set up your OAuth Application"
        }, 
        {
            "location": "/cluster-management/configure-api/#github", 
            "text": "Navigate to the  Github OAuth applications  page.  Click on the application you created to get your OAuth Client ID and Secret.  Fill out the  Homepage URL  and  Authorization callback URL  to be the IP address of where your API is running.", 
            "title": "Github:"
        }, 
        {
            "location": "/cluster-management/configure-api/#bitbucketorg", 
            "text": "Navigate to the Bitbucket OAuth applications:  https://bitbucket.org/account/user/{your-username}/api  Click on  Add Consumer .  Fill out the  URL  and  Callback URL  to be the IP address of where your API is running.", 
            "title": "Bitbucket.org:"
        }, 
        {
            "location": "/cluster-management/configure-api/#step-2-configure-your-scm-plugin", 
            "text": "Set these environment variables:     Environment name  Required  Default Value  Description      SCM_PLUGIN  No  github  github  or  bitbucket    SECRET_OAUTH_CLIENT_ID  Yes   Your OAuth Client Id (Application key)    SECRET_OAUTH_CLIENT_SECRET  Yes   You OAuth Client secret (Application secret)    WEBHOOK_GITHUB_SECRET  Yes for Github   Secret to sign for webhooks    SCM_GITHUB_GHE_HOST  Yes for Github Enterprise   GHE host for Github Enterprise    SCM_PRIVATE_REPO_SUPPORT  No  false  Ask Github users for 'repo' scope to allow read/write access to public and private repo    SCM_USERNAME  No  sd-buildbot  Username for checkout    SCM_EMAIL  No  dev-null@screwdriver.cd  Email of user for checkout", 
            "title": "Step 2: Configure your SCM plugin"
        }, 
        {
            "location": "/cluster-management/configure-api/#github_1", 
            "text": "# config/local.yaml\nscm:\n    plugin: github\n    github:\n        oauthClientId: YOUR-OAUTH-CLIENT-ID\n        oauthClientSecret: YOUR-OAUTH-CLIENT-SECRET\n        # Secret to add to GitHub webhooks so that we can validate them\n        secret: SUPER-SECRET-SIGNING-THING\n        # You can also configure for use with GitHub enterprise\n        # gheHost: github.screwdriver.cd\n        # Whether to support private repo\n        # privateRepo: true  If users want to use private repo, they also need to set up  SCM_USERNAME  and  SCM_ACCESS_TOKEN  as  secrets  in their  screwdriver.yaml .", 
            "title": "Github:"
        }, 
        {
            "location": "/cluster-management/configure-api/#bitbucketorg_1", 
            "text": "# config/local.yaml\nscm:\n    plugin: bitbucket\n    bitbucket:\n        oauthClientId: YOUR-APP-KEY\n        oauthClientSecret: YOUR-APP-SECRET", 
            "title": "Bitbucket.org"
        }, 
        {
            "location": "/cluster-management/configure-api/#extending-the-docker-container", 
            "text": "There are some scenarios where you would prefer to extend the Screwdriver.cd Docker image, such as using custom Bookend plugins. This section is not meant to be exhaustive or complete, but will provide insight into some of the fundamental cases.", 
            "title": "Extending the Docker container"
        }, 
        {
            "location": "/cluster-management/configure-api/#using-a-custom-bookend", 
            "text": "Using a custom bookend is a common case where you would extend the Screwdriver.cd Docker image.  In this chosen example, we want to have our bookend execute before the  scm  (which checks out the code from the configured SCM). Although the bookend plugins can be configured by environment variables, we will show how to accomplish the same task with a  local.yaml  file.  This is shown in the following  local.yaml  snippet:  # local.yaml\n---\n  ...\nbookends:\n  setup:\n    - my-custom-bookend\n    - scm  For building our extended Docker image, we will need to create a  Dockerfile  that will have our extra dependencies installed. If you would prefer to save the  local.yaml  configuration file in the Docker image instead of mounting it in later, you may do so in the Dockerfile as well.  # Dockerfile\nFROM screwdrivercd/screwdriver:stable\n\n# Install additional NPM bookend plugin\nRUN cd /usr/src/app   /usr/local/bin/npm install my-custom-bookend\n\n# Optionally save the configuration file in the image\nADD local.yaml /config/local.yaml  Once you build the Docker image, you will need to deploy it to your Screwdriver.cd cluster. For instance, if you're using Kubernetes, you would replace the  screwdrivercd/api:stable  image to your custom Docker image.  The following is an example snippet of an updated Kubernetes deployment configuration:  # partial Kubernetes configuration\n  ...\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: screwdriver-api\n        # The image name is the one you specified when built\n        # The tag name is the tag you specified when built\n        image: my_extended_docker_image_name:tag_name", 
            "title": "Using a custom bookend"
        }, 
        {
            "location": "/cluster-management/configure-ui/", 
            "text": "Managing the User Interface\n\n\nPackages\n\n\nLike the other services, the User Interface is shipped as a \nDocker image\n with port 80 exposed.\n\n\n$ docker run -d -p 8000:80 screwdrivercd/ui:stable\n$ open http://localhost:8000\n\n\n\n\nOur images are tagged for their version (eg. \n1.2.3\n) as well as a floating \nlatest\n and \nstable\n.  Most installations should be using \nstable\n or the fixed version tags.\n\n\nConfiguration\n\n\nThe User Interface only has one configuration option, the location of the API.  It is set via an environment variable \nECOSYSTEM_API\n.\n\n\nExample:\n\n\n$ docker run -d -p 8000:80 -e ECOSYSTEM_API=http://localhost:9000 screwdrivercd/ui:stable", 
            "title": "Configuring the UI"
        }, 
        {
            "location": "/cluster-management/configure-ui/#managing-the-user-interface", 
            "text": "", 
            "title": "Managing the User Interface"
        }, 
        {
            "location": "/cluster-management/configure-ui/#packages", 
            "text": "Like the other services, the User Interface is shipped as a  Docker image  with port 80 exposed.  $ docker run -d -p 8000:80 screwdrivercd/ui:stable\n$ open http://localhost:8000  Our images are tagged for their version (eg.  1.2.3 ) as well as a floating  latest  and  stable .  Most installations should be using  stable  or the fixed version tags.", 
            "title": "Packages"
        }, 
        {
            "location": "/cluster-management/configure-ui/#configuration", 
            "text": "The User Interface only has one configuration option, the location of the API.  It is set via an environment variable  ECOSYSTEM_API .  Example:  $ docker run -d -p 8000:80 -e ECOSYSTEM_API=http://localhost:9000 screwdrivercd/ui:stable", 
            "title": "Configuration"
        }, 
        {
            "location": "/cluster-management/configure-store/", 
            "text": "Managing the Store\n\n\nPackages\n\n\nLike the other services, the API is shipped as a \nDocker image\n with port 80 exposed.\n\n\n$ docker run -d -p 7000:80 screwdrivercd/store:stable\n$ open http://localhost:7000\n\n\n\n\nOur images are tagged for their version (eg. \n1.2.3\n) as well as a floating \nlatest\n and \nstable\n. Most installations should be using \nstable\n or the fixed version tags.\n\n\nConfiguration\n\n\nScrewdriver already \ndefaults most configuration\n, but you can override defaults using a \nconfig/local.yaml\n or environment variables. All the possible environment variables are \ndefined here\n.\n\n\nAuthentication\n\n\nConfigure the validation of incoming JWTs from the API.\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSECRET_JWT_PUBLIC_KEY\n\n\nnone\n\n\nThe public key used for verifying the signature of the JWT. Use the same one as configured in the API\n\n\n\n\n\n\n\n\n# config/local.yaml\nauth:\n    jwtPublicKey: |\n        PUBLIC KEY HERE\n\n\n\n\nServing\n\n\nConfigure the how the service is listening for traffic.\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPORT\n\n\n80\n\n\nPort to listen on\n\n\n\n\n\n\nHOST\n\n\n0.0.0.0\n\n\nHost to listen on (set to localhost to only accept connections from this machine)\n\n\n\n\n\n\nURI\n\n\nhttp://localhost:80\n\n\nExternally routable URI (usually your load balancer or CNAME)\n\n\n\n\n\n\nHTTPD_TLS\n\n\nfalse\n\n\nSSL support; for SSL, replace \nfalse\n with a JSON object that provides the options required by \ntls.createServer\n\n\n\n\n\n\n\n\n# config/local.yaml\nhttpd:\n    port: 443\n    host: 0.0.0.0\n    uri: https://localhost\n    tls:\n        key: |\n            PRIVATE KEY HERE\n        cert: |\n            YOUR CERT HERE\n\n\n\n\nBuild Artifacts\n\n\nConfigure some settings about storing Build Artifacts.\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBUILDS_EXPIRE_TIME\n\n\n1814400000 (3 weeks)\n\n\nHow long should build logs stay around for\n\n\n\n\n\n\nBUILDS_MAX_BYTES\n\n\n1073741824 (1GB)\n\n\nUpper limit on incoming uploads to builds artifacts\n\n\n\n\n\n\n\n\n# config/local.yaml\nbuilds:\n    expiresInSec: 1814400000 # 3 weeks\n    maxByteSize: 1073741824 # 1GB\n\n\n\n\nStorage\n\n\nWe have two methods of storing artifacts right now: - \nmemory\n - In-memory store (inefficient and non-permanent) - \ns3\n - Amazon S3\n\n\n\n\n\n\n\n\nKey\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSTRATEGY\n\n\nmemory\n\n\nMethod of storing artifacts (memory or s3)\n\n\n\n\n\n\nS3_ACCESS_KEY_ID\n\n\nnone\n\n\nAmazon access key\n\n\n\n\n\n\nS3_ACCESS_KEY_SECRET\n\n\nnone\n\n\nAmazon secret access key\n\n\n\n\n\n\nS3_REGION\n\n\nnone\n\n\nAmazon S3 region\n\n\n\n\n\n\nS3_BUCKET\n\n\nnone\n\n\nAmazon S3 bucket that you have write access to\n\n\n\n\n\n\nS3_ENDPOINT\n\n\nnone\n\n\nCustom endpoint for Amazon S3 compatible API\n\n\n\n\n\n\n\n\n# config/local.yaml\nstrategy:\n    plugin: memory\n    s3:\n        accessKeyId: YOUR-KEY-ID\n        secretAccessKey: YOUR-KEY-SECRET\n        region: YOUR-REGION\n        bucket: YOUR-BUCKET-ID\n        endpoint: YOUR-S3-API-URL", 
            "title": "Configuring the Store"
        }, 
        {
            "location": "/cluster-management/configure-store/#managing-the-store", 
            "text": "", 
            "title": "Managing the Store"
        }, 
        {
            "location": "/cluster-management/configure-store/#packages", 
            "text": "Like the other services, the API is shipped as a  Docker image  with port 80 exposed.  $ docker run -d -p 7000:80 screwdrivercd/store:stable\n$ open http://localhost:7000  Our images are tagged for their version (eg.  1.2.3 ) as well as a floating  latest  and  stable . Most installations should be using  stable  or the fixed version tags.", 
            "title": "Packages"
        }, 
        {
            "location": "/cluster-management/configure-store/#configuration", 
            "text": "Screwdriver already  defaults most configuration , but you can override defaults using a  config/local.yaml  or environment variables. All the possible environment variables are  defined here .", 
            "title": "Configuration"
        }, 
        {
            "location": "/cluster-management/configure-store/#authentication", 
            "text": "Configure the validation of incoming JWTs from the API.     Key  Default  Description      SECRET_JWT_PUBLIC_KEY  none  The public key used for verifying the signature of the JWT. Use the same one as configured in the API     # config/local.yaml\nauth:\n    jwtPublicKey: |\n        PUBLIC KEY HERE", 
            "title": "Authentication"
        }, 
        {
            "location": "/cluster-management/configure-store/#serving", 
            "text": "Configure the how the service is listening for traffic.     Key  Default  Description      PORT  80  Port to listen on    HOST  0.0.0.0  Host to listen on (set to localhost to only accept connections from this machine)    URI  http://localhost:80  Externally routable URI (usually your load balancer or CNAME)    HTTPD_TLS  false  SSL support; for SSL, replace  false  with a JSON object that provides the options required by  tls.createServer     # config/local.yaml\nhttpd:\n    port: 443\n    host: 0.0.0.0\n    uri: https://localhost\n    tls:\n        key: |\n            PRIVATE KEY HERE\n        cert: |\n            YOUR CERT HERE", 
            "title": "Serving"
        }, 
        {
            "location": "/cluster-management/configure-store/#build-artifacts", 
            "text": "Configure some settings about storing Build Artifacts.     Key  Default  Description      BUILDS_EXPIRE_TIME  1814400000 (3 weeks)  How long should build logs stay around for    BUILDS_MAX_BYTES  1073741824 (1GB)  Upper limit on incoming uploads to builds artifacts     # config/local.yaml\nbuilds:\n    expiresInSec: 1814400000 # 3 weeks\n    maxByteSize: 1073741824 # 1GB", 
            "title": "Build Artifacts"
        }, 
        {
            "location": "/cluster-management/configure-store/#storage", 
            "text": "We have two methods of storing artifacts right now: -  memory  - In-memory store (inefficient and non-permanent) -  s3  - Amazon S3     Key  Default  Description      STRATEGY  memory  Method of storing artifacts (memory or s3)    S3_ACCESS_KEY_ID  none  Amazon access key    S3_ACCESS_KEY_SECRET  none  Amazon secret access key    S3_REGION  none  Amazon S3 region    S3_BUCKET  none  Amazon S3 bucket that you have write access to    S3_ENDPOINT  none  Custom endpoint for Amazon S3 compatible API     # config/local.yaml\nstrategy:\n    plugin: memory\n    s3:\n        accessKeyId: YOUR-KEY-ID\n        secretAccessKey: YOUR-KEY-SECRET\n        region: YOUR-REGION\n        bucket: YOUR-BUCKET-ID\n        endpoint: YOUR-S3-API-URL", 
            "title": "Storage"
        }, 
        {
            "location": "/cluster-management/running-locally/", 
            "text": "Running Locally\n\n\nYou can run Screwdriver locally by using our Screwdriver-in-a-box tool.\n\n\nSD-in-a-Box\n\n\nThis handy feature will bring up an entire Screwdriver instance (UI, API, and log store) locally for you to play with.\n\n\nRequires:\n\n\n\n\nMac OSX 10.10+\n\n\nDocker for Mac\n\n\nDocker Compose 1.8.1+\n\n\nPython 2.6+\n\n\n\n\nRun the below command in your terminal to bring up a Screwdriver cluster locally.\n\n\n$ python \n(curl https://raw.githubusercontent.com/screwdriver-cd/screwdriver/master/in-a-box.py)\n\n\n\n\nYou will be prompted to enter your Client ID and Client Secret. Afterwards, type \ny\n to launch Screwdriver!\n\n\n\n\nConfiguring SD-in-a-Box\n\n\nSD-in-a-box was intended to be an easy way to run a Screwdriver cluster locally on your development machine so you can demo its features first-hand.\n\n\nCustom Docker Images\n\n\nSince it's powered by Docker, you can determine which images to use for it. SD-in-a-Box (and Screwdriver as a whole) uses the following Docker images:\n\n\n\n\nscrewdrivercd/screwdriver\n - API. The main engine of the CI/CD cluster.\n\n\nscrewdrivercd/ui\n - UI. To pleasantly interact with Screwdriver.\n\n\nscrewdrivercd/store\n - Artifact repository. Responsible for artifacts like build logs \n templates\n\n\nscrewdrivercd/launcher\n - Worker component that executes the build. You \ncannot\n change the image. You can only specify a specific tag to use.\n\n\n\n\nHere is a snippet of the \ndocker-compose.yml\n file\n\n\nversion: '2'\nservices:\n  api:\n    image: screwdrivercd/screwdriver:stable\n    . . .\n  ui:\n    image: screwdrivercd/ui:stable\n    . . .\n  store:\n    image: screwdrivercd/store:stable\n    . . .\n\n\n\n\nYou can make a local Docker image to use instead of one of these.\n\n\nTo start up the SD-in-a-Box, execute the following command\n\n\n$ docker-compose -p screwdriver up\n\n\n\n\nVolume-Mounted Source Code\n\n\nYou can choose to replace a component with a local copy. This is incredibly helpful if you're trying to implement an update to a service and want to see how it impacts the entire cluster.\n\n\nModify the \ndocker-compose.yaml\n, targeting the component you would like to replace. In the following snippet, we replace the API with a local source.\n\n\nservices:\n  api:\n    # this \nbuild\n stanza replaces the default \nimage\n setting\n    build:\n      context: ./relative/path/to/api_source\n      dockerfile: Dockerfile.local\n  ui:\n    . . .\n  store:\n    . . .\n\n\n\n\nTo set your update, you'll need to rebuild the docker-compose services first.\n\n\n$ docker-compose build\n\n\n\n\nRestart the local cluster to have your changes take effect.\n\n\n$ docker-compose -p screwdriver down\n$ docker-compose -p screwdriver up\n\n\n\n\nCaveats\n\n\nThis approach does very well with replacing complete services, and also carries some limitations:\n\n\n\n\nUnable to replace individual modules with this methodology.\n\n\n\n\nLocal Development Instances\n\n\nIf you plan on making adjustments to a specific Screwdriver component, you can choose to replace a component with your development instance. This will give you a good idea on how it affects the other Screwdriver components before submitting it via Pull Request.\n\n\nGeneral configuration\n\n\nOne important thing to note is that your \ndocker-compose.yml\n will have all the components configured by I.P. address (as opposed to \nlocalhost\n). The following features will cease to work if you choose to use \nlocalhost\n instead of an I.P. address:\n\n\n\n\nBuilds will not start locally\n\n\n\n\nConfiguring the UI\n\n\nYou can choose to use a local development instance of the UI.\n\n\nIn development mode, the UI hosts itself on port \n4200\n and assumes the API is served locally on \n8080\n by default. You would need to modify the UI's \nconfig/environment.js\n file to point to your local Screwdriver cluster, specifcally the API. This can be done by modifying the value right before the \nreturn ENV;\n statement.\n\n\nThe following is a snippet that highlights the change you would make in the \nconfig/environment.js\n\n\n . . .\n ENV.APP.SDAPI_HOSTNAME = 'http://11.22.33.44:8080';  // 8080 is the default. You can also change this\n return ENV;\n\n\n\n\nThe following snippet highlights the \ndocker-compose.yml\n values that need to be modified to use your local UI instance along with the SD-in-a-box cluster.\n\n\nversion: '2'\nservices:\n  api:\n    . . .\n    ports:\n      - 8080:80    # UI default port for API is 8080. This can be changed according to the value you set in config/environment.js\n    environment:\n      URI: http://11.22.33.44:8080             # Tells the launcher where to communicate build updates to the API.\n      ECOSYSTEM_UI: http://11.22.33.44:4200    # Tells the API where the UI is hosted. Related to OAuth mismatching-hostname issues\n    . . .\n\n\n\n\nPlease note that you cannot use \nlocalhost\n for the \nECOSYSTEM_UI\n value if the \nURI\n value is set to an I.P. address. You will receive an invalid token after log-in.\n\n\nConfiguring the API\n\n\nYou can choose to use a local development instance of the API.\n\n\nFurther customization can be done by setting the related environment variables. Learn more in \nthe API documentation\n\n\nConfiguring the Store\n\n\nYou can choose to use a localhost development instance of the Store.\n\n\nIn development mode, the Store hosts itself on port \n80\n by default. You may change this value to whichever port you desire. For the purposes of this guide, we will assume it's hosted on \n8888\n.\n\n\nThe following snippet highlights the \ndocker-compose.yml\n values that need to be modified to use your local store instance along with the SD-in-a-box cluster.\n\n\nversion: '2'\nservices:\n  store:\n    . . .\n    ports:\n      - 8888:80    # Port 8888 is arbitrary. You can choose another if you prefer\n    environment:\n      URI: http://11.22.33.44:9001\n      ECOSYSTEM_STORE: http://10.73.202.183:8888    # Tells the API where the store is hosted\n    . . .", 
            "title": "Running Locally"
        }, 
        {
            "location": "/cluster-management/running-locally/#running-locally", 
            "text": "You can run Screwdriver locally by using our Screwdriver-in-a-box tool.", 
            "title": "Running Locally"
        }, 
        {
            "location": "/cluster-management/running-locally/#sd-in-a-box", 
            "text": "This handy feature will bring up an entire Screwdriver instance (UI, API, and log store) locally for you to play with.", 
            "title": "SD-in-a-Box"
        }, 
        {
            "location": "/cluster-management/running-locally/#requires", 
            "text": "Mac OSX 10.10+  Docker for Mac  Docker Compose 1.8.1+  Python 2.6+   Run the below command in your terminal to bring up a Screwdriver cluster locally.  $ python  (curl https://raw.githubusercontent.com/screwdriver-cd/screwdriver/master/in-a-box.py)  You will be prompted to enter your Client ID and Client Secret. Afterwards, type  y  to launch Screwdriver!", 
            "title": "Requires:"
        }, 
        {
            "location": "/cluster-management/running-locally/#configuring-sd-in-a-box", 
            "text": "SD-in-a-box was intended to be an easy way to run a Screwdriver cluster locally on your development machine so you can demo its features first-hand.", 
            "title": "Configuring SD-in-a-Box"
        }, 
        {
            "location": "/cluster-management/running-locally/#custom-docker-images", 
            "text": "Since it's powered by Docker, you can determine which images to use for it. SD-in-a-Box (and Screwdriver as a whole) uses the following Docker images:   screwdrivercd/screwdriver  - API. The main engine of the CI/CD cluster.  screwdrivercd/ui  - UI. To pleasantly interact with Screwdriver.  screwdrivercd/store  - Artifact repository. Responsible for artifacts like build logs   templates  screwdrivercd/launcher  - Worker component that executes the build. You  cannot  change the image. You can only specify a specific tag to use.   Here is a snippet of the  docker-compose.yml  file  version: '2'\nservices:\n  api:\n    image: screwdrivercd/screwdriver:stable\n    . . .\n  ui:\n    image: screwdrivercd/ui:stable\n    . . .\n  store:\n    image: screwdrivercd/store:stable\n    . . .  You can make a local Docker image to use instead of one of these.  To start up the SD-in-a-Box, execute the following command  $ docker-compose -p screwdriver up", 
            "title": "Custom Docker Images"
        }, 
        {
            "location": "/cluster-management/running-locally/#volume-mounted-source-code", 
            "text": "You can choose to replace a component with a local copy. This is incredibly helpful if you're trying to implement an update to a service and want to see how it impacts the entire cluster.  Modify the  docker-compose.yaml , targeting the component you would like to replace. In the following snippet, we replace the API with a local source.  services:\n  api:\n    # this  build  stanza replaces the default  image  setting\n    build:\n      context: ./relative/path/to/api_source\n      dockerfile: Dockerfile.local\n  ui:\n    . . .\n  store:\n    . . .  To set your update, you'll need to rebuild the docker-compose services first.  $ docker-compose build  Restart the local cluster to have your changes take effect.  $ docker-compose -p screwdriver down\n$ docker-compose -p screwdriver up", 
            "title": "Volume-Mounted Source Code"
        }, 
        {
            "location": "/cluster-management/running-locally/#caveats", 
            "text": "This approach does very well with replacing complete services, and also carries some limitations:   Unable to replace individual modules with this methodology.", 
            "title": "Caveats"
        }, 
        {
            "location": "/cluster-management/running-locally/#local-development-instances", 
            "text": "If you plan on making adjustments to a specific Screwdriver component, you can choose to replace a component with your development instance. This will give you a good idea on how it affects the other Screwdriver components before submitting it via Pull Request.", 
            "title": "Local Development Instances"
        }, 
        {
            "location": "/cluster-management/running-locally/#general-configuration", 
            "text": "One important thing to note is that your  docker-compose.yml  will have all the components configured by I.P. address (as opposed to  localhost ). The following features will cease to work if you choose to use  localhost  instead of an I.P. address:   Builds will not start locally", 
            "title": "General configuration"
        }, 
        {
            "location": "/cluster-management/running-locally/#configuring-the-ui", 
            "text": "You can choose to use a local development instance of the UI.  In development mode, the UI hosts itself on port  4200  and assumes the API is served locally on  8080  by default. You would need to modify the UI's  config/environment.js  file to point to your local Screwdriver cluster, specifcally the API. This can be done by modifying the value right before the  return ENV;  statement.  The following is a snippet that highlights the change you would make in the  config/environment.js   . . .\n ENV.APP.SDAPI_HOSTNAME = 'http://11.22.33.44:8080';  // 8080 is the default. You can also change this\n return ENV;  The following snippet highlights the  docker-compose.yml  values that need to be modified to use your local UI instance along with the SD-in-a-box cluster.  version: '2'\nservices:\n  api:\n    . . .\n    ports:\n      - 8080:80    # UI default port for API is 8080. This can be changed according to the value you set in config/environment.js\n    environment:\n      URI: http://11.22.33.44:8080             # Tells the launcher where to communicate build updates to the API.\n      ECOSYSTEM_UI: http://11.22.33.44:4200    # Tells the API where the UI is hosted. Related to OAuth mismatching-hostname issues\n    . . .  Please note that you cannot use  localhost  for the  ECOSYSTEM_UI  value if the  URI  value is set to an I.P. address. You will receive an invalid token after log-in.", 
            "title": "Configuring the UI"
        }, 
        {
            "location": "/cluster-management/running-locally/#configuring-the-api", 
            "text": "You can choose to use a local development instance of the API.  Further customization can be done by setting the related environment variables. Learn more in  the API documentation", 
            "title": "Configuring the API"
        }, 
        {
            "location": "/cluster-management/running-locally/#configuring-the-store", 
            "text": "You can choose to use a localhost development instance of the Store.  In development mode, the Store hosts itself on port  80  by default. You may change this value to whichever port you desire. For the purposes of this guide, we will assume it's hosted on  8888 .  The following snippet highlights the  docker-compose.yml  values that need to be modified to use your local store instance along with the SD-in-a-box cluster.  version: '2'\nservices:\n  store:\n    . . .\n    ports:\n      - 8888:80    # Port 8888 is arbitrary. You can choose another if you prefer\n    environment:\n      URI: http://11.22.33.44:9001\n      ECOSYSTEM_STORE: http://10.73.202.183:8888    # Tells the API where the store is hosted\n    . . .", 
            "title": "Configuring the Store"
        }, 
        {
            "location": "/cluster-management/kubernetes/", 
            "text": "Setting Up a Screwdriver Cluster on AWS using Kubernetes\n\n\nWe'll go over how to set up a Screwdriver cluster on AWS using Kubernetes, Github, and a Postgres database. You can setup a Screwdriver cluster using \nKubernetes\n.\n\n\nScrewdriver cluster\n\n\nA Screwdriver cluster consists of a Kubernetes cluster running the Screwdriver API. The Screwdriver API modifies Screwdriver tables in AWS RDS.\n\n\n\n\nPrerequisites\n\n\n\n\nkubectl\n\n\nan \nAWS\n account\n\n\nAWS CLI\n\n\n\n\nCreate your Kubernetes cluster\n\n\nFollow instructions at \nRunning Kubernetes on AWS EC2\n.\n\n\nSetup Screwdriver secrets\n\n\nAfter creating your Kubernetes cluster, you'll need to populate it with some secrets that will give you access to your database and Github.\nA \nSecret\n is an object that contains a small amount of sensitive data such as a password, token, or key.\n\n\nHere's a list of secrets we will need:\n\n\n\n\n\n\n\n\nSecret Key\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSECRET_JWT_PRIVATE_KEY\n\n\nA private key used for signing JWT tokens\n\n\n\n\n\n\nSECRET_JWT_PUBLIC_KEY\n\n\nA public key used for signing JWT tokens\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_DATABASE\n\n\nSQL database name\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_USERNAME\n\n\nSQL database username\n\n\n\n\n\n\nDATASTORE_SEQUELIZE_PASSWORD\n\n\nSQL database password\n\n\n\n\n\n\nSECRET_OAUTH_CLIENT_ID\n\n\nThe client ID used for \nOAuth\n with Github\n\n\n\n\n\n\nSECRET_OAUTH_CLIENT_SECRET\n\n\nThe client secret used for OAuth with github\n\n\n\n\n\n\nWEBHOOK_GITHUB_SECRET\n\n\nSecret to add to GitHub webhooks so that we can validate them\n\n\n\n\n\n\nSECRET_PASSWORD\n\n\nA password used for encrypting session, and OAuth data. Can be anything. \nNeeds to be minimum 32 characters\n\n\n\n\n\n\nK8S_TOKEN\n\n\nYour Kubernetes \n\n\n\n\n\n\n\n\nGenerate JWT keys\n\n\nTo generate a \njwtprivatekey\n, run:\n\n\n$ openssl genrsa -out jwt.pem 2048\n\n\nTo generate a \njwtpublickey\n, run:\n\n\n$ openssl rsa -in jwt.pem -pubout -out jwt.pub\n\n\nGet your OAuth Client ID and Secret\n\n\n\n\n\n\nNavigate to the \nOAuth applications\n page.\n\n\n\n\n\n\nClick Register a new application.\n\n\n\n\n\n\nFill out the information and click Register application.\n\n\n\n\n\n\n\n\nYou should see a \nClient ID\n and \nClient Secret\n, which will be used for your \noauthclientid\n and \noauthclientsecret\n, respectively.\n\n\nCreate a datastore\n\n\nTo get your SQL datastore secrets, set up a datastore with \nAWS RDS\n.\n\n\n\n\n\n\nNavigate to \nAWS RDS\n. Click on Launch a DB Instance.\n\n\n\n\n\n\n\n\nSelect the \nPostgreSQL\n tab. Click Select.\n\n\n\n\n\n\n\n\nChoose an environment (Production or Dev/Test) and click Next Step.\n\n\n\n\n\n\n\n\nFill out the DB Instance Identifier (\nDATASTORE_SEQUELIZE_DATABASE\n), Master Username (\nDATASTORE_SEQUELIZE_USERNAME\n), Master Password (\nDATASTORE_SEQUELIZE_PASSWORD\n), and Confirm Password. Click Next Step.\n\n\n\n\n\n\n\n\nAdd a Database Name. Make sure the VPC Security Group chosen gives inbound access to all IPs. Click Launch DB Instance.\n\n\n\n\n\n\n\n\nClick View Your DB Instances. Click on the small triangle next to the Engine column on your DB instance row to open up the details. Your endpoint will be your Database host name.\n\n\n\n\n\n\n\n\nBase64 encode your secrets\n\n\nEach secret must be \nbase64 encoded\n. You must base64 encode each of your secrets:\n\n\n$ echo -n \nsomejwtprivatekey\n | base64\nc29tZWp3dHByaXZhdGVrZXk=\n$ echo -n \nanypassword\n | base64\nYW55cGFzc3dvcmQ=\n\n\n\n\nSetting up secrets in Kubernetes\n\n\nTo create secrets in Kubernetes, create a \nsecret.yaml\n file and populate it with your secrets. These secrets will be used in your Kubernetes \ndeployment.yaml\n file.\n\n\nIt should look similar to the following:\n\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: screwdriver-api-secrets\ntype: Opaque\ndata:\n  # make sure the values are all base64 encoded\n  dbhost: ZGJob3N0bmFtZWhlcmU=\n  dbusername: bXlkYXRhYmFzZQ==\n  dbpassword: c29tZXBhc3N3b3Jk\n  password: YW55cGFzc3dvcmQ=\n  oauthclientid: c29tZWNsaWVudGlk\n  oauthclientsecret: c29tZWNsaWVudHNlY3JldA==\n  jwtprivatekey: c29tZWp3dHByaXZhdGVrZXk=\n  jwtpublickey: c29tZWp3dHB1YmxpY2tleQ==\n  githubsecret: c29tZWdpdGh1YnNlY3JldA==\n\n\n\n\nCreate the secrets using \nkubectl create\n:\n\n\n$ kubectl create -f ./secret.yaml\n\n\n\n\nAdditional environment variables\n\n\nOther environment variables can also be customized for Screwdriver. For a full list, see the \ncustom-environment-variables.yaml\n file.\n\n\nDeploy Screwdriver\n\n\nYou can check out the \napi.yaml\n in the \nScrewdriver config examples repo\n for example service and deployment definitions to run the Screwdriver API.\n\n\nCreate a Service\n\n\nA Kubernetes Service is an abstraction which defines a set of Pods and is assigned a unique IP address which persists.\nFollow instructions in \nCreating a Service\n to set up your \nservice.yaml\n.\n\n\nIt should look like the Service in \napi.yaml\n.\n\n\nTo create your service, run the \nkubectl create\n command on your \nservice.yaml\n file:\n\n\n$ kubectl create -f service.yaml\n\n\n\n\nGet your Kubernetes token name\n\n\nKubernetes actually sets up your Kubernetes token by default. You will need this for your \ndeployment.yaml\n.\nYou can use \nkubectl\n to see your \nKubernetes secrets\n.\n\n\nGet the \nDEFAULT_TOKEN_NAME\n, by running:\n\n\n$ kubectl get secrets\n\nNAME                      TYPE                                  DATA      AGE\ndefault-token-abc55       kubernetes.io/service-account-token   3         50d\n\n\n\n\nThe \nDEFAULT_TOKEN_NAME\n will be listed under \nName\n when the \nType\n is \nkubernetes.io/service-account-token\n.\n\n\nGet your URI\n\n\nYou will need to get the Load Balancer Ingress to set your \nURI\n in your \ndeployment.yaml\n.\n\n\nGet the \nLoadBalancer Ingress\n, by running:\n\n\n$ kubectl describe services sdapi\n\n\n\n\nCreate a Deployment\n\n\nA Deployment makes sure a specified number of pod \u201creplicas\u201d are running at any one time. If there are too many, it will kill some; if there are too few, it will start more. Follow instructions on the \nDeploying Applications\n page to create your \ndeployment.yaml\n.\n\n\nIt should look like the Deployment in \napi.yaml\n.\n\n\nDeploy\n\n\nFor a fresh deployment, run the \nkubectl create\n command on your \ndeployment.yaml\n file:\n\n\n$ kubectl create -f deployment.yaml\n\n\n\n\nView your pods\n\n\nA Kubernetes \npod\n is a group of containers, tied together for the purposes of administration and networking.\n\n\nTo view the pod created by the deployment, run:\n\n\n$ kubectl get pods\n\n\n\n\nTo view the stdout / stderr from a pod, run:\n\n\n$ kubectl logs \nPOD-NAME\n\n\n\n\n\nUpdate your OAuth Application\n\n\nYou will need to navigate back to your original OAuth Application that you used for your OAuth Client ID and Secret to update the URLs.\n\n\n\n\n\n\nNavigate to the \nOAuth applications\n page.\n\n\n\n\n\n\nClick on the application you created to get your OAuth Client ID and Secret.\n\n\n\n\n\n\nFill out the \nHomepage URL\n and \nAuthorization callback URL\n with your \nLoadBalancer Ingress\n.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/cluster-management/kubernetes/#setting-up-a-screwdriver-cluster-on-aws-using-kubernetes", 
            "text": "We'll go over how to set up a Screwdriver cluster on AWS using Kubernetes, Github, and a Postgres database. You can setup a Screwdriver cluster using  Kubernetes .", 
            "title": "Setting Up a Screwdriver Cluster on AWS using Kubernetes"
        }, 
        {
            "location": "/cluster-management/kubernetes/#screwdriver-cluster", 
            "text": "A Screwdriver cluster consists of a Kubernetes cluster running the Screwdriver API. The Screwdriver API modifies Screwdriver tables in AWS RDS.", 
            "title": "Screwdriver cluster"
        }, 
        {
            "location": "/cluster-management/kubernetes/#prerequisites", 
            "text": "kubectl  an  AWS  account  AWS CLI", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/cluster-management/kubernetes/#create-your-kubernetes-cluster", 
            "text": "Follow instructions at  Running Kubernetes on AWS EC2 .", 
            "title": "Create your Kubernetes cluster"
        }, 
        {
            "location": "/cluster-management/kubernetes/#setup-screwdriver-secrets", 
            "text": "After creating your Kubernetes cluster, you'll need to populate it with some secrets that will give you access to your database and Github.\nA  Secret  is an object that contains a small amount of sensitive data such as a password, token, or key.  Here's a list of secrets we will need:     Secret Key  Description      SECRET_JWT_PRIVATE_KEY  A private key used for signing JWT tokens    SECRET_JWT_PUBLIC_KEY  A public key used for signing JWT tokens    DATASTORE_SEQUELIZE_DATABASE  SQL database name    DATASTORE_SEQUELIZE_USERNAME  SQL database username    DATASTORE_SEQUELIZE_PASSWORD  SQL database password    SECRET_OAUTH_CLIENT_ID  The client ID used for  OAuth  with Github    SECRET_OAUTH_CLIENT_SECRET  The client secret used for OAuth with github    WEBHOOK_GITHUB_SECRET  Secret to add to GitHub webhooks so that we can validate them    SECRET_PASSWORD  A password used for encrypting session, and OAuth data. Can be anything.  Needs to be minimum 32 characters    K8S_TOKEN  Your Kubernetes", 
            "title": "Setup Screwdriver secrets"
        }, 
        {
            "location": "/cluster-management/kubernetes/#generate-jwt-keys", 
            "text": "To generate a  jwtprivatekey , run:  $ openssl genrsa -out jwt.pem 2048  To generate a  jwtpublickey , run:  $ openssl rsa -in jwt.pem -pubout -out jwt.pub", 
            "title": "Generate JWT keys"
        }, 
        {
            "location": "/cluster-management/kubernetes/#get-your-oauth-client-id-and-secret", 
            "text": "Navigate to the  OAuth applications  page.    Click Register a new application.    Fill out the information and click Register application.     You should see a  Client ID  and  Client Secret , which will be used for your  oauthclientid  and  oauthclientsecret , respectively.", 
            "title": "Get your OAuth Client ID and Secret"
        }, 
        {
            "location": "/cluster-management/kubernetes/#create-a-datastore", 
            "text": "To get your SQL datastore secrets, set up a datastore with  AWS RDS .    Navigate to  AWS RDS . Click on Launch a DB Instance.     Select the  PostgreSQL  tab. Click Select.     Choose an environment (Production or Dev/Test) and click Next Step.     Fill out the DB Instance Identifier ( DATASTORE_SEQUELIZE_DATABASE ), Master Username ( DATASTORE_SEQUELIZE_USERNAME ), Master Password ( DATASTORE_SEQUELIZE_PASSWORD ), and Confirm Password. Click Next Step.     Add a Database Name. Make sure the VPC Security Group chosen gives inbound access to all IPs. Click Launch DB Instance.     Click View Your DB Instances. Click on the small triangle next to the Engine column on your DB instance row to open up the details. Your endpoint will be your Database host name.", 
            "title": "Create a datastore"
        }, 
        {
            "location": "/cluster-management/kubernetes/#base64-encode-your-secrets", 
            "text": "Each secret must be  base64 encoded . You must base64 encode each of your secrets:  $ echo -n  somejwtprivatekey  | base64\nc29tZWp3dHByaXZhdGVrZXk=\n$ echo -n  anypassword  | base64\nYW55cGFzc3dvcmQ=", 
            "title": "Base64 encode your secrets"
        }, 
        {
            "location": "/cluster-management/kubernetes/#setting-up-secrets-in-kubernetes", 
            "text": "To create secrets in Kubernetes, create a  secret.yaml  file and populate it with your secrets. These secrets will be used in your Kubernetes  deployment.yaml  file.  It should look similar to the following:  apiVersion: v1\nkind: Secret\nmetadata:\n  name: screwdriver-api-secrets\ntype: Opaque\ndata:\n  # make sure the values are all base64 encoded\n  dbhost: ZGJob3N0bmFtZWhlcmU=\n  dbusername: bXlkYXRhYmFzZQ==\n  dbpassword: c29tZXBhc3N3b3Jk\n  password: YW55cGFzc3dvcmQ=\n  oauthclientid: c29tZWNsaWVudGlk\n  oauthclientsecret: c29tZWNsaWVudHNlY3JldA==\n  jwtprivatekey: c29tZWp3dHByaXZhdGVrZXk=\n  jwtpublickey: c29tZWp3dHB1YmxpY2tleQ==\n  githubsecret: c29tZWdpdGh1YnNlY3JldA==  Create the secrets using  kubectl create :  $ kubectl create -f ./secret.yaml", 
            "title": "Setting up secrets in Kubernetes"
        }, 
        {
            "location": "/cluster-management/kubernetes/#additional-environment-variables", 
            "text": "Other environment variables can also be customized for Screwdriver. For a full list, see the  custom-environment-variables.yaml  file.", 
            "title": "Additional environment variables"
        }, 
        {
            "location": "/cluster-management/kubernetes/#deploy-screwdriver", 
            "text": "You can check out the  api.yaml  in the  Screwdriver config examples repo  for example service and deployment definitions to run the Screwdriver API.", 
            "title": "Deploy Screwdriver"
        }, 
        {
            "location": "/cluster-management/kubernetes/#create-a-service", 
            "text": "A Kubernetes Service is an abstraction which defines a set of Pods and is assigned a unique IP address which persists.\nFollow instructions in  Creating a Service  to set up your  service.yaml .  It should look like the Service in  api.yaml .  To create your service, run the  kubectl create  command on your  service.yaml  file:  $ kubectl create -f service.yaml", 
            "title": "Create a Service"
        }, 
        {
            "location": "/cluster-management/kubernetes/#get-your-kubernetes-token-name", 
            "text": "Kubernetes actually sets up your Kubernetes token by default. You will need this for your  deployment.yaml .\nYou can use  kubectl  to see your  Kubernetes secrets .  Get the  DEFAULT_TOKEN_NAME , by running:  $ kubectl get secrets\n\nNAME                      TYPE                                  DATA      AGE\ndefault-token-abc55       kubernetes.io/service-account-token   3         50d  The  DEFAULT_TOKEN_NAME  will be listed under  Name  when the  Type  is  kubernetes.io/service-account-token .", 
            "title": "Get your Kubernetes token name"
        }, 
        {
            "location": "/cluster-management/kubernetes/#get-your-uri", 
            "text": "You will need to get the Load Balancer Ingress to set your  URI  in your  deployment.yaml .  Get the  LoadBalancer Ingress , by running:  $ kubectl describe services sdapi", 
            "title": "Get your URI"
        }, 
        {
            "location": "/cluster-management/kubernetes/#create-a-deployment", 
            "text": "A Deployment makes sure a specified number of pod \u201creplicas\u201d are running at any one time. If there are too many, it will kill some; if there are too few, it will start more. Follow instructions on the  Deploying Applications  page to create your  deployment.yaml .  It should look like the Deployment in  api.yaml .", 
            "title": "Create a Deployment"
        }, 
        {
            "location": "/cluster-management/kubernetes/#deploy", 
            "text": "For a fresh deployment, run the  kubectl create  command on your  deployment.yaml  file:  $ kubectl create -f deployment.yaml", 
            "title": "Deploy"
        }, 
        {
            "location": "/cluster-management/kubernetes/#view-your-pods", 
            "text": "A Kubernetes  pod  is a group of containers, tied together for the purposes of administration and networking.  To view the pod created by the deployment, run:  $ kubectl get pods  To view the stdout / stderr from a pod, run:  $ kubectl logs  POD-NAME", 
            "title": "View your pods"
        }, 
        {
            "location": "/cluster-management/kubernetes/#update-your-oauth-application", 
            "text": "You will need to navigate back to your original OAuth Application that you used for your OAuth Client ID and Secret to update the URLs.    Navigate to the  OAuth applications  page.    Click on the application you created to get your OAuth Client ID and Secret.    Fill out the  Homepage URL  and  Authorization callback URL  with your  LoadBalancer Ingress .", 
            "title": "Update your OAuth Application"
        }, 
        {
            "location": "/user-guide/quickstart/", 
            "text": "Getting Started with Screwdriver\n\n\nThis page will cover how to build and deploy a sample app with Screwdriver in minutes. In this example, we are using the SCM provider Github.\n\n\nRequirements\n\n\n\n\nGithub account\n\n\n\n\nSet Up\n\n\nFirst, fork and clone a sample repository into your local development environment and cd into the project directory. We will cover the generic quickstart in this example.\n\n\n\n\ngeneric\n*\n\n\nGolang\n\n\nNodejs\n\n\nRuby\n\n\n\n\n$ git clone git@github.com:\nYOUR_USERNAME_HERE\n/quickstart-generic.git\n$ cd quickstart-generic/\n\n\n\n\n*For applications that are better suited to Makefiles and small scripts, we recommend referencing the generic \nscrewdriver.yaml\n.\n\n\nDeveloping the App\n\n\nNow that we\u2019ve setup our app, we can start developing. This app demonstrates how to run a \nMakefile\n and bash script (\nmy_script.sh\n) in your Screwdriver build.\n\n\nscrewdriver.yaml\n\n\nThe \nscrewdriver.yaml\n is the only config file you need for using Screwdriver. In it, you will define all your steps needed to successfully develop, build and deploy your application.\n\n\nWorkflow\n\n\nThe \nworkflow\n describes the order that the jobs execute. The \"main\" job, which is created by default, is always\nexecuted first, followed by jobs listed in this workflow block.\n\n\nHere, we have defined a job named \"second_job\", which\nwill run after the \"main\" job.\n\n\n---\n# Workflow list definition\nworkflow:\n  - second_job\n\n\n\n\nShared\n\n\nThe \nshared\n section is where you would define any attributes that all your jobs will inherit.\n\n\nIn our example, we state that all our jobs will run in the same Docker container image \"buildpack-deps\". The \nimage\n is usually defined in the form of \"repo_name\". Alternatively, you can define the image as \"repo_name:tag_label\", where \"tag_label\" is a version. See the \nDocker documentation\n for more information.\n\n\n# Shared definition block\nshared:\n  # Source: https://hub.docker.com/r/library/buildpack-deps/\n  image: buildpack-deps\n\n\n\n\nJobs\n\n\nThe \njobs\n section is where all the tasks (or \nsteps\n) that each job will execute is defined. All pipelines have \"main\" implicitly defined. The definitions in your screwdriver.yaml file will override the implied defaults.\n\n\nSteps\n\n\nThe \nsteps\n section contains a list of commands to execute.\nEach step takes the form \"step_name: command_to_run\". The \"step_name\" is a convenient label to reference it by. The\n\"command_to_run\" is the single command that is executed during this step. Step names cannot start with \nsd-\n, as those steps are reserved for Screwdriver steps. Environment variables will be passed between steps, within the same job. In essence, Screwdriver runs \n/bin/sh\n in your terminal then executes all the steps; in rare cases, different terminal/shell setups may have unexpected behavior.\n\n\nIn our example, our \"main\" job executes a simple piece of inline bash code. The first step (\nexport\n) exports an environment variable, \nGREETING=\"Hello, world!\"\n. The second step (\nhello\n) echoes the environment variable from the first step.\n\n\nWe also define another job called \"second_job\". In this job, we intend on running a different set of commands. The \"make_target\" step calls a Makefile target to perform some set of actions. This is incredibly useful when you need to perform a multi-line command.\nThe \"run_arbitrary_script\" executes a script. This is an alternative to a Makefile target where you want to run a series of commands related to this step.\n\n\n# Job definition block\n# \nmain\n is a default job that all pipelines have\njobs:\n  main:\n    # Steps definition block.\n    steps:\n      - export: export GREETING=\nHello, world!\n\n      - hello: echo $GREETING\n  second_job:\n    steps:\n      - make_target: make greetings\n      - run_arbitrary_script: ./my_script.sh\n\n\n\n\nNow that we have a working repository, let's head over to the Screwdriver UI to build and deploy an app. (For more information on Screwdriver YAMLs, see the \nconfiguration\n page.)\n\n\nBuilding with Screwdriver\n\n\nIn order to use Screwdriver, you will need to login to Screwdriver using Github, set up your pipeline, and start a build.\n\n\nCreate a New Pipeline\n\n\n\n\n\n\nClick on the Create icon. (You will be redirected to login if you have not already.)\n\n\n\n\n\n\nClick Login with SCM Provider.\n\n\n\n\n\n\nYou will be asked to give Screwdriver access to your repositories. Choose appropriately and click Authorize.\n\n\n\n\n\n\nEnter your repository link into the field. SSH or HTTPS link is fine, with \n#\nYOUR_BRANCH_NAME\n immediately after (ex: \ngit@github.com:screwdriver-cd/guide.git#test\n). If no \nBRANCH_NAME\n is provided, it will default to the \nmaster\n branch.\nClick Use this repository to confirm and then click Create Pipeline.\n\n\n\n\n\n\nStart Your First Build\n\n\nNow that you've created a pipeline, you should be directed to your new pipeline page. Click the Start button to start your build.\n\n\nCongratulations! You just built and ran your first app using Screwdriver!", 
            "title": "Quickstart"
        }, 
        {
            "location": "/user-guide/quickstart/#getting-started-with-screwdriver", 
            "text": "This page will cover how to build and deploy a sample app with Screwdriver in minutes. In this example, we are using the SCM provider Github.", 
            "title": "Getting Started with Screwdriver"
        }, 
        {
            "location": "/user-guide/quickstart/#requirements", 
            "text": "Github account", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/quickstart/#set-up", 
            "text": "First, fork and clone a sample repository into your local development environment and cd into the project directory. We will cover the generic quickstart in this example.   generic *  Golang  Nodejs  Ruby   $ git clone git@github.com: YOUR_USERNAME_HERE /quickstart-generic.git\n$ cd quickstart-generic/  *For applications that are better suited to Makefiles and small scripts, we recommend referencing the generic  screwdriver.yaml .", 
            "title": "Set Up"
        }, 
        {
            "location": "/user-guide/quickstart/#developing-the-app", 
            "text": "Now that we\u2019ve setup our app, we can start developing. This app demonstrates how to run a  Makefile  and bash script ( my_script.sh ) in your Screwdriver build.", 
            "title": "Developing the App"
        }, 
        {
            "location": "/user-guide/quickstart/#screwdriveryaml", 
            "text": "The  screwdriver.yaml  is the only config file you need for using Screwdriver. In it, you will define all your steps needed to successfully develop, build and deploy your application.", 
            "title": "screwdriver.yaml"
        }, 
        {
            "location": "/user-guide/quickstart/#workflow", 
            "text": "The  workflow  describes the order that the jobs execute. The \"main\" job, which is created by default, is always\nexecuted first, followed by jobs listed in this workflow block.  Here, we have defined a job named \"second_job\", which\nwill run after the \"main\" job.  ---\n# Workflow list definition\nworkflow:\n  - second_job", 
            "title": "Workflow"
        }, 
        {
            "location": "/user-guide/quickstart/#shared", 
            "text": "The  shared  section is where you would define any attributes that all your jobs will inherit.  In our example, we state that all our jobs will run in the same Docker container image \"buildpack-deps\". The  image  is usually defined in the form of \"repo_name\". Alternatively, you can define the image as \"repo_name:tag_label\", where \"tag_label\" is a version. See the  Docker documentation  for more information.  # Shared definition block\nshared:\n  # Source: https://hub.docker.com/r/library/buildpack-deps/\n  image: buildpack-deps", 
            "title": "Shared"
        }, 
        {
            "location": "/user-guide/quickstart/#jobs", 
            "text": "The  jobs  section is where all the tasks (or  steps ) that each job will execute is defined. All pipelines have \"main\" implicitly defined. The definitions in your screwdriver.yaml file will override the implied defaults.", 
            "title": "Jobs"
        }, 
        {
            "location": "/user-guide/quickstart/#steps", 
            "text": "The  steps  section contains a list of commands to execute.\nEach step takes the form \"step_name: command_to_run\". The \"step_name\" is a convenient label to reference it by. The\n\"command_to_run\" is the single command that is executed during this step. Step names cannot start with  sd- , as those steps are reserved for Screwdriver steps. Environment variables will be passed between steps, within the same job. In essence, Screwdriver runs  /bin/sh  in your terminal then executes all the steps; in rare cases, different terminal/shell setups may have unexpected behavior.  In our example, our \"main\" job executes a simple piece of inline bash code. The first step ( export ) exports an environment variable,  GREETING=\"Hello, world!\" . The second step ( hello ) echoes the environment variable from the first step.  We also define another job called \"second_job\". In this job, we intend on running a different set of commands. The \"make_target\" step calls a Makefile target to perform some set of actions. This is incredibly useful when you need to perform a multi-line command.\nThe \"run_arbitrary_script\" executes a script. This is an alternative to a Makefile target where you want to run a series of commands related to this step.  # Job definition block\n#  main  is a default job that all pipelines have\njobs:\n  main:\n    # Steps definition block.\n    steps:\n      - export: export GREETING= Hello, world! \n      - hello: echo $GREETING\n  second_job:\n    steps:\n      - make_target: make greetings\n      - run_arbitrary_script: ./my_script.sh  Now that we have a working repository, let's head over to the Screwdriver UI to build and deploy an app. (For more information on Screwdriver YAMLs, see the  configuration  page.)", 
            "title": "Steps"
        }, 
        {
            "location": "/user-guide/quickstart/#building-with-screwdriver", 
            "text": "In order to use Screwdriver, you will need to login to Screwdriver using Github, set up your pipeline, and start a build.", 
            "title": "Building with Screwdriver"
        }, 
        {
            "location": "/user-guide/quickstart/#create-a-new-pipeline", 
            "text": "Click on the Create icon. (You will be redirected to login if you have not already.)    Click Login with SCM Provider.    You will be asked to give Screwdriver access to your repositories. Choose appropriately and click Authorize.    Enter your repository link into the field. SSH or HTTPS link is fine, with  # YOUR_BRANCH_NAME  immediately after (ex:  git@github.com:screwdriver-cd/guide.git#test ). If no  BRANCH_NAME  is provided, it will default to the  master  branch.\nClick Use this repository to confirm and then click Create Pipeline.", 
            "title": "Create a New Pipeline"
        }, 
        {
            "location": "/user-guide/quickstart/#start-your-first-build", 
            "text": "Now that you've created a pipeline, you should be directed to your new pipeline page. Click the Start button to start your build.", 
            "title": "Start Your First Build"
        }, 
        {
            "location": "/user-guide/quickstart/#congratulations-you-just-built-and-ran-your-first-app-using-screwdriver", 
            "text": "", 
            "title": "Congratulations! You just built and ran your first app using Screwdriver!"
        }, 
        {
            "location": "/user-guide/api/", 
            "text": "API\n\n\nScrewdriver APIs and the data models around them are documented via \nSwagger\n. This prevents out-of-date documentation, enables clients to be auto-generated, and most importantly exposes a human-readable interface.\n\n\n\n\nVersion 4\n is the current API, all links should be prefixed with \n/v4\n.\n\n\n\n\nOur API documentation can be found at \napi.screwdriver.cd/v4/documentation\n. To see yours, go to \n/v4/documentation\n.\n\n\nUsing the API\n\n\nWith Swagger\n\n\nSwagger documentation includes examples and editable parameters to play around with. Visit the \n/v4/documentation\n page and use the interactive \nTry it out!\n buttons to make calls to our API.\n\n\nSwagger page:\n\n\n\nResponse:\n\n\n\nWith a REST Client\n\n\nUse a REST client like \nPostman\n to make requests against the API. You will need an authorization token. To get an authorization token, login using \n/v4/auth/login\n and copy the token value when redirected to \n/v4/auth/token\n. See the \nAuthorization and Authentication\n section for more information.\n\n\nRequests can be made to the API with headers like below:\n\n\nContent-Type: application/json\nAuthorization: Bearer \nYOUR_TOKEN_HERE\n\n\n\n\n\nExample request:\n\n\n\nFor more information and examples, check out our [API documentation].\n\n\nAuthorization and Authentication\n\n\nFor Authentication we're using \nJSON Web Tokens\n. They need to be passed via\nan \nAuthorization\n header. To generate a JWT, visit the \n/v4/auth/login\n endpoint which will redirect you to the \n/v4/auth/token\n endpoint.\n\n\nAuthorization on the other hand is handled by OAuth. This occurs when\nyou visit the \n/v4/auth/login\n endpoint. Screwdriver uses SCM user tokens\nand identity to:\n\n\n\n\nidentify what repositories you have read, write, and admin access to\n\n\nread allows you to view the pipeline\n\n\nwrite allows you to start or stop jobs\n\n\nadmin allows you to create, edit, or delete pipelines\n\n\n\n\n\n\nread the repository's \nscrewdriver.yaml\n\n\nenumerate the list of pull-requests open on your repository\n\n\nupdate the pull-request with the success/failure of the build\n\n\nadd/remove repository web-hooks so Screwdriver can be notified on changes\n\n\n\n\nFor more information, see the \nGitHub OAuth\n documentation.\n\n\nDesign\n\n\nOur API was designed with three principles in mind:\n\n\n\n\nAll interactions of user's data should be done through the API, so that\nthere is a consistent interface across the tools (CLI, Web UI, etc.).\n\n\nResources should be REST-ful and operations should be atomic so that intent\nis clear and human readable.\n\n\nAPI should be versioned and self-documented, so that client code generation\nis possible.\n\n\n\n\nMake Your Own\n\n\nIf you'd like to make your own Swagger documentation, check out our JSON for reference at  \nhttps://api.screwdriver.cd/v4/swagger.json\n. To see your Swagger.json, visit \n/v4/swagger.json\n.", 
            "title": "API"
        }, 
        {
            "location": "/user-guide/api/#api", 
            "text": "Screwdriver APIs and the data models around them are documented via  Swagger . This prevents out-of-date documentation, enables clients to be auto-generated, and most importantly exposes a human-readable interface.   Version 4  is the current API, all links should be prefixed with  /v4 .   Our API documentation can be found at  api.screwdriver.cd/v4/documentation . To see yours, go to  /v4/documentation .", 
            "title": "API"
        }, 
        {
            "location": "/user-guide/api/#using-the-api", 
            "text": "", 
            "title": "Using the API"
        }, 
        {
            "location": "/user-guide/api/#with-swagger", 
            "text": "Swagger documentation includes examples and editable parameters to play around with. Visit the  /v4/documentation  page and use the interactive  Try it out!  buttons to make calls to our API.  Swagger page:  Response:", 
            "title": "With Swagger"
        }, 
        {
            "location": "/user-guide/api/#with-a-rest-client", 
            "text": "Use a REST client like  Postman  to make requests against the API. You will need an authorization token. To get an authorization token, login using  /v4/auth/login  and copy the token value when redirected to  /v4/auth/token . See the  Authorization and Authentication  section for more information.  Requests can be made to the API with headers like below:  Content-Type: application/json\nAuthorization: Bearer  YOUR_TOKEN_HERE   Example request:  For more information and examples, check out our [API documentation].", 
            "title": "With a REST Client"
        }, 
        {
            "location": "/user-guide/api/#authorization-and-authentication", 
            "text": "For Authentication we're using  JSON Web Tokens . They need to be passed via\nan  Authorization  header. To generate a JWT, visit the  /v4/auth/login  endpoint which will redirect you to the  /v4/auth/token  endpoint.  Authorization on the other hand is handled by OAuth. This occurs when\nyou visit the  /v4/auth/login  endpoint. Screwdriver uses SCM user tokens\nand identity to:   identify what repositories you have read, write, and admin access to  read allows you to view the pipeline  write allows you to start or stop jobs  admin allows you to create, edit, or delete pipelines    read the repository's  screwdriver.yaml  enumerate the list of pull-requests open on your repository  update the pull-request with the success/failure of the build  add/remove repository web-hooks so Screwdriver can be notified on changes   For more information, see the  GitHub OAuth  documentation.", 
            "title": "Authorization and Authentication"
        }, 
        {
            "location": "/user-guide/api/#design", 
            "text": "Our API was designed with three principles in mind:   All interactions of user's data should be done through the API, so that\nthere is a consistent interface across the tools (CLI, Web UI, etc.).  Resources should be REST-ful and operations should be atomic so that intent\nis clear and human readable.  API should be versioned and self-documented, so that client code generation\nis possible.", 
            "title": "Design"
        }, 
        {
            "location": "/user-guide/api/#make-your-own", 
            "text": "If you'd like to make your own Swagger documentation, check out our JSON for reference at   https://api.screwdriver.cd/v4/swagger.json . To see your Swagger.json, visit  /v4/swagger.json .", 
            "title": "Make Your Own"
        }, 
        {
            "location": "/user-guide/authentication-authorization/", 
            "text": "Access Control\n\n\nTo simplify access, Screwdriver uses the same security model as the Pipeline's Git repository.\n\n\nAuthorization\n\n\nFor this example, we will be using the GitHub SCM provider.\n\n\nDepending on your permission level to a Git repository, you will have corresponding access to the linked Screwdriver Pipeline.\n\n\n\n\nRead (Guest)\n\n\nView the overall status of the pipeline\n\n\nView the log of a build\n\n\n\n\n\n\nWrite (Collaborator)\n\n\nAll permissions as a Guest\n\n\nStart a new build\n\n\nStop an existing build\n\n\n\n\n\n\nAdmin (Owner)\n\n\nAll permissions as a Collaborator\n\n\nCreate a new pipeline for this repository\n\n\nDelete the existing pipeline\n\n\nCreate, update, delete secrets\n\n\nDisable and enable jobs\n\n\n\n\n\n\n\n\nAuthentication\n\n\nFor Screwdriver to determine your permission level, you need to complete a one-time procedure to link your Git accounts to Screwdriver.  This will only give Screwdriver limited access to your repositories:\n\n\n\n\nREAD-ONLY access to public repositories\n - To read the contents of \nscrewdriver.yaml\n files.\n\n\nFull control of repository hooks\n - To add/remove Screwdriver webhook on pipeline creation/deletion.\n\n\nRead org and team membership\n - To determine your permission-level (see above).\n\n\nAccess commit status\n - To update Pull Requests with the success or failure of your builds.", 
            "title": "Authentication and Authorization"
        }, 
        {
            "location": "/user-guide/authentication-authorization/#access-control", 
            "text": "To simplify access, Screwdriver uses the same security model as the Pipeline's Git repository.", 
            "title": "Access Control"
        }, 
        {
            "location": "/user-guide/authentication-authorization/#authorization", 
            "text": "For this example, we will be using the GitHub SCM provider.  Depending on your permission level to a Git repository, you will have corresponding access to the linked Screwdriver Pipeline.   Read (Guest)  View the overall status of the pipeline  View the log of a build    Write (Collaborator)  All permissions as a Guest  Start a new build  Stop an existing build    Admin (Owner)  All permissions as a Collaborator  Create a new pipeline for this repository  Delete the existing pipeline  Create, update, delete secrets  Disable and enable jobs", 
            "title": "Authorization"
        }, 
        {
            "location": "/user-guide/authentication-authorization/#authentication", 
            "text": "For Screwdriver to determine your permission level, you need to complete a one-time procedure to link your Git accounts to Screwdriver.  This will only give Screwdriver limited access to your repositories:   READ-ONLY access to public repositories  - To read the contents of  screwdriver.yaml  files.  Full control of repository hooks  - To add/remove Screwdriver webhook on pipeline creation/deletion.  Read org and team membership  - To determine your permission-level (see above).  Access commit status  - To update Pull Requests with the success or failure of your builds.", 
            "title": "Authentication"
        }, 
        {
            "location": "/user-guide/configuration/", 
            "text": "Yaml Configuration\n\n\nThis is an interactive guide for exploring various important properties of the screwdriver.yaml configuration for projects.\n\n\nYou can access information about properties by hovering over the property name.\n\n\n\n\n\n\n\nworkflow\n:\n    - \npublish\n\n    - \nparallel\n:\n        - \nseries\n:\n            - \ndeploy-east\n\n            - \nvalidate-east\n\n        - \nseries\n:\n            - \ndeploy-west\n\n            - \nvalidate-west\n\n\nshared\n:\n\n    \nenvironment\n:\n    \nNODE_ENV\n: \ntest\n\n    \nsettings\n:\n\n        \nemail\n:\n    \naddresses\n: \n[test@email.com, test2@email.com]\n\n    \nstatuses\n: \n[SUCCESS, FAILURE]\n\n\njobs\n:\n\n    \nmain\n:\n\n        \nimage\n: \nnode:{{NODE_VERSION}}\n\n        \nmatrix\n:\n    \nNODE_VERSION\n: \n[4,5,6]\n\n        \nsteps\n:\n    - \ninit\n: \nnpm install\n\n    - \ntest\n: \nnpm test\n\n    \npublish\n:\n    \nimage\n: \nnode:6\n\n    \nsteps\n:\n        - \npublish\n: \nnpm publish\n\n    \ndeploy-west\n:\n    \nimage\n: \nnode:6\n\n    \nenvironment\n:\n        \nDEPLOY_ENV\n: \nwest\n\n    \nsteps\n:\n        - \ninit\n: \nnpm install\n\n        - \npublish\n: \nnpm deploy\n\n    \n...\n\n\n\n    \n\n        \n\n            \nWorkflow\n\n            \nDefines the order of jobs that are executed for the project. All jobs referenced by the workflow must be defined in the jobs section.\n\n            \nJobs can execute in parallel, in series, or in any combination of the two, per this example. Special keywords \nparallel\n and \nseries\n define the flow of the jobs. By default, the jobs in the workflow list are run in series after \nmain\n job has completed successfully.\n\n        \n\n        \n\n            \nShared\n\n            \nDefines a global configuration that applies to all jobs. Shared configurations are merged with each job, but may be overridden by more specific configuration in a specific job.\n\n        \n\n        \n\n            \nEnvironment\n\n            \nA set of key/value pairs for environment variables that need to be set. Any configuration that is valid for a job configuration is valid in shared, but will be overridden by specific job configurations.\n\n        \n\n        \n\n            \nSettings\n\n            \nConfigurable settings for any additional build plugins added to Screwdriver.cd.\n\n        \n\n        \n\n            \nEmail\n\n            \nEmails addresses to send notifications to and statuses to send notifications for.\n\n        \n\n        \n\n            \nJobs\n\n            \nA series of jobs that define the behavior of your builds.\n\n        \n\n        \n\n            \nMain\n\n            \nThe only required job. This job is executed automatically whenever there is a code change.\n\n        \n\n        \n\n            \nImage\n\n            \nThis defines the docker image(s) used for the builds. This example shows a template replacement, where a variable is enclosed in curly braces, e.g. {{NODE_VERSION}}. This variable will be changed to the value(s) of the equivalent variable in the matrix setting, resulting in multiple builds running in parallel, each using one of those various images.\n\n        \n\n        \n\n            \nMatrix\n\n            \nThis causes the builds for the job to execute on multiple images in parallel, when used a templated image configuration.\n\n        \n\n        \n\n            \nSteps\n\n            \nDefines the explicit list of commands that are executed in the build, just as if they were entered on the command line. Environment variables will be passed between steps, within the same job. Step definitions are required for all jobs. Step names cannot start with `sd-`, as those steps are reserved for Screwdriver steps. In essence, Screwdriver runs `/bin/sh` in your terminal then executes all the steps; in rare cases, different terminal/shell setups may have unexpected behavior.", 
            "title": "Overall"
        }, 
        {
            "location": "/user-guide/configuration/#yaml-configuration", 
            "text": "This is an interactive guide for exploring various important properties of the screwdriver.yaml configuration for projects.  You can access information about properties by hovering over the property name.    workflow :\n    -  publish \n    -  parallel :\n        -  series :\n            -  deploy-east \n            -  validate-east \n        -  series :\n            -  deploy-west \n            -  validate-west  shared : \n     environment :\n     NODE_ENV :  test \n     settings : \n         email :\n     addresses :  [test@email.com, test2@email.com] \n     statuses :  [SUCCESS, FAILURE]  jobs : \n     main : \n         image :  node:{{NODE_VERSION}} \n         matrix :\n     NODE_VERSION :  [4,5,6] \n         steps :\n    -  init :  npm install \n    -  test :  npm test \n     publish :\n     image :  node:6 \n     steps :\n        -  publish :  npm publish \n     deploy-west :\n     image :  node:6 \n     environment :\n         DEPLOY_ENV :  west \n     steps :\n        -  init :  npm install \n        -  publish :  npm deploy \n     ...", 
            "title": "Yaml Configuration"
        }, 
        {
            "location": "/user-guide/configuration/secrets/", 
            "text": "Build Secrets\n\n\nYou've got secrets to share with your jobs, but these shouldn't be shared with everyone. Screwdriver provides a mechanism to insert secrets as environment variables. Since secrets are exposed as environment variables, they are easy to use inside builds.\n\n\nSecurity\n\n\nThe Screwdriver team takes security very seriously and encrypts all traffic between its various services. User secrets are stored encrypted in our datastore, and their values are only released to those builds that are authorized by the user.\n\n\nWe understand that you, the security-conscious pipeline admin, may not wish to put secrets into pull-request builds, as a malicious pull-requester could expose those secrets without your consent, but still need those secrets as part of the main build. Screwdriver provides an additional flag on secrets, \nallowInPR\n (default: false), that is required to be enabled for a secret to be exposed.\n\n\nSecrets may only be added, modified, or removed by people that are listed as admins of the Git repository associated with a given pipeline. People with \"push\" privileges may also fetch a list of secrets, but not the secret values.\n\n\nConfiguring a job to expose secrets\n\n\nA list of allowed secrets are added to your pipeline configuration. Secret keys may only contain A-Z and underscore characters ( _ ) and must start with a letter.\n\n\nIn the below example, an \nNPM_TOKEN\n secret is added to the \npublish\n job:\n\n\npublish:\n    steps:\n        - publish-npm: npm publish\n    secrets:\n        # Publishing to NPM\n        - NPM_TOKEN\n\n\n\n\nYou may add secrets to any jobs you wish.\n\n\nSecrets in pull-requests\n\n\nFor your own security, we don't recommend exposing secrets to pull-request builds. Pull-request jobs can be created with modified \nscrewdriver.yaml\n files including changes to the secrets configuration.\n\n\nPull-requests operate essentially as copies of the \nmain\n job. The \nmain\n job can be set up to use a secret, and does not expose that secret to pull-requests by default.\n\n\nmain:\n    steps:\n        - my-step: maybeDoSomethingWithASecret.sh\n    secrets:\n        - MY_SECRET\n\n\n\n\nWhen a secret is created via the UI, or API, enabling \nallowInPR\n will cause that secret to be available to pull-request builds, if those secrets are also configured to be exposed in the \nmain\n job.\n\n\nUser Interface\n\n\nThe easiest way to create a secret for your pipeline is via the Screwdriver UI.\n\n\n\nCreating Secrets\n\n\nSimply enter the key and value in the inputs in the grey box, and click the add button. A checkbox is provided to allow you to enable \nallowInPR\n.\n\n\nUpdating Secrets\n\n\nA secret's original value is never delivered to the UI, but values of secrets may be updated in the UI by adding a new value in the text field next to the appropriate key name and clicking the update button.\n\n\nDeleting secrets\n\n\nIndividual secrets may be removed by clicking the Delete button.", 
            "title": "Secrets"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#build-secrets", 
            "text": "You've got secrets to share with your jobs, but these shouldn't be shared with everyone. Screwdriver provides a mechanism to insert secrets as environment variables. Since secrets are exposed as environment variables, they are easy to use inside builds.", 
            "title": "Build Secrets"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#security", 
            "text": "The Screwdriver team takes security very seriously and encrypts all traffic between its various services. User secrets are stored encrypted in our datastore, and their values are only released to those builds that are authorized by the user.  We understand that you, the security-conscious pipeline admin, may not wish to put secrets into pull-request builds, as a malicious pull-requester could expose those secrets without your consent, but still need those secrets as part of the main build. Screwdriver provides an additional flag on secrets,  allowInPR  (default: false), that is required to be enabled for a secret to be exposed.  Secrets may only be added, modified, or removed by people that are listed as admins of the Git repository associated with a given pipeline. People with \"push\" privileges may also fetch a list of secrets, but not the secret values.", 
            "title": "Security"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#configuring-a-job-to-expose-secrets", 
            "text": "A list of allowed secrets are added to your pipeline configuration. Secret keys may only contain A-Z and underscore characters ( _ ) and must start with a letter.  In the below example, an  NPM_TOKEN  secret is added to the  publish  job:  publish:\n    steps:\n        - publish-npm: npm publish\n    secrets:\n        # Publishing to NPM\n        - NPM_TOKEN  You may add secrets to any jobs you wish.", 
            "title": "Configuring a job to expose secrets"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#secrets-in-pull-requests", 
            "text": "For your own security, we don't recommend exposing secrets to pull-request builds. Pull-request jobs can be created with modified  screwdriver.yaml  files including changes to the secrets configuration.  Pull-requests operate essentially as copies of the  main  job. The  main  job can be set up to use a secret, and does not expose that secret to pull-requests by default.  main:\n    steps:\n        - my-step: maybeDoSomethingWithASecret.sh\n    secrets:\n        - MY_SECRET  When a secret is created via the UI, or API, enabling  allowInPR  will cause that secret to be available to pull-request builds, if those secrets are also configured to be exposed in the  main  job.", 
            "title": "Secrets in pull-requests"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#user-interface", 
            "text": "The easiest way to create a secret for your pipeline is via the Screwdriver UI.", 
            "title": "User Interface"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#creating-secrets", 
            "text": "Simply enter the key and value in the inputs in the grey box, and click the add button. A checkbox is provided to allow you to enable  allowInPR .", 
            "title": "Creating Secrets"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#updating-secrets", 
            "text": "A secret's original value is never delivered to the UI, but values of secrets may be updated in the UI by adding a new value in the text field next to the appropriate key name and clicking the update button.", 
            "title": "Updating Secrets"
        }, 
        {
            "location": "/user-guide/configuration/secrets/#deleting-secrets", 
            "text": "Individual secrets may be removed by clicking the Delete button.", 
            "title": "Deleting secrets"
        }, 
        {
            "location": "/user-guide/environment-variables/", 
            "text": "Environment Variables\n\n\nScrewdriver exports a set of environment variables that you can rely on during the course of a build.\n\n\nBuild Specific\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSD_PIPELINE_ID\n\n\nThe ID of your pipeline\n\n\n\n\n\n\nSD_JOB_NAME\n\n\nJob name (e.g.: main)\n\n\n\n\n\n\nSD_BUILD_ID\n\n\nBuild number (e.g.: 1, 2, etc)\n\n\n\n\n\n\nSD_PULL_REQUEST\n\n\nPull Request number (blank if non-PR)\n\n\n\n\n\n\nSD_TOKEN\n\n\nJWT token for the build\n\n\n\n\n\n\n\n\nDirectories\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSD_SOURCE_DIR\n\n\nLocation of checked-out code\n\n\n\n\n\n\nSD_ARTIFACTS_DIR\n\n\nLocation of built/generated files\n\n\n\n\n\n\n\n\nEnvironment Variables\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nenvironment_variable\n\n\nEnvironment variable specified under the \"environment\" section in your \nscrewdriver.yaml\n\n\n\n\n\n\n\n\nSource Code\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSCM_URL\n\n\nSCM URL that was checked out\n\n\n\n\n\n\n\n\nURLs\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSD_API_URL\n\n\nLink to the Screwdriver API URL\n\n\n\n\n\n\n\n\nContinuous Integration\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nSCREWDRIVER\n\n\ntrue\n\n\n\n\n\n\nCI\n\n\ntrue\n\n\n\n\n\n\nCONTINUOUS_INTEGRATION\n\n\ntrue", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/user-guide/environment-variables/#environment-variables", 
            "text": "Screwdriver exports a set of environment variables that you can rely on during the course of a build.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/user-guide/environment-variables/#build-specific", 
            "text": "Name  Value      SD_PIPELINE_ID  The ID of your pipeline    SD_JOB_NAME  Job name (e.g.: main)    SD_BUILD_ID  Build number (e.g.: 1, 2, etc)    SD_PULL_REQUEST  Pull Request number (blank if non-PR)    SD_TOKEN  JWT token for the build", 
            "title": "Build Specific"
        }, 
        {
            "location": "/user-guide/environment-variables/#directories", 
            "text": "Name  Value      SD_SOURCE_DIR  Location of checked-out code    SD_ARTIFACTS_DIR  Location of built/generated files", 
            "title": "Directories"
        }, 
        {
            "location": "/user-guide/environment-variables/#environment-variables_1", 
            "text": "Name  Value      environment_variable  Environment variable specified under the \"environment\" section in your  screwdriver.yaml", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/user-guide/environment-variables/#source-code", 
            "text": "Name  Value      SCM_URL  SCM URL that was checked out", 
            "title": "Source Code"
        }, 
        {
            "location": "/user-guide/environment-variables/#urls", 
            "text": "Name  Value      SD_API_URL  Link to the Screwdriver API URL", 
            "title": "URLs"
        }, 
        {
            "location": "/user-guide/environment-variables/#continuous-integration", 
            "text": "Name  Value      SCREWDRIVER  true    CI  true    CONTINUOUS_INTEGRATION  true", 
            "title": "Continuous Integration"
        }, 
        {
            "location": "/user-guide/templates/", 
            "text": "Templates\n\n\nTemplates are snippets of predefined code that people can use to replace a job definition in a \nscrewdriver.yaml\n. A template contains a series of predefined steps along with a selected Docker image.\n\n\nUsing a template\n\n\nTo use a template, define a \nscrewdriver.yaml\n:\n\n\njobs:\n   main:\n      template: template_name@1.3.0\n\n\n\n\nScrewdriver takes the template configuration and plugs it in, so that the \nscrewdriver.yaml\n becomes:\n\n\njobs:\n   main:\n        image: node:6\n        steps:\n          - install: npm install\n          - test: npm test\n          - echo: echo $FOO\n        environment:\n           FOO: bar\n        secrets:\n          - NPM_TOKEN\n\n\n\n\nCreating a template\n\n\nWriting a template yaml\n\n\nTo create a template, create a new repo with a \nscrewdriver-template.yaml\n file. The file should contain a name, version, description, maintainer email, and a config with an image and steps.\n\n\nExample \nscrewdriver-template.yaml\n:\n\n\nname: template_name\nversion: '1.3'\ndescription: template for testing\nmaintainer: foo@bar.com\nconfig:\n  image: node:6\n  steps:\n    - install: npm install\n    - test: npm test\n    - echo: echo $FOO\n  environment:\n    FOO: bar\n  secrets:\n     - NPM_TOKEN\n\n\n\n\nWriting a screwdriver.yaml for your template repo\n\n\nTo validate your template, run the \ntemplate-validate\n script from the \nscrewdriver-template-main\n npm module in your \nmain\n job to validate your template. To publish your template, run the \ntemplate-publish\n script from the same module in a separate job.\n\n\nBy default, the file at \n./sd-template.yaml\n will be read. However, a user can specify a custom path using the env variable: \nSD_TEMPLATE_PATH\n.\n\n\nExample \nscrewdriver.yaml\n:\n\n\nshared:\n    image: node:6\njobs:\n    # the main job is run in pull requests as well\n    main:\n        steps:\n            - install: npm install screwdriver-template-main\n            - validate: ./node_modules/.bin/template-validate\n        environment:\n            SD_TEMPLATE_PATH: ./path/to/template.yaml\n    publish:\n        steps:\n            - install: npm install screwdriver-template-main\n            - publish: ./node_modules/.bin/template-publish\n        environment:\n            SD_TEMPLATE_PATH: ./path/to/template.yaml\n\n\n\n\nCreate a Screwdriver pipeline with your template repo and start the build to validate and publish it.\n\n\nTo update a Screwdriver template, make changes in your SCM repository and rerun the pipeline build.\n\n\nFinding templates\n\n\nTo figure out which templates already exist, you can make a \nGET\n call to the \n/templates\n endpoint. See the \nAPI documentation\n for more information.", 
            "title": "Templates"
        }, 
        {
            "location": "/user-guide/templates/#templates", 
            "text": "Templates are snippets of predefined code that people can use to replace a job definition in a  screwdriver.yaml . A template contains a series of predefined steps along with a selected Docker image.", 
            "title": "Templates"
        }, 
        {
            "location": "/user-guide/templates/#using-a-template", 
            "text": "To use a template, define a  screwdriver.yaml :  jobs:\n   main:\n      template: template_name@1.3.0  Screwdriver takes the template configuration and plugs it in, so that the  screwdriver.yaml  becomes:  jobs:\n   main:\n        image: node:6\n        steps:\n          - install: npm install\n          - test: npm test\n          - echo: echo $FOO\n        environment:\n           FOO: bar\n        secrets:\n          - NPM_TOKEN", 
            "title": "Using a template"
        }, 
        {
            "location": "/user-guide/templates/#creating-a-template", 
            "text": "", 
            "title": "Creating a template"
        }, 
        {
            "location": "/user-guide/templates/#writing-a-template-yaml", 
            "text": "To create a template, create a new repo with a  screwdriver-template.yaml  file. The file should contain a name, version, description, maintainer email, and a config with an image and steps.  Example  screwdriver-template.yaml :  name: template_name\nversion: '1.3'\ndescription: template for testing\nmaintainer: foo@bar.com\nconfig:\n  image: node:6\n  steps:\n    - install: npm install\n    - test: npm test\n    - echo: echo $FOO\n  environment:\n    FOO: bar\n  secrets:\n     - NPM_TOKEN", 
            "title": "Writing a template yaml"
        }, 
        {
            "location": "/user-guide/templates/#writing-a-screwdriveryaml-for-your-template-repo", 
            "text": "To validate your template, run the  template-validate  script from the  screwdriver-template-main  npm module in your  main  job to validate your template. To publish your template, run the  template-publish  script from the same module in a separate job.  By default, the file at  ./sd-template.yaml  will be read. However, a user can specify a custom path using the env variable:  SD_TEMPLATE_PATH .  Example  screwdriver.yaml :  shared:\n    image: node:6\njobs:\n    # the main job is run in pull requests as well\n    main:\n        steps:\n            - install: npm install screwdriver-template-main\n            - validate: ./node_modules/.bin/template-validate\n        environment:\n            SD_TEMPLATE_PATH: ./path/to/template.yaml\n    publish:\n        steps:\n            - install: npm install screwdriver-template-main\n            - publish: ./node_modules/.bin/template-publish\n        environment:\n            SD_TEMPLATE_PATH: ./path/to/template.yaml  Create a Screwdriver pipeline with your template repo and start the build to validate and publish it.  To update a Screwdriver template, make changes in your SCM repository and rerun the pipeline build.", 
            "title": "Writing a screwdriver.yaml for your template repo"
        }, 
        {
            "location": "/user-guide/templates/#finding-templates", 
            "text": "To figure out which templates already exist, you can make a  GET  call to the  /templates  endpoint. See the  API documentation  for more information.", 
            "title": "Finding templates"
        }, 
        {
            "location": "/user-guide/FAQ/", 
            "text": "Frequently Asked Questions\n\n\nHow do I create a pipeline?\n\n\nTo create a pipeline, click the Create icon and paste a Git URL into the form. Followed by \n#\n and the branch name, if the branch is not \nmaster\n.\n\n\n\n\nHow do I start a pipeline manually?\n\n\nTo start a build manually, click the Start button on your pipeline page.\n\n\n\n\nHow do I update a pipeline repo and branch?\n\n\nIf you want to update your pipeline repo and branch, modify the checkout URL in the Options tab on your pipeline page and click Update.\n\n\n\n\nHow do I disable/enable a job temporarily?\n\n\nTo temporarily disable/enable a job to quickly stop the line, toggle the switch to disable/enable for a particular job under the Options tab on your pipeline page.\n\n\n\n\nHow do I make sure my code is in sync with my pipeline?\n\n\nIf your pipeline looks out of sync after changes were made to it, to make sure it's in sync with your source code. On the Options tab in your pipeline page, click the Sync icon to update the out of sync elements. There are separate Sync icons for SCM webhooks, pull request builds, and pipeline jobs.\n\n\n\n\nHow do I delete a pipeline permanently?\n\n\nIndividual pipelines may be removed by clicking the Delete icon on the Options tab in your pipeline page. This action is not undoable.", 
            "title": "FAQ"
        }, 
        {
            "location": "/user-guide/FAQ/#frequently-asked-questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/user-guide/FAQ/#how-do-i-create-a-pipeline", 
            "text": "To create a pipeline, click the Create icon and paste a Git URL into the form. Followed by  #  and the branch name, if the branch is not  master .", 
            "title": "How do I create a pipeline?"
        }, 
        {
            "location": "/user-guide/FAQ/#how-do-i-start-a-pipeline-manually", 
            "text": "To start a build manually, click the Start button on your pipeline page.", 
            "title": "How do I start a pipeline manually?"
        }, 
        {
            "location": "/user-guide/FAQ/#how-do-i-update-a-pipeline-repo-and-branch", 
            "text": "If you want to update your pipeline repo and branch, modify the checkout URL in the Options tab on your pipeline page and click Update.", 
            "title": "How do I update a pipeline repo and branch?"
        }, 
        {
            "location": "/user-guide/FAQ/#how-do-i-disableenable-a-job-temporarily", 
            "text": "To temporarily disable/enable a job to quickly stop the line, toggle the switch to disable/enable for a particular job under the Options tab on your pipeline page.", 
            "title": "How do I disable/enable a job temporarily?"
        }, 
        {
            "location": "/user-guide/FAQ/#how-do-i-make-sure-my-code-is-in-sync-with-my-pipeline", 
            "text": "If your pipeline looks out of sync after changes were made to it, to make sure it's in sync with your source code. On the Options tab in your pipeline page, click the Sync icon to update the out of sync elements. There are separate Sync icons for SCM webhooks, pull request builds, and pipeline jobs.", 
            "title": "How do I make sure my code is in sync with my pipeline?"
        }, 
        {
            "location": "/user-guide/FAQ/#how-do-i-delete-a-pipeline-permanently", 
            "text": "Individual pipelines may be removed by clicking the Delete icon on the Options tab in your pipeline page. This action is not undoable.", 
            "title": "How do I delete a pipeline permanently?"
        }, 
        {
            "location": "/about/", 
            "text": "What is Screwdriver?\n\n\nA collection of services that facilitate the workflow for continuous delivery pipelines.\n\n\n\n    \n\n        \nSecure Continuous Delivery\n\n        \nScrewdriver treats Continuous Delivery as a first-class citizen in your build pipeline.\n        Easily define the path that your code takes from Pull Request to Production.\n\n    \n\n    \n\n        \n\n    \n\n\n\n\n\n\n    \n\n        \n\n    \n\n    \n\n        \nIntegrates with Daily Habits\n\n        \nScrewdriver ties directly into your DevOps daily habits.\n        It tests your pull requests, builds your merged commits, and deploys to your environments.\n        Define load tests, canary deployments, and multi-environment deployment pipelines with ease.\n\n    \n\n\n\n\n\n\n    \n\n        \nPipeline as Code\n\n        \nDefine your pipeline in a simple YAML file that lives beside your code.\n        There is no external configuration of your pipeline to deal with,\n        so you can review pipeline changes and roll them out with the rest of your codebase.\n\n    \n\n    \n\n        \n\n    \n\n\n\n\n\n\n    \n\n        \n\n    \n\n    \n\n        \nRuns Anywhere\n\n        \nScrewdriver's architecture uses pluggable components under the hood\n        to allow you to swap out the pieces that make sense for your infrastructure.\n        Swap in Postgres for the Datastore or Jenkins for the Executor.\n        You can even dynamically select an execution engine based on the needs of each pipeline.\n        For example, send golang builds to the kubernetes executor while your iOS builds go to a\n        Jenkins execution farm.", 
            "title": "What is Screwdriver?"
        }, 
        {
            "location": "/about/#what-is-screwdriver", 
            "text": "A collection of services that facilitate the workflow for continuous delivery pipelines.", 
            "title": "What is Screwdriver?"
        }, 
        {
            "location": "/about/appendix/domain/", 
            "text": "Domain Model\n\n\n\n\n\n\nSource Code\n\n\nSource Code is a specified SCM repository and branch that contains a \nscrewdriver.yaml\n and the code required to build, test, and publish your application.\n\n\nStep\n\n\nA step is a named action that needs to be performed, usually a single shell command. In essence, Screwdriver runs \n/bin/sh\n in your terminal then executes all the steps; in rare cases, different terminal/shell setups may have unexpected behavior. If the command finishes with a non-zero exit code, the step is considered a failure. Environment variables will be passed between steps, within the same job.\n\n\nContainer\n\n\nA container runs \nsteps\n in an isolated environment. This is done in order to test the compatibility of code running in different environments with different versions, without affecting other \nbuilds\n that may be running at the same time. This is implemented using Docker containers.\n\n\nJob\n\n\nA job consists of executing multiple sequential \nsteps\n inside a specified \ncontainer\n. If any step in the series fails, then the entire job is considered failed and subsequent steps will be skipped (unless configured otherwise).\n\n\nJobs work by checking out the \nsource code\n to a specified commit, setting the desired environment variables, and executing the specified \nsteps\n.\n\n\nDuring the job, the executing \nsteps\n share three pieces of context:\n\n\n\n\nFilesystem\n\n\nContainer\n\n\nMetadata\n\n\n\n\nJobs can be started automatically by changes made in the \nsource code\n or triggered through the \nworkflow\n. Jobs can also be started manually through the UI.\n\n\nPull Requests\n\n\nPull requests are run separately from existing pipeline jobs. They will only execute steps from the \nmain\n job in the Screwdriver configuration.\n\n\nParallelization\n\n\nIt is possible to parallelize a job by defining a matrix of environment variables. These are usually used for testing against multiple \ncontainers\n or test types.\n\n\nIn this example job definition, 4 \nbuilds\n will run in parallel:\n\n\nimage: node:{{NODE_VERSION}}\nsteps:\n    test: npm run test-${TEST_TYPE}\nmatrix:\n    NODE_VERSION:\n        - 4\n        - 6\n    TEST_TYPE:\n        - unit\n        - functional\n\n\n\n\n\n\nNODE_VERSION=4\n and \nTEST_TYPE=unit\n\n\nNODE_VERSION=4\n and \nTEST_TYPE=functional\n\n\nNODE_VERSION=6\n and \nTEST_TYPE=unit\n\n\nNODE_VERSION=6\n and \nTEST_TYPE=functional\n\n\n\n\nBuild\n\n\nA build is an instance of a running \njob\n. All builds are assigned a unique build number. Each build is associated with an \nevent\n. With a basic job configuration, only one build of a job will be running at any given time. If a \njob matrix\n is configured, then there can be multiple builds running in parallel.\n\n\nA build can be in one of five different states:\n\n\n\n\nQUEUED\n - Build is waiting for available resources\n\n\nRUNNING\n - Build is actively running on an executor\n\n\nSUCCESS\n - All steps completed successfully\n\n\nFAILURE\n - One of the steps failed\n\n\nABORTED\n - User canceled the running build\n\n\n\n\nEvent\n\n\nAn event represents a commit or a manual restart of a \npipeline\n. There are 2 types of events:\n\n\n\n\npipeline\n: - Events created when a user manually restarts a pipeline or merges a pull request. This type of event triggers the same sequence of jobs as the pipeline's workflow. For example: \n['main', 'publish', 'deploy']\n\n\npr\n:  - Events created by opening or updating a pull request. This type of event only triggers the \nmain\n job.\n\n\n\n\nMetadata\n\n\nMetadata is a structured key/value storage of relevant information about a \nbuild\n. Metadata will be shared with subsequent builds in the same \nworkflow\n. It can be updated or retrieved throughout the build by using the built-in CLI (\nmeta\n) in the \nsteps\n.\n\n\nExample:\n\n\n$ meta set example.coverage 99.95\n$ meta get example.coverage\n99.95\n$ meta get example\n{\ncoverage\n:99.95}\n\n\n\n\nWorkflow\n\n\nWorkflow is the order that \njobs\n will execute in after a successful \nbuild\n of the \nmain\n job on the default branch. Jobs can be executed in parallel, series, or a combination of the two to allow for all possibilities. Workflow must contain all defined jobs in the pipeline.\n\n\nAll jobs executed in a given workflow share:\n\n\n\n\nSource code checked out from the same git commit\n\n\nAccess to \nmetadata\n from a \nmain\n build that triggered or was selected for this job's build\n\n\n\n\nIn the following example of a workflow section, this is the flow:\n\n\nworkflow:\n    - publish\n    - parallel:\n        - series:\n            - deploy-west\n            - validate-west\n        - series:\n            - deploy-east\n            - validate-east\n\n\n\n\nAfter the merge of a pull-request to master:\n\n\n\n\nmain\n will run and trigger \npublish\n\n\npublish\n will trigger \ndeploy-west\n and \ndeploy-east\n in parallel\n\n\ndeploy-west\n will trigger \nvalidate-west\n\n\ndeploy-east\n will trigger \nvalidate-east\n\n\n\n\nPipeline\n\n\nA pipeline represents a collection of \njobs\n that share the same \nsource code\n. These jobs are executed in the order defined by the \nworkflow\n.\n\n\nThe \nmain\n job is required to be defined in every pipeline as it is the one that builds for each change made to the source code (and proposed changes).", 
            "title": "Domain Model"
        }, 
        {
            "location": "/about/appendix/domain/#domain-model", 
            "text": "", 
            "title": "Domain Model"
        }, 
        {
            "location": "/about/appendix/domain/#source-code", 
            "text": "Source Code is a specified SCM repository and branch that contains a  screwdriver.yaml  and the code required to build, test, and publish your application.", 
            "title": "Source Code"
        }, 
        {
            "location": "/about/appendix/domain/#step", 
            "text": "A step is a named action that needs to be performed, usually a single shell command. In essence, Screwdriver runs  /bin/sh  in your terminal then executes all the steps; in rare cases, different terminal/shell setups may have unexpected behavior. If the command finishes with a non-zero exit code, the step is considered a failure. Environment variables will be passed between steps, within the same job.", 
            "title": "Step"
        }, 
        {
            "location": "/about/appendix/domain/#container", 
            "text": "A container runs  steps  in an isolated environment. This is done in order to test the compatibility of code running in different environments with different versions, without affecting other  builds  that may be running at the same time. This is implemented using Docker containers.", 
            "title": "Container"
        }, 
        {
            "location": "/about/appendix/domain/#job", 
            "text": "A job consists of executing multiple sequential  steps  inside a specified  container . If any step in the series fails, then the entire job is considered failed and subsequent steps will be skipped (unless configured otherwise).  Jobs work by checking out the  source code  to a specified commit, setting the desired environment variables, and executing the specified  steps .  During the job, the executing  steps  share three pieces of context:   Filesystem  Container  Metadata   Jobs can be started automatically by changes made in the  source code  or triggered through the  workflow . Jobs can also be started manually through the UI.", 
            "title": "Job"
        }, 
        {
            "location": "/about/appendix/domain/#pull-requests", 
            "text": "Pull requests are run separately from existing pipeline jobs. They will only execute steps from the  main  job in the Screwdriver configuration.", 
            "title": "Pull Requests"
        }, 
        {
            "location": "/about/appendix/domain/#parallelization", 
            "text": "It is possible to parallelize a job by defining a matrix of environment variables. These are usually used for testing against multiple  containers  or test types.  In this example job definition, 4  builds  will run in parallel:  image: node:{{NODE_VERSION}}\nsteps:\n    test: npm run test-${TEST_TYPE}\nmatrix:\n    NODE_VERSION:\n        - 4\n        - 6\n    TEST_TYPE:\n        - unit\n        - functional   NODE_VERSION=4  and  TEST_TYPE=unit  NODE_VERSION=4  and  TEST_TYPE=functional  NODE_VERSION=6  and  TEST_TYPE=unit  NODE_VERSION=6  and  TEST_TYPE=functional", 
            "title": "Parallelization"
        }, 
        {
            "location": "/about/appendix/domain/#build", 
            "text": "A build is an instance of a running  job . All builds are assigned a unique build number. Each build is associated with an  event . With a basic job configuration, only one build of a job will be running at any given time. If a  job matrix  is configured, then there can be multiple builds running in parallel.  A build can be in one of five different states:   QUEUED  - Build is waiting for available resources  RUNNING  - Build is actively running on an executor  SUCCESS  - All steps completed successfully  FAILURE  - One of the steps failed  ABORTED  - User canceled the running build", 
            "title": "Build"
        }, 
        {
            "location": "/about/appendix/domain/#event", 
            "text": "An event represents a commit or a manual restart of a  pipeline . There are 2 types of events:   pipeline : - Events created when a user manually restarts a pipeline or merges a pull request. This type of event triggers the same sequence of jobs as the pipeline's workflow. For example:  ['main', 'publish', 'deploy']  pr :  - Events created by opening or updating a pull request. This type of event only triggers the  main  job.", 
            "title": "Event"
        }, 
        {
            "location": "/about/appendix/domain/#metadata", 
            "text": "Metadata is a structured key/value storage of relevant information about a  build . Metadata will be shared with subsequent builds in the same  workflow . It can be updated or retrieved throughout the build by using the built-in CLI ( meta ) in the  steps .  Example:  $ meta set example.coverage 99.95\n$ meta get example.coverage\n99.95\n$ meta get example\n{ coverage :99.95}", 
            "title": "Metadata"
        }, 
        {
            "location": "/about/appendix/domain/#workflow", 
            "text": "Workflow is the order that  jobs  will execute in after a successful  build  of the  main  job on the default branch. Jobs can be executed in parallel, series, or a combination of the two to allow for all possibilities. Workflow must contain all defined jobs in the pipeline.  All jobs executed in a given workflow share:   Source code checked out from the same git commit  Access to  metadata  from a  main  build that triggered or was selected for this job's build   In the following example of a workflow section, this is the flow:  workflow:\n    - publish\n    - parallel:\n        - series:\n            - deploy-west\n            - validate-west\n        - series:\n            - deploy-east\n            - validate-east  After the merge of a pull-request to master:   main  will run and trigger  publish  publish  will trigger  deploy-west  and  deploy-east  in parallel  deploy-west  will trigger  validate-west  deploy-east  will trigger  validate-east", 
            "title": "Workflow"
        }, 
        {
            "location": "/about/appendix/domain/#pipeline", 
            "text": "A pipeline represents a collection of  jobs  that share the same  source code . These jobs are executed in the order defined by the  workflow .  The  main  job is required to be defined in every pipeline as it is the one that builds for each change made to the source code (and proposed changes).", 
            "title": "Pipeline"
        }, 
        {
            "location": "/about/appendix/execution-engines/", 
            "text": "Execution Engines\n\n\n\n\nA workload management system for the scheduling and running of jobs.\n\n\n\n\nThe typical enterprise requires support for multiple operating systems.  A core tenet of the project is to leverage both open source projects and commercial cloud products and their capabilities where we can.\nTODO: Tie back to targeted use cases and typical enterprise\n\n\nSupported environments\n\n\nTier 1:\n\n\n\n\nLinux and a typical use case of web serving and associated backend systems.  For the Open Source release we are evaluating a number of options as explained below.\n\n\n\n\nTier 2:\n\n\n\n\nMac OSX for iOS and desktop clients.  Windows may be supported in the future.  Jenkins will be a supported execution engine to handle scheduling across Mac OSX slaves.  Jenkins support across many OSes is useful here.\n\n\n\n\nWhy not Jenkins everywhere?  While Jenkins serves us well in other areas, it has issues scaling and limits overall performance with its architecture.  In addition, managing a Jenkins cluster has high operational overhead.  A downside to not using Jenkins is not having access to the existing plugin ecosystem.\n\n\nSelection Criteria\n\n\n\n\nAvailability outside of Yahoo\n\n\nEase of setup\n\n\nCommunity momentum (leverage industry innovation and future proof our solution)\n\n\nCapabilities (semi-persistent storage, scheduler options, etc)\n\n\nRun on-premise or in cloud (AWS or GCP)\n\n\nOperability\n\n\n\n\nCandidates\n\n\n\n\nKubernetes (also GCP's Container Engine)\n\n\nAmazon's ECS\n\n\nMesos\n\n\nDocker Swarm\n\n\n\n\nInitial analysis\n\n\n\n\nAmazon ECS and Kubernetes had easiest setup.  This allows users to play with the platform easily.  These also have low operational overhead (if using GCP Container Engine in Kubernetes' case) since they are cloud options.\n\n\nKubernetes allows us to have the same interface for on-premise use as well as a native supported interface in GCP.\n\n\nECS would limit us to Amazon and doesn't have an on-premise option.\n\n\nMesos is used internally at Yahoo.  Getting started is more difficult. Need more time to investigate frameworks.\n\n\nDocker Swarm is a candidate but is less mature than other options.  Something to keep an eye on.\n\n\n\n\nCapabilities analysis requires learning the underlying systems to a certain degree.  The evaluation process includes an end to end integration to understand integration points as well as the strength and weaknesses of the system.  Kubernetes was chosen for the first end to end integration.\n\n\nTODO: add results of evaluations", 
            "title": "Execution Engines"
        }, 
        {
            "location": "/about/appendix/execution-engines/#execution-engines", 
            "text": "A workload management system for the scheduling and running of jobs.   The typical enterprise requires support for multiple operating systems.  A core tenet of the project is to leverage both open source projects and commercial cloud products and their capabilities where we can.\nTODO: Tie back to targeted use cases and typical enterprise", 
            "title": "Execution Engines"
        }, 
        {
            "location": "/about/appendix/execution-engines/#supported-environments", 
            "text": "Tier 1:   Linux and a typical use case of web serving and associated backend systems.  For the Open Source release we are evaluating a number of options as explained below.   Tier 2:   Mac OSX for iOS and desktop clients.  Windows may be supported in the future.  Jenkins will be a supported execution engine to handle scheduling across Mac OSX slaves.  Jenkins support across many OSes is useful here.   Why not Jenkins everywhere?  While Jenkins serves us well in other areas, it has issues scaling and limits overall performance with its architecture.  In addition, managing a Jenkins cluster has high operational overhead.  A downside to not using Jenkins is not having access to the existing plugin ecosystem.", 
            "title": "Supported environments"
        }, 
        {
            "location": "/about/appendix/execution-engines/#selection-criteria", 
            "text": "Availability outside of Yahoo  Ease of setup  Community momentum (leverage industry innovation and future proof our solution)  Capabilities (semi-persistent storage, scheduler options, etc)  Run on-premise or in cloud (AWS or GCP)  Operability", 
            "title": "Selection Criteria"
        }, 
        {
            "location": "/about/appendix/execution-engines/#candidates", 
            "text": "Kubernetes (also GCP's Container Engine)  Amazon's ECS  Mesos  Docker Swarm", 
            "title": "Candidates"
        }, 
        {
            "location": "/about/appendix/execution-engines/#initial-analysis", 
            "text": "Amazon ECS and Kubernetes had easiest setup.  This allows users to play with the platform easily.  These also have low operational overhead (if using GCP Container Engine in Kubernetes' case) since they are cloud options.  Kubernetes allows us to have the same interface for on-premise use as well as a native supported interface in GCP.  ECS would limit us to Amazon and doesn't have an on-premise option.  Mesos is used internally at Yahoo.  Getting started is more difficult. Need more time to investigate frameworks.  Docker Swarm is a candidate but is less mature than other options.  Something to keep an eye on.   Capabilities analysis requires learning the underlying systems to a certain degree.  The evaluation process includes an end to end integration to understand integration points as well as the strength and weaknesses of the system.  Kubernetes was chosen for the first end to end integration.  TODO: add results of evaluations", 
            "title": "Initial analysis"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing\n\n\nThank you for considering contributing! There are many ways you can help.\n\n\nIssues\n\n\nFile an issue if you think you've found a bug. Be sure to describe\n\n\n\n\nHow can it be reproduced?\n\n\nWhat did you expect?\n\n\nWhat actually occurred?\n\n\nVersion, platform, etc. if possibly relevant.\n\n\n\n\nYou can file all issues with Screwdriver in the \nscrewdriver repo\n. We will update any issues we're working on with a daily summary. To see what we're currently working on, you can check out our \ndigital scrum board\n in the Projects section in the \nScrewdriver API repo\n.\n\n\nDocs\n\n\nDocumentation, READMEs, and examples are extremely important. Please help improve them and if you find a typo or notice a problem, feel free to send a fix or say something.\n\n\nSubmitting Patches\n\n\nPatches for fixes, features, and improvements are accepted through pull requests.\n\n\n\n\nWrite good commit messages, in the present tense! (Add X, not Added X). Short title, blank line, bullet points if needed. Capitalize the first letter of the title or bullet item. No punctuation in the title.\n\n\nCode must pass lint and style checks.\n\n\nAll external methods must be documented. Add README docs and/or user documentation in our \nguide\n when appropriate.\n\n\nInclude tests to improve coverage and prevent regressions.\n\n\nSquash changes into a single commit per feature/fix. Ask if you're unsure how to discretize your work.\n\n\n\n\nPlease ask before embarking on a large improvement so you're not disappointed if it does not align with the goals of the project or owner(s).\n\n\nCommit Message Format\n\n\nWe use \nsemantic-release\n, which requires commit messages to be in this specific format: \ntype\n(\nscope\n): \nsubject\n\n\n\n\nTypes:\n\n\nfeat (feature)\n\n\nfix (bug fix)\n\n\ndocs (documentation)\n\n\nstyle (formatting, missing semi colons, \u2026)\n\n\nrefactor\n\n\ntest (when adding missing tests)\n\n\nchore (maintain)\n\n\nScope: anything that specifies the scope of the commit. Can be blank or \n*\n\n\nSubject: description of the commit. For \nbreaking changes\n that require major version bump, add \nBREAKING CHANGE\n to the commit message.\n\n\n\n\nExamples commit messages:\n\n\n Bug fix: \nfix: Remove extra space\n\n\n Breaking change: \nfeat(scm): Support new scm plugin. BREAKING CHANGE: github no longer works\n\n\nFeature Requests\n\n\nMake the case for a feature via an issue with a good title. The feature should be discussed and given a target inclusion milestone or closed.\n\n\nWhere to Contribute\n\n\nScrewdriver has a modular architecture, and the various responsibilities are split up in separate repos.\n\n\nScrewdriver API\n \n \n\n\nThe \nscrewdriver\n repo is the core of screwdriver, providing the API endpoints for everything that screwdriver does. The API is based on the \nhapijs framework\n and is implemented in node as a series of plugins.\n\n\nLauncher\n\n\nThe \nlauncher\n performs step execution and housekeeping internal to build containers. This is written in Go, and mounted into build containers as a binary.\n\n\nExecutors\n\n\nAn executor is used to manage build containers for any given job. Several implementations of executors have been created. All are designed to follow a common interface. Executor implementations are written in node:\n\n\n\n\nexecutor-base\n: Common interface \n\n\nexecutor-docker\n: Docker implementation \n\n\nexecutor-j5s\n: Jenkins implementation \n\n\nexecutor-k8s\n: Kubernetes implementation \n\n\n\n\nModels\n\n\nThe object models provide the definition of the data that is stored in data stores. This is done in two parts:\n\n\n\n\ndata-schema\n: Schema definition with \nJoi\n \n\n\nmodels\n: Specific business logic around the data schema \n\n\n\n\nDatastores\n\n\nA datastore implementation is used as the interface between the API and a data storage mechanism. There are several implementations written in node around a common interface.\n\n\n\n\ndatastore-base\n: Common interface \n\n\ndatastore-sequelize\n: Mysql, postgres, sqlite3 and mssql implementation \n\n\n\n\nScms\n\n\nAn scm implementation is used as the interface between the API and an scm. There are several implementations written in node around a common interface.\n\n\n\n\nscm-base\n: Common interface \n\n\nscm-bitbucket\n: Bitbucket implementation \n\n\nscm-github\n: Github implementation \n\n\n\n\nConfig Parser\n \n\n\nNode module for validating and parsing user's \nscrewdriver.yaml\n configurations.\n\n\nGuide\n\n\nThis documentation! Everything you ever hoped to know about the Screwdriver project.\n\n\nMiscellaneous Tools\n\n\n\n\nclient\n: Simple Go-based CLI for accessing the Screwdriver API\n\n\ngitversion\n: Go-based tool for updating git tags on a repo for a new version number\n\n\ncircuit-fuses\n: Wrapper to provide a node-circuitbreaker w/ callback interface \n\n\nkeymbinatorial\n: Generates the unique combinations of key values by taking a single value from each keys array \n\n\n\n\nAdding a New Screwdriver Repo\n\n\nWe have some tools to help start out new repos for screwdriver:\n\n\n\n\ngenerator-screwdriver\n: Yeoman generator that bootstraps new repos for screwdriver\n\n\neslint-config-screwdriver\n: Our eslint rules for node-based code. Included in each new repo as part of the bootstrap process", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing", 
            "text": "Thank you for considering contributing! There are many ways you can help.", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#issues", 
            "text": "File an issue if you think you've found a bug. Be sure to describe   How can it be reproduced?  What did you expect?  What actually occurred?  Version, platform, etc. if possibly relevant.   You can file all issues with Screwdriver in the  screwdriver repo . We will update any issues we're working on with a daily summary. To see what we're currently working on, you can check out our  digital scrum board  in the Projects section in the  Screwdriver API repo .", 
            "title": "Issues"
        }, 
        {
            "location": "/about/contributing/#docs", 
            "text": "Documentation, READMEs, and examples are extremely important. Please help improve them and if you find a typo or notice a problem, feel free to send a fix or say something.", 
            "title": "Docs"
        }, 
        {
            "location": "/about/contributing/#submitting-patches", 
            "text": "Patches for fixes, features, and improvements are accepted through pull requests.   Write good commit messages, in the present tense! (Add X, not Added X). Short title, blank line, bullet points if needed. Capitalize the first letter of the title or bullet item. No punctuation in the title.  Code must pass lint and style checks.  All external methods must be documented. Add README docs and/or user documentation in our  guide  when appropriate.  Include tests to improve coverage and prevent regressions.  Squash changes into a single commit per feature/fix. Ask if you're unsure how to discretize your work.   Please ask before embarking on a large improvement so you're not disappointed if it does not align with the goals of the project or owner(s).", 
            "title": "Submitting Patches"
        }, 
        {
            "location": "/about/contributing/#commit-message-format", 
            "text": "We use  semantic-release , which requires commit messages to be in this specific format:  type ( scope ):  subject   Types:  feat (feature)  fix (bug fix)  docs (documentation)  style (formatting, missing semi colons, \u2026)  refactor  test (when adding missing tests)  chore (maintain)  Scope: anything that specifies the scope of the commit. Can be blank or  *  Subject: description of the commit. For  breaking changes  that require major version bump, add  BREAKING CHANGE  to the commit message.   Examples commit messages:   Bug fix:  fix: Remove extra space   Breaking change:  feat(scm): Support new scm plugin. BREAKING CHANGE: github no longer works", 
            "title": "Commit Message Format"
        }, 
        {
            "location": "/about/contributing/#feature-requests", 
            "text": "Make the case for a feature via an issue with a good title. The feature should be discussed and given a target inclusion milestone or closed.", 
            "title": "Feature Requests"
        }, 
        {
            "location": "/about/contributing/#where-to-contribute", 
            "text": "Screwdriver has a modular architecture, and the various responsibilities are split up in separate repos.", 
            "title": "Where to Contribute"
        }, 
        {
            "location": "/about/contributing/#screwdriver-api", 
            "text": "The  screwdriver  repo is the core of screwdriver, providing the API endpoints for everything that screwdriver does. The API is based on the  hapijs framework  and is implemented in node as a series of plugins.", 
            "title": "Screwdriver API"
        }, 
        {
            "location": "/about/contributing/#launcher", 
            "text": "The  launcher  performs step execution and housekeeping internal to build containers. This is written in Go, and mounted into build containers as a binary.", 
            "title": "Launcher"
        }, 
        {
            "location": "/about/contributing/#executors", 
            "text": "An executor is used to manage build containers for any given job. Several implementations of executors have been created. All are designed to follow a common interface. Executor implementations are written in node:   executor-base : Common interface   executor-docker : Docker implementation   executor-j5s : Jenkins implementation   executor-k8s : Kubernetes implementation", 
            "title": "Executors"
        }, 
        {
            "location": "/about/contributing/#models", 
            "text": "The object models provide the definition of the data that is stored in data stores. This is done in two parts:   data-schema : Schema definition with  Joi    models : Specific business logic around the data schema", 
            "title": "Models"
        }, 
        {
            "location": "/about/contributing/#datastores", 
            "text": "A datastore implementation is used as the interface between the API and a data storage mechanism. There are several implementations written in node around a common interface.   datastore-base : Common interface   datastore-sequelize : Mysql, postgres, sqlite3 and mssql implementation", 
            "title": "Datastores"
        }, 
        {
            "location": "/about/contributing/#scms", 
            "text": "An scm implementation is used as the interface between the API and an scm. There are several implementations written in node around a common interface.   scm-base : Common interface   scm-bitbucket : Bitbucket implementation   scm-github : Github implementation", 
            "title": "Scms"
        }, 
        {
            "location": "/about/contributing/#config-parser", 
            "text": "Node module for validating and parsing user's  screwdriver.yaml  configurations.", 
            "title": "Config Parser"
        }, 
        {
            "location": "/about/contributing/#guide", 
            "text": "This documentation! Everything you ever hoped to know about the Screwdriver project.", 
            "title": "Guide"
        }, 
        {
            "location": "/about/contributing/#miscellaneous-tools", 
            "text": "client : Simple Go-based CLI for accessing the Screwdriver API  gitversion : Go-based tool for updating git tags on a repo for a new version number  circuit-fuses : Wrapper to provide a node-circuitbreaker w/ callback interface   keymbinatorial : Generates the unique combinations of key values by taking a single value from each keys array", 
            "title": "Miscellaneous Tools"
        }, 
        {
            "location": "/about/contributing/#adding-a-new-screwdriver-repo", 
            "text": "We have some tools to help start out new repos for screwdriver:   generator-screwdriver : Yeoman generator that bootstraps new repos for screwdriver  eslint-config-screwdriver : Our eslint rules for node-based code. Included in each new repo as part of the bootstrap process", 
            "title": "Adding a New Screwdriver Repo"
        }, 
        {
            "location": "/about/support/", 
            "text": "Support\n\n\nGitHub\n\n\nScrewdriver is completely open source and can be found under the \nscrewdriver-cd organization\n\non Github. We welcome any \nissues\n and \npull requests\n!\nFor more information on our Github repositories and how to contribute, see the \nContributing\n page.\n\n\nSlack\n\n\nWe use Slack for discussion and support. For any Screwdriver-related questions, join the \n#general\n channel on the\n\nScrewdriver Slack team\n. For everything else, join the \n#random\n channel.\n\n\nTo sign up, use our \nSlack inviter\n.\n\n\nStack Overflow\n\n\nWe monitor Stack Overflow for any posts tagged with \nscrewdriver-cd\n. If\nthere aren't any existing questions that help with your problem, feel free to ask a new one!", 
            "title": "Support"
        }, 
        {
            "location": "/about/support/#support", 
            "text": "", 
            "title": "Support"
        }, 
        {
            "location": "/about/support/#github", 
            "text": "Screwdriver is completely open source and can be found under the  screwdriver-cd organization \non Github. We welcome any  issues  and  pull requests !\nFor more information on our Github repositories and how to contribute, see the  Contributing  page.", 
            "title": "GitHub"
        }, 
        {
            "location": "/about/support/#slack", 
            "text": "We use Slack for discussion and support. For any Screwdriver-related questions, join the  #general  channel on the Screwdriver Slack team . For everything else, join the  #random  channel.  To sign up, use our  Slack inviter .", 
            "title": "Slack"
        }, 
        {
            "location": "/about/support/#stack-overflow", 
            "text": "We monitor Stack Overflow for any posts tagged with  screwdriver-cd . If\nthere aren't any existing questions that help with your problem, feel free to ask a new one!", 
            "title": "Stack Overflow"
        }
    ]
}